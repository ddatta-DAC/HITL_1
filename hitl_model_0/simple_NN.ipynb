{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "sys.path.append('./..')\n",
    "sys.path.append('./../..')\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "import glob \n",
    "import yaml\n",
    "import pickle\n",
    "from scipy.special import softmax as SOFTMAX\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from common_utils import utils\n",
    "try:\n",
    "    from common_utils import AD_result_fetcher\n",
    "except:\n",
    "    from .common_utils import AD_result_fetcher\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import LongTensor as LT\n",
    "from torch import FloatTensor as FT\n",
    "from scipy.spatial.distance import cosine\n",
    "from collections import Counter\n",
    "import faiss    \n",
    "from  sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_bipartite_embeddings():\n",
    "    global DIR\n",
    "    LOC = './saved_model_data'\n",
    "    _files = sorted(glob.glob(os.path.join(LOC,DIR, '**.npy')))\n",
    "    emb_dict = {}\n",
    "    for file in _files:\n",
    "        _domain = file.split('/')[-1].split('_')[0]\n",
    "        emb_dict[_domain] = np.load(file)\n",
    "    return emb_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Convert the df from serial_ID to entity_ID\n",
    "# ---------------------------------------------\n",
    "def convert_toEntityID(target_df):\n",
    "    global bipartite_domains\n",
    "    serialID_map_df = utils.fetch_idMappingFile(DIR)\n",
    "    serialID_entityID_dict = {}\n",
    "    \n",
    "    for domain in set(bipartite_domains):\n",
    "        if domain not in target_df.columns : continue\n",
    "        tmp =  serialID_map_df.loc[(serialID_map_df['domain'] == domain)]\n",
    "        serial_id = tmp['serial_id'].values.tolist()\n",
    "        entity_id = tmp['entity_id'].values.tolist()\n",
    "        serialID_entityID_dict[domain] = {k:v for k,v in zip(serial_id,entity_id)}\n",
    "   \n",
    "    def convert_aux(val, domain):\n",
    "        return serialID_entityID_dict[domain][val]\n",
    "    domain_list = bipartite_domains\n",
    "    for domain in tqdm(domain_list):\n",
    "        target_df[domain] = target_df[domain].parallel_apply(convert_aux, args=(domain,))\n",
    "    return target_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Create synthetic mapping \n",
    "# So that ids are continuous. \n",
    "# Also embeddings can be accessed using an numpy array\n",
    "# -----------------------------  \n",
    "def convert_to_SynID (data_df, emb_dict):\n",
    "    global bipartite_domains\n",
    "    global DATA_LOC\n",
    "    global DIR\n",
    "    \n",
    "    with open(os.path.join(DATA_LOC, DIR, 'domain_dims.pkl'),'rb') as fh:\n",
    "        domain_dims = pickle.load(fh)\n",
    "        \n",
    "    synID = 0\n",
    "    cur = 0\n",
    "    col_syn_id = []\n",
    "    col_entity_id = []\n",
    "    col_domain_names = []\n",
    "    \n",
    "    # ------------------\n",
    "    for d in sorted(bipartite_domains):\n",
    "        s = domain_dims[d]\n",
    "        col_entity_id.extend(list(range(s)))\n",
    "        col_domain_names.extend([d for _ in range(s)])\n",
    "        tmp = np.arange(s) + cur\n",
    "        tmp = tmp.tolist()\n",
    "        col_syn_id.extend(tmp)\n",
    "        cur += s\n",
    "\n",
    "    data = {'domain': col_domain_names, 'entity_id': col_entity_id, 'syn_id': col_syn_id}\n",
    "    synID_mapping_df = pd.DataFrame(data)\n",
    "\n",
    "    # -------------------\n",
    "    # Replace entity_id with synthetic id \n",
    "    # -------------------\n",
    "    mapping_dict = {}\n",
    "    for domain in sorted(set(synID_mapping_df['domain'])):\n",
    "        tmp =  synID_mapping_df.loc[(synID_mapping_df['domain'] == domain)]\n",
    "        syn_id = tmp['syn_id'].values.tolist()\n",
    "        entity_id = tmp['entity_id'].values.tolist()\n",
    "        mapping_dict[domain] = { k:v for k,v in zip(entity_id,syn_id) }\n",
    "        def convert_aux(val, domain):\n",
    "            return mapping_dict[domain][val]\n",
    "\n",
    "    for domain in tqdm(bipartite_domains):\n",
    "        data_df[domain] = data_df[domain].parallel_apply(convert_aux, args=(domain,))\n",
    "    \n",
    "    num_entities = len(synID_mapping_df)\n",
    "    emb_array = np.zeros( [num_entities, emb_dict[bipartite_domains[0]].shape[-1]])\n",
    "    \n",
    "    for dom in bipartite_domains:\n",
    "        tmp = synID_mapping_df.loc[synID_mapping_df['domain']==dom]\n",
    "        synID = tmp['syn_id'].values\n",
    "        entityID = tmp['entity_id'].values\n",
    "        emb_array[synID] = emb_dict[dom][entityID]\n",
    "        \n",
    "    return data_df,  emb_array , synID_mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'config.yaml'\n",
    "with open(config_file,'r') as fh:\n",
    "    CONFIG = yaml.safe_load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = 6\n",
    "WHITELIST_COUNT = 5\n",
    "attr_consignee = CONFIG['attribute_CONSIGNEE'] \n",
    "attr_shipper = CONFIG['attribute_SHIPPER']\n",
    "ID_COL = CONFIG['ID_COL']\n",
    "DATA_LOC = CONFIG['DATA_LOC']\n",
    "bipartite_domains = sorted([attr_consignee, attr_shipper])\n",
    "NEG_PERCENTILE_THRESHOLD = CONFIG['NEG_PERCENTILE_THRESHOLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22648 22648\n",
      "95 0.5650949312585849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.47s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "DIR = 'us_import1'\n",
    "\n",
    "labelled_results = AD_result_fetcher.read_in_AD_result(DIR)\n",
    "ANOMALY_PERCENTILE_THRESHOLD  = CONFIG['ANOMALY_PERCENTILE_THRESHOLD']\n",
    "score_threshold = np.percentile(labelled_results['score'],CONFIG['ANOMALY_PERCENTILE_THRESHOLD'])\n",
    "print(CONFIG['ANOMALY_PERCENTILE_THRESHOLD'], score_threshold)\n",
    "bipartite_embeddings = obtain_bipartite_embeddings()\n",
    "\n",
    "main_df = (labelled_results[[ID_COL , 'label', 'score'] + bipartite_domains]).copy()\n",
    "df1 = convert_toEntityID(main_df.copy())\n",
    "data_df, emb_array , synID_mapping_df = convert_to_SynID (df1.copy(), bipartite_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "10012\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Build Nearest Neighbor Query Index\n",
    "# -----------------------------------\n",
    "index = faiss.IndexFlatL2(32)   # build the index\n",
    "print(index.is_trained)\n",
    "index.add(emb_array.astype(np.float32))                 \n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = data_df.copy()\n",
    "working_df['dynamic_score'] = working_df['score'].values\n",
    "# ------------------\n",
    "# Type conversion : to ensure no bugs\n",
    "# ------------------\n",
    "working_df['PanjivaRecordID'] = working_df['PanjivaRecordID'].astype(int)\n",
    "working_df['ConsigneePanjivaID'] = working_df['ConsigneePanjivaID'].astype(int)\n",
    "working_df['ShipperPanjivaID'] = working_df['ShipperPanjivaID'].astype(int)\n",
    "working_df = working_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_label_cutoff_score = np.percentile(df_cur['score'], 60)\n",
    "df2 = working_df.loc[working_df['score'] <= neg_label_cutoff_score]\n",
    "elements = df2[attr_consignee].values.tolist() + df2[attr_shipper].values.tolist()\n",
    "white_list_entities = []\n",
    "for k,v in Counter(elements).items(): \n",
    "    if v > WHITELIST_COUNT : \n",
    "        white_list_entities.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # white_list_entities 1014\n"
     ]
    }
   ],
   "source": [
    "print(' # white_list_entities', len(white_list_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_top_k = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_v1(df_cur, label_top_k):\n",
    "    global attr_consignee\n",
    "    global ID_COL\n",
    "    global attr_shipper\n",
    "    print('label_top_k ',label_top_k)\n",
    "    df_cur = df_cur.sort_values(by='score',ascending=False)\n",
    "    min_score = np.percentile(df_cur['score'], ANOMALY_PERCENTILE_THRESHOLD)\n",
    "    seen_ids = []\n",
    "    count = 0\n",
    "    labelled_df = None\n",
    "    idx = 0\n",
    "\n",
    "    for i,row in df_cur.iterrows():\n",
    "        _id = int(row[ID_COL])\n",
    "        idx = i\n",
    "        seen_ids.append(_id)\n",
    "\n",
    "        if row['label']==1: \n",
    "            count +=1\n",
    "            if count > label_top_k : \n",
    "                break\n",
    "\n",
    "    seen_ids = set(seen_ids)\n",
    "    labelled_df = df_cur.iloc[:idx,:]\n",
    "    unlabelled_df = df_cur.iloc[idx:,:]\n",
    "    orig_target_df = unlabelled_df.copy().reset_index(drop=True)\n",
    "\n",
    "\n",
    "    tmp_N = labelled_df.loc[labelled_df['label']!=1]\n",
    "    tmp_P = labelled_df.loc[labelled_df['label']==1]\n",
    "    neg_entities = tmp_N[attr_consignee].values.tolist() + tmp_N[attr_shipper].values.tolist()\n",
    "    pos_entities = tmp_P[attr_consignee].values.tolist() + tmp_P[attr_shipper].values.tolist()\n",
    "    pos_entities = list(sorted(set(pos_entities)))\n",
    "    neg_entities = list(sorted(set(neg_entities)))\n",
    "\n",
    "    print(' - / +', len(set(neg_entities)),len(set(pos_entities)) )\n",
    "    query_P = (emb_array[pos_entities]).astype(np.float32)\n",
    "    D, nn_index_P = index.search(query_P, KNN)  \n",
    "    nn_index_P = nn_index_P[:,1:]\n",
    "\n",
    "    query_N = (emb_array[neg_entities]).astype(np.float32)\n",
    "    D, nn_index_N = index.search(query_N, KNN)  \n",
    "    nn_index_N = nn_index_N[:,1:]\n",
    "\n",
    "\n",
    "    candidates = unlabelled_df.loc[unlabelled_df['score']>=min_score ]\n",
    "    candidates.loc[((candidates[attr_consignee].isin(white_list)) | (candidates[attr_shipper].isin(white_list)))]           \n",
    "    # first see records where both match +\n",
    "    df1 = candidates.loc[candidates[attr_consignee].isin(pos_label_entities) & (candidates[attr_shipper].isin(pos_label_entities))]\n",
    "    df1.loc[:,'dynamic_score'] = 2\n",
    "\n",
    "    # first see records where both match -\n",
    "    df2 = candidates.loc[candidates[attr_consignee].isin(neg_entities) & (candidates[attr_shipper].isin(neg_entities))]\n",
    "    df2.loc[:,'dynamic_score'] = 0\n",
    "\n",
    "    # first see records where one matches -\n",
    "    df3 = candidates.loc[((candidates[attr_consignee].isin(white_list)) | (candidates[attr_shipper].isin(white_list)))]\n",
    "    df3.loc[:,'dynamic_score'] = 0\n",
    "\n",
    "    # At least one matches +, and the other is not part of  -\n",
    "    df5_1 = candidates.loc[(candidates[attr_consignee].isin(pos_label_entities)) & ~(candidates[attr_shipper].isin(neg_label_entities))]\n",
    "    df5_2 = candidates.loc[(candidates[attr_shipper].isin(pos_label_entities)) & ~(candidates[attr_consignee].isin(neg_label_entities))]\n",
    "    df_5 = pd.concat([df5_1,df5_2])\n",
    "    df5 = df_5.loc[~df_5[ID_COL].isin(df1)]\n",
    "    df5 = df5.loc[~df5[ID_COL].isin(df3)]\n",
    "    df5.loc[:,'dynamic_score'] = 0.75\n",
    "\n",
    "    # One of the entities has been marked negative \n",
    "    df6 = candidates.loc[(candidates[attr_shipper].isin(neg_label_entities)) | (candidates[attr_consignee].isin(neg_label_entities))]\n",
    "    df6 = df6.loc[~df6[ID_COL].isin(df2)]\n",
    "    df6.loc[:,'dynamic_score'] = 0.25\n",
    "\n",
    "    id_list = df1[ID_COL].values.tolist() + df2[ID_COL].values.tolist() + df3[ID_COL].values.tolist() + df5[ID_COL].values.tolist() +  df6[ID_COL].values.tolist()\n",
    "    id_list = list(set(id_list))\n",
    "\n",
    "    df4 = candidates.loc[~candidates[ID_COL].isin(id_list)]\n",
    "    df4.loc[:,'dynamic_score'] = 0.5\n",
    "    df0 = pd.concat([df1,df5,df4,df6,df3,df2])\n",
    "\n",
    "    new_df = df0.sort_values( by = 'dynamic_score', ascending=False)\n",
    "\n",
    "    for k in [50,100,150,200,250]:\n",
    "        # without input?\n",
    "        top_k_target = orig_target_df.head(k)\n",
    "        wo_input_precison = len(top_k_target.loc[top_k_target['label']==1])/k\n",
    "        tmp =  new_df.head(k)\n",
    "        correct = tmp.loc[tmp['label']==1]\n",
    "        prec = len(correct)/len(tmp)\n",
    "        print(' precision at next top {:3d} : {:.3f}'.format(k, prec), ' || Without input {:.3f}'.format(wo_input_precison) )\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_top_k  10\n",
      " - / + 20 11\n",
      " precision at next top  50 : 0.900  || Without input 0.420\n",
      " precision at next top 100 : 0.780  || Without input 0.410\n",
      " precision at next top 150 : 0.773  || Without input 0.433\n",
      " precision at next top 200 : 0.725  || Without input 0.480\n",
      " precision at next top 250 : 0.680  || Without input 0.488\n",
      "label_top_k  20\n",
      " - / + 35 18\n",
      " precision at next top  50 : 0.860  || Without input 0.320\n",
      " precision at next top 100 : 0.780  || Without input 0.430\n",
      " precision at next top 150 : 0.767  || Without input 0.433\n",
      " precision at next top 200 : 0.690  || Without input 0.490\n",
      " precision at next top 250 : 0.644  || Without input 0.488\n",
      "label_top_k  30\n",
      " - / + 53 25\n",
      " precision at next top  50 : 0.860  || Without input 0.400\n",
      " precision at next top 100 : 0.800  || Without input 0.450\n",
      " precision at next top 150 : 0.740  || Without input 0.500\n",
      " precision at next top 200 : 0.670  || Without input 0.510\n",
      " precision at next top 250 : 0.612  || Without input 0.516\n",
      "label_top_k  40\n",
      " - / + 81 34\n",
      " precision at next top  50 : 0.840  || Without input 0.520\n",
      " precision at next top 100 : 0.780  || Without input 0.540\n",
      " precision at next top 150 : 0.760  || Without input 0.560\n",
      " precision at next top 200 : 0.675  || Without input 0.540\n",
      " precision at next top 250 : 0.592  || Without input 0.544\n",
      "label_top_k  50\n",
      " - / + 91 36\n",
      " precision at next top  50 : 0.840  || Without input 0.500\n",
      " precision at next top 100 : 0.770  || Without input 0.550\n",
      " precision at next top 150 : 0.727  || Without input 0.547\n",
      " precision at next top 200 : 0.640  || Without input 0.545\n",
      " precision at next top 250 : 0.560  || Without input 0.540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "process_v1(working_df.copy(), label_top_k=10)\n",
    "process_v1(working_df.copy(), label_top_k=20)\n",
    "process_v1(working_df.copy(), label_top_k=30)\n",
    "process_v1(working_df.copy(), label_top_k=40)\n",
    "process_v1(working_df.copy(), label_top_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_v2(df_cur, label_top_k):\n",
    "    global attr_consignee\n",
    "    global ID_COL\n",
    "    global attr_shipper\n",
    "    print('label_top_k ',label_top_k)\n",
    "    df_cur = df_cur.sort_values(by='score',ascending=False)\n",
    "    min_score = np.percentile(df_cur['score'], ANOMALY_PERCENTILE_THRESHOLD)\n",
    "    seen_ids = []\n",
    "    count = 0\n",
    "    labelled_df = None\n",
    "    idx = 0\n",
    "\n",
    "    for i,row in df_cur.iterrows():\n",
    "        _id = int(row[ID_COL])\n",
    "        idx = i\n",
    "        seen_ids.append(_id)\n",
    "\n",
    "        if row['label']==1: \n",
    "            count +=1\n",
    "            if count > label_top_k : \n",
    "                break\n",
    "\n",
    "    seen_ids = set(seen_ids)\n",
    "    labelled_df = df_cur.iloc[:idx,:]\n",
    "    unlabelled_df = df_cur.iloc[idx:,:]\n",
    "    orig_target_df = unlabelled_df.copy().reset_index(drop=True)\n",
    "\n",
    "    tmp_N = labelled_df.loc[labelled_df['label']!=1]\n",
    "    tmp_P = labelled_df.loc[labelled_df['label']==1]\n",
    "    neg_entities = tmp_N[attr_consignee].values.tolist() + tmp_N[attr_shipper].values.tolist()\n",
    "    pos_entities = tmp_P[attr_consignee].values.tolist() + tmp_P[attr_shipper].values.tolist()\n",
    "    pos_entities = list(sorted(set(pos_entities)))\n",
    "    neg_entities = list(sorted(set(neg_entities)))\n",
    "\n",
    "    print(' - / +', len(set(neg_entities)),len(set(pos_entities)))\n",
    "    query_P = (emb_array[pos_entities]).astype(np.float32)\n",
    "    D, nn_index_P = index.search(query_P, KNN)  \n",
    "    nn_index_P = nn_index_P[:,:]\n",
    "\n",
    "    query_N = (emb_array[neg_entities]).astype(np.float32)\n",
    "    D, nn_index_N = index.search(query_N, KNN)  \n",
    "    nn_index_N = nn_index_N[:,:]\n",
    "\n",
    "    candidates = unlabelled_df.loc[unlabelled_df['score']>=min_score ]\n",
    "    candidates.loc[((candidates[attr_consignee].isin(white_list)) | (candidates[attr_shipper].isin(white_list)))]           \n",
    "    # first see records where both match +\n",
    "    df1 = candidates.loc[candidates[attr_consignee].isin(pos_label_entities) & (candidates[attr_shipper].isin(pos_label_entities))]\n",
    "    df1.loc[:,'dynamic_score'] = 2\n",
    "\n",
    "    # first see records where both match -\n",
    "    df2 = candidates.loc[candidates[attr_consignee].isin(neg_entities) & (candidates[attr_shipper].isin(neg_entities))]\n",
    "    df2.loc[:,'dynamic_score'] = 0\n",
    "\n",
    "    # first see records where one matches -\n",
    "    df3 = candidates.loc[((candidates[attr_consignee].isin(white_list)) | (candidates[attr_shipper].isin(white_list)))]\n",
    "    df3.loc[:,'dynamic_score'] = 0\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # df1 at top \n",
    "    # df2 + df_3 at end \n",
    "    # We use nearest neighbor based filters in the sizeable number of records in between\n",
    "    # ------------------------------------------\n",
    "    id_list = df1[ID_COL].values.tolist() + df2[ID_COL].values.tolist() + df3[ID_COL].values.tolist()\n",
    "    id_list = list(set(id_list))\n",
    "    df4 = candidates.loc[~candidates[ID_COL].isin(id_list)]  \n",
    "    _P = nn_index_P[:,0].reshape(-1)\n",
    "    nn_index_P = nn_index_P[:,1:].reshape(-1)\n",
    "    nn_index_N = nn_index_N.reshape(-1)\n",
    "    \n",
    "    df4_0 = df4.loc[\n",
    "        ((df4[attr_consignee].isin(pos_label_entities) )&(df4[attr_shipper].isin(nn_index_P)))|\n",
    "        ((df4[attr_consignee].isin(nn_index_P) )&(df4[attr_shipper].isin(pos_label_entities)))\n",
    "    ]\n",
    "    \n",
    "    df4 =  df4_0.loc[\n",
    "        ~(df4_0[attr_consignee].isin(nn_index_N) )&(df4[attr_shipper].isin(nn_index_N))\n",
    "    ]\n",
    "    \n",
    "    df4.loc[:,'dynamic_score'] = 0.75\n",
    "    \n",
    "    id_list = df1[ID_COL].values.tolist() + df2[ID_COL].values.tolist() + df3[ID_COL].values.tolist() + df4[ID_COL].values.tolist()\n",
    "    df5 = candidates.loc[~candidates[ID_COL].isin(id_list)] \n",
    "    df0 = pd.concat([df1,df4,df5,df3,df2])\n",
    "    new_df = df0.sort_values( by = 'dynamic_score', ascending=False)\n",
    "\n",
    "    for k in [50,100,150,200,250]:\n",
    "        # without input?\n",
    "        top_k_target = orig_target_df.head(k)\n",
    "        wo_input_precison = len(top_k_target.loc[top_k_target['label']==1])/k\n",
    "        tmp =  new_df.head(k)\n",
    "        correct = tmp.loc[tmp['label']==1]\n",
    "        prec = len(correct)/len(tmp)\n",
    "        print(' precision at next top {:3d} : {:.3f}'.format(k, prec), ' || Without input {:.3f}'.format(wo_input_precison) )\n",
    "    \n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_v1 : Whitelist Blacklist  (multiple filters )\n",
    "## process_v2 : Whitelist Blascklist  + embedding based nearest neighbor  ( LSH used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_top_k  10\n",
      " - / + 20 11\n",
      " precision at next top  50 : 0.900  || Without input 0.420\n",
      " precision at next top 100 : 0.780  || Without input 0.410\n",
      " precision at next top 150 : 0.773  || Without input 0.433\n",
      " precision at next top 200 : 0.725  || Without input 0.480\n",
      " precision at next top 250 : 0.680  || Without input 0.488\n",
      "label_top_k  10\n",
      " - / + 20 11\n",
      " precision at next top  50 : 0.900  || Without input 0.420\n",
      " precision at next top 100 : 0.790  || Without input 0.410\n",
      " precision at next top 150 : 0.800  || Without input 0.433\n",
      " precision at next top 200 : 0.790  || Without input 0.480\n",
      " precision at next top 250 : 0.776  || Without input 0.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "process_v1(df_cur, label_top_k = 10)\n",
    "process_v2(df_cur, label_top_k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_top_k  20\n",
      " - / + 35 18\n",
      " precision at next top  50 : 0.860  || Without input 0.320\n",
      " precision at next top 100 : 0.780  || Without input 0.430\n",
      " precision at next top 150 : 0.767  || Without input 0.433\n",
      " precision at next top 200 : 0.690  || Without input 0.490\n",
      " precision at next top 250 : 0.644  || Without input 0.488\n",
      "label_top_k  20\n",
      " - / + 35 18\n",
      " precision at next top  50 : 0.840  || Without input 0.320\n",
      " precision at next top 100 : 0.810  || Without input 0.430\n",
      " precision at next top 150 : 0.787  || Without input 0.433\n",
      " precision at next top 200 : 0.785  || Without input 0.490\n",
      " precision at next top 250 : 0.772  || Without input 0.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "process_v1(df_cur, label_top_k = 20)\n",
    "process_v2(df_cur, label_top_k = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_top_k  30\n",
      " - / + 53 25\n",
      " precision at next top  50 : 0.860  || Without input 0.400\n",
      " precision at next top 100 : 0.800  || Without input 0.450\n",
      " precision at next top 150 : 0.740  || Without input 0.500\n",
      " precision at next top 200 : 0.670  || Without input 0.510\n",
      " precision at next top 250 : 0.612  || Without input 0.516\n",
      "label_top_k  30\n",
      " - / + 53 25\n",
      " precision at next top  50 : 0.860  || Without input 0.400\n",
      " precision at next top 100 : 0.800  || Without input 0.450\n",
      " precision at next top 150 : 0.787  || Without input 0.500\n",
      " precision at next top 200 : 0.785  || Without input 0.510\n",
      " precision at next top 250 : 0.764  || Without input 0.516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "process_v1(df_cur, label_top_k = 30)\n",
    "process_v2(df_cur, label_top_k = 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_top_k  40\n",
      " - / + 81 34\n",
      " precision at next top  50 : 0.840  || Without input 0.520\n",
      " precision at next top 100 : 0.780  || Without input 0.540\n",
      " precision at next top 150 : 0.760  || Without input 0.560\n",
      " precision at next top 200 : 0.675  || Without input 0.540\n",
      " precision at next top 250 : 0.592  || Without input 0.544\n",
      "label_top_k  40\n",
      " - / + 81 34\n",
      " precision at next top  50 : 0.880  || Without input 0.520\n",
      " precision at next top 100 : 0.840  || Without input 0.540\n",
      " precision at next top 150 : 0.820  || Without input 0.560\n",
      " precision at next top 200 : 0.785  || Without input 0.540\n",
      " precision at next top 250 : 0.776  || Without input 0.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "process_v1(df_cur, label_top_k = 40)\n",
    "process_v2(df_cur, label_top_k = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_top_k  50\n",
      " - / + 91 36\n",
      " precision at next top  50 : 0.840  || Without input 0.500\n",
      " precision at next top 100 : 0.770  || Without input 0.550\n",
      " precision at next top 150 : 0.727  || Without input 0.547\n",
      " precision at next top 200 : 0.640  || Without input 0.545\n",
      " precision at next top 250 : 0.560  || Without input 0.540\n",
      "label_top_k  50\n",
      " - / + 91 36\n",
      " precision at next top  50 : 0.840  || Without input 0.500\n",
      " precision at next top 100 : 0.830  || Without input 0.550\n",
      " precision at next top 150 : 0.807  || Without input 0.547\n",
      " precision at next top 200 : 0.795  || Without input 0.545\n",
      " precision at next top 250 : 0.772  || Without input 0.540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "process_v1(df_cur, label_top_k = 50)\n",
    "process_v2(df_cur, label_top_k = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
