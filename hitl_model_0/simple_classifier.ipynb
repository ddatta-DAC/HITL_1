{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "sys.path.append('./..')\n",
    "sys.path.append('./../..')\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "import glob \n",
    "import yaml\n",
    "import pickle\n",
    "from scipy.special import softmax as SOFTMAX\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from common_utils import utils\n",
    "try:\n",
    "    from common_utils import AD_result_fetcher\n",
    "except:\n",
    "    from .common_utils import AD_result_fetcher\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import LongTensor as LT\n",
    "from torch import FloatTensor as FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_bipartite_embeddings():\n",
    "    global DIR\n",
    "    LOC = './saved_model_data'\n",
    "    _files = sorted(glob.glob(os.path.join(LOC,DIR, '**.npy')))\n",
    "    emb_dict = {}\n",
    "    for file in _files:\n",
    "        _domain = file.split('/')[-1].split('_')[0]\n",
    "        emb_dict[_domain] = np.load(file)\n",
    "    return emb_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Convert the df from serial_ID to entity_ID\n",
    "# ---------------------------------------------\n",
    "def convert_toEntityID(target_df):\n",
    "    global bipartite_domains\n",
    "    serialID_map_df = utils.fetch_idMappingFile(DIR)\n",
    "    serialID_entityID_dict = {}\n",
    "    \n",
    "    for domain in set(bipartite_domains):\n",
    "        if domain not in target_df.columns : continue\n",
    "        tmp =  serialID_map_df.loc[(serialID_map_df['domain'] == domain)]\n",
    "        serial_id = tmp['serial_id'].values.tolist()\n",
    "        entity_id = tmp['entity_id'].values.tolist()\n",
    "        serialID_entityID_dict[domain] = {k:v for k,v in zip(serial_id,entity_id)}\n",
    "   \n",
    "    def convert_aux(val, domain):\n",
    "        return serialID_entityID_dict[domain][val]\n",
    "    domain_list = bipartite_domains\n",
    "    for domain in tqdm(domain_list):\n",
    "        target_df[domain] = target_df[domain].parallel_apply(convert_aux, args=(domain,))\n",
    "    return target_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Create synthetic mapping \n",
    "# So that ids are continuous. \n",
    "# Also embeddings can be accessed using an numpy array\n",
    "# -----------------------------  \n",
    "def convert_to_SynID (data_df, emb_dict):\n",
    "    global bipartite_domains\n",
    "    global DATA_LOC\n",
    "    global DIR\n",
    "    \n",
    "    with open(os.path.join(DATA_LOC, DIR, 'domain_dims.pkl'),'rb') as fh:\n",
    "        domain_dims = pickle.load(fh)\n",
    "        \n",
    "    synID = 0\n",
    "    cur = 0\n",
    "    col_syn_id = []\n",
    "    col_entity_id = []\n",
    "    col_domain_names = []\n",
    "    \n",
    "    # ------------------\n",
    "    for d in sorted(bipartite_domains):\n",
    "        s = domain_dims[d]\n",
    "        col_entity_id.extend(list(range(s)))\n",
    "        col_domain_names.extend([d for _ in range(s)])\n",
    "        tmp = np.arange(s) + cur\n",
    "        tmp = tmp.tolist()\n",
    "        col_syn_id.extend(tmp)\n",
    "        cur += s\n",
    "\n",
    "    data = {'domain': col_domain_names, 'entity_id': col_entity_id, 'syn_id': col_syn_id}\n",
    "    synID_mapping_df = pd.DataFrame(data)\n",
    "\n",
    "    # -------------------\n",
    "    # Replace entity_id with synthetic id \n",
    "    # -------------------\n",
    "    mapping_dict = {}\n",
    "    for domain in sorted(set(synID_mapping_df['domain'])):\n",
    "        tmp =  synID_mapping_df.loc[(synID_mapping_df['domain'] == domain)]\n",
    "        syn_id = tmp['syn_id'].values.tolist()\n",
    "        entity_id = tmp['entity_id'].values.tolist()\n",
    "        mapping_dict[domain] = { k:v for k,v in zip(entity_id,syn_id) }\n",
    "        def convert_aux(val, domain):\n",
    "            return mapping_dict[domain][val]\n",
    "\n",
    "    for domain in tqdm(bipartite_domains):\n",
    "        data_df[domain] = data_df[domain].parallel_apply(convert_aux, args=(domain,))\n",
    "    \n",
    "    num_entities = len(synID_mapping_df)\n",
    "    emb_array = np.zeros( [num_entities, emb_dict[bipartite_domains[0]].shape[-1]])\n",
    "    \n",
    "    for dom in bipartite_domains:\n",
    "        tmp = synID_mapping_df.loc[synID_mapping_df['domain']==dom]\n",
    "        synID = tmp['syn_id'].values\n",
    "        entityID = tmp['entity_id'].values\n",
    "        emb_array[synID] = emb_dict[dom][entityID]\n",
    "        \n",
    "    return data_df,  emb_array , synID_mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'config.yaml'\n",
    "with open(config_file,'r') as fh:\n",
    "    CONFIG = yaml.safe_load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "attr_consignee = CONFIG['attribute_SHIPPER']\n",
    "attr_shipper = CONFIG['attribute_CONSIGNEE']\n",
    "ID_COL = CONFIG['ID_COL']\n",
    "DATA_LOC = CONFIG['DATA_LOC']\n",
    "bipartite_domains = sorted([attr_consignee, attr_shipper])\n",
    "NEG_PERCENTILE_THRESHOLD = CONFIG['NEG_PERCENTILE_THRESHOLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22648 22648\n",
      "95 0.5650949312585849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.72s/it]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.68s/it]\n"
     ]
    }
   ],
   "source": [
    "DIR = 'us_import1'\n",
    "\n",
    "labelled_results = AD_result_fetcher.read_in_AD_result(DIR)\n",
    "ANOMALY_PERCENTILE_THRESHOLD  = CONFIG['ANOMALY_PERCENTILE_THRESHOLD']\n",
    "score_threshold = np.percentile(labelled_results['score'],CONFIG['ANOMALY_PERCENTILE_THRESHOLD'])\n",
    "print(CONFIG['ANOMALY_PERCENTILE_THRESHOLD'], score_threshold)\n",
    "bipartite_embeddings = obtain_bipartite_embeddings()\n",
    "\n",
    "main_df = (labelled_results[[ID_COL , 'label', 'score'] + bipartite_domains]).copy()\n",
    "df1 = convert_toEntityID(main_df.copy())\n",
    "data_df, emb_array , synID_mapping_df = convert_to_SynID (df1.copy(), bipartite_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = data_df.copy()\n",
    "working_df['dynamic_score'] = working_df['score'].values\n",
    "# ------------------\n",
    "# Type conversion : to ensure no bugs\n",
    "# ------------------\n",
    "working_df['PanjivaRecordID'] = working_df['PanjivaRecordID'].astype(int)\n",
    "working_df['ConsigneePanjivaID'] = working_df['ConsigneePanjivaID'].astype(int)\n",
    "working_df['ShipperPanjivaID'] = working_df['ShipperPanjivaID'].astype(int)\n",
    "working_df = working_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_clf(nn.Module):\n",
    "    def __init__(self, emb):\n",
    "        super(mlp_clf, self).__init__()\n",
    "        print(emb)\n",
    "        emb_dim = emb.shape[-1]\n",
    "        self.embedding = nn.Embedding.from_pretrained(FT(emb))\n",
    "        self.fc = nn.Sequential(\n",
    "            self.embedding,\n",
    "            nn.Linear(emb_dim, emb_dim//2),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(emb_dim//2, emb_dim//4),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(emb_dim//4,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.fc(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_container():\n",
    "    def __init__(self,emb):\n",
    "        self.clf_obj = mlp_clf(emb) \n",
    "    \n",
    "    def train(\n",
    "        self, \n",
    "        x_pos, \n",
    "        x_neg,\n",
    "        batch_size = 64,\n",
    "        epochs = 10000,\n",
    "        log_interval = 200\n",
    "    ):\n",
    "        \n",
    "        self.clf_obj.train()\n",
    "        x_pos = np.array(list(x_pos)).reshape([-1,1])\n",
    "        x_neg = np.array(list(x_neg)).reshape([-1,1])\n",
    "        opt = torch.optim.Adam(list(self.clf_obj.parameters()))\n",
    "        for e in range(epochs):\n",
    "            opt.zero_grad()\n",
    "            idx_p = np.random.choice(np.arange(x_pos.shape[0]),batch_size,replace=True)\n",
    "            _x_p = x_pos[idx_p]\n",
    "            y_p = np.ones(_x_p.shape[0])\n",
    "            \n",
    "            idx_n = np.random.choice(np.arange(x_neg.shape[0]),batch_size,replace=True)\n",
    "            _x_n = x_neg[idx_n]\n",
    "            \n",
    "            y_n = np.zeros(_x_n.shape[0])\n",
    "            _y = FT(np.vstack([y_p,y_n]))\n",
    "            _x = LT(np.vstack([_x_p,_x_n]))\n",
    "            pred = self.clf_obj(_x)\n",
    "            \n",
    "            _loss = F.binary_cross_entropy(pred,_y,reduction='none')\n",
    "            _loss = torch.mean(_loss)\n",
    "            _loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            if e%log_interval == 0 :\n",
    "                print('Epoch {} Loss {}'.format(e, np.mean(_loss.cpu().data.numpy())))\n",
    "        self.clf_obj.eval()\n",
    "    \n",
    "    def score(self, x):\n",
    "        \n",
    "        self.clf_obj.eval()\n",
    "        return self.clf_obj(LT(x)).cpu().data.numpy()\n",
    "            \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df_cur, emb_array, label_top_k = 5,  mlp = None,) :   \n",
    "    global ID_COL\n",
    "    global attr_consignee\n",
    "    global attr_shipper\n",
    "    global ANOMALY_PERCENTILE_THRESHOLD\n",
    "    global NEG_PERCENTILE_THRESHOLD\n",
    "    global bipartite_domains\n",
    "    df_cur = df_cur.sort_values(by='score',ascending=False)\n",
    "    min_score = np.percentile(df_cur['score'], ANOMALY_PERCENTILE_THRESHOLD)\n",
    "\n",
    "    seen_ids = []\n",
    "    count = 0\n",
    "    labelled_df = None\n",
    "    idx = 0\n",
    "\n",
    "    for i,row in df_cur.iterrows():\n",
    "        _id = int(row[ID_COL])\n",
    "        idx = i\n",
    "        seen_ids.append(_id)\n",
    "\n",
    "        if row['label']==1: \n",
    "            count +=1\n",
    "            if count > label_top_k : \n",
    "                break\n",
    "        \n",
    "    seen_ids = set(seen_ids)\n",
    "    labelled_df = df_cur.iloc[:idx,:]\n",
    "    unlabelled_df = df_cur.iloc[idx:,:]\n",
    "    orig_target_df = unlabelled_df.copy().reset_index(drop=True)\n",
    "    \n",
    "    print('# of records revealed to find {} instances at the top :: {}'.format(\n",
    "        label_top_k, \n",
    "        len(labelled_df))\n",
    "    )\n",
    "    labelled_df_n = labelled_df.loc[labelled_df['label']==-1]\n",
    "    labelled_df_p = labelled_df.loc[labelled_df['label']==1]\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # Assume labels to be 0 for entities in records in lower 50 percentile\n",
    "    # ------------------------------------------------\n",
    "    neg_label_cutoff_score = np.percentile(df_cur['score'], NEG_PERCENTILE_THRESHOLD)\n",
    "\n",
    "    tmp = unlabelled_df.loc[unlabelled_df['score'] < neg_label_cutoff_score]\n",
    "    neg_label_entities = []\n",
    "    \n",
    "    for domain in bipartite_domains:\n",
    "        neg_label_entities.extend(tmp[domain].values.tolist())\n",
    "        neg_label_entities.extend(labelled_df_n[domain].values.tolist())\n",
    "        \n",
    "    neg_label_entities = set(neg_label_entities)\n",
    "    \n",
    "    pos_label_entities = []\n",
    "    for domain in bipartite_domains: \n",
    "        pos_label_entities.extend(labelled_df_n[domain].values.tolist())\n",
    "    \n",
    "    pos_label_entities = set(pos_label_entities)\n",
    "    \n",
    "    neg_label_entities = neg_label_entities.difference(pos_label_entities)\n",
    "    print(len(pos_label_entities), len(neg_label_entities))\n",
    "    \n",
    "    pos_label_entities = list(set(pos_label_entities))\n",
    "    neg_label_entities = list(set(neg_label_entities))\n",
    "    # ---------------------------------\n",
    "    # Build a classifier \n",
    "    # ---------------------------------\n",
    "    if mlp is None:\n",
    "        mlp = mlp_container(emb_array)\n",
    "        mlp.train(\n",
    "            pos_label_entities,\n",
    "            neg_label_entities\n",
    "        )\n",
    "    print(pos_label_entities[0])\n",
    "    # ---------------------------------\n",
    "    # score entities \n",
    "    num_entities = emb_array.shape[0]\n",
    "    clf_scores = mlp.score(np.arange(num_entities))\n",
    "    print(clf_scores.shape)\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Now for records with score higher than threshold \n",
    "    # Calculate a new score for each record \n",
    "    # ------------------------------\n",
    "    candidates = unlabelled_df.loc[unlabelled_df['score']>=min_score ]\n",
    "    \n",
    "    def aux_score(row):\n",
    "        s = 0\n",
    "        if int(row[attr_consignee]) in pos_label_entities:\n",
    "            s +=1\n",
    "        else:\n",
    "            s += clf_scores[int(row[attr_consignee])]\n",
    "        \n",
    "        if int(row[attr_shipper]) in pos_label_entities: \n",
    "            s +=1\n",
    "        else:\n",
    "            s += clf_scores[int(row[attr_shipper])]\n",
    "        s = s/2\n",
    "        s = row['score'] + s\n",
    "        return s\n",
    "            \n",
    "    # first see records where both match\n",
    "    df1 = candidates.loc[candidates[attr_consignee].isin(pos_label_entities) & (candidates[attr_shipper].isin(pos_label_entities))]\n",
    "    df1['dynamic_score'] = 2\n",
    "    df2 = candidates.loc[~candidates[ID_COL].isin(df1[ID_COL])]\n",
    "    df2['dynamic_score'] = df2.parallel_apply(aux_score,axis=1)\n",
    "    \n",
    "    new_df = df1.append(df2,ignore_index=True)\n",
    "    new_df = new_df.sort_values( by = 'dynamic_score', ascending=False)\n",
    "    \n",
    "    for k in [50,100,150,200,250]:\n",
    "        # without input?\n",
    "        top_k_target = orig_target_df.head(k)\n",
    "        wo_input_precison = len(top_k_target.loc[top_k_target['label']==1])/k\n",
    "        tmp =  new_df.head(k)\n",
    "        correct = tmp.loc[tmp['label']==1]\n",
    "        prec = len(correct)/len(tmp)\n",
    "        print(' precision at next top {:3d} : {:.3f}'.format(k, prec), ' || Without input {:.3f}'.format(wo_input_precison) )\n",
    "\n",
    "    return mlp\n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10012, 32)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of records revealed to find 5 instances at the top :: 11\n",
      "12 4061\n",
      "2048\n",
      "(10012, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/ipykernel_launcher.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " precision at next top  50 : 0.020  || Without input 0.460\n",
      " precision at next top 100 : 0.210  || Without input 0.410\n",
      " precision at next top 150 : 0.347  || Without input 0.433\n",
      " precision at next top 200 : 0.355  || Without input 0.460\n",
      " precision at next top 250 : 0.324  || Without input 0.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/ipykernel_launcher.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "mlp = process(working_df.copy(),emb_array, 5, mlp)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entities = emb_array.shape[1]\n",
    "scores = mlp.score(np.arange(num_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x7f15e4548fd0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f15e4548dd0>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7f15e44fe210>,\n",
       "  <matplotlib.lines.Line2D at 0x7f15e44fe6d0>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x7f15e45488d0>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7f15e44fe110>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x7f15e4520090>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of records revealed to find 5 instances at the top :: 11\n",
      "# of records revealed to find 10 instances at the top :: 24\n",
      "# of records revealed to find 20 instances at the top :: 44\n",
      "# of records revealed to find 30 instances at the top :: 73\n",
      "# of records revealed to find 40 instances at the top :: 105\n",
      "# of records revealed to find 50 instances at the top :: 123\n"
     ]
    }
   ],
   "source": [
    "process(working_df.copy(),10)  \n",
    "process(working_df.copy(),20)  \n",
    "process(working_df.copy(),30)   \n",
    "process(working_df.copy(),40)   \n",
    "process(working_df.copy(),50)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
