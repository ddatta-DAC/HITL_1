{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "/home/ddatta/anaconda3/envs/graph1/lib/python3.7/site-packages/dgl/base.py:45: DGLWarning: Detected an old version of PyTorch. Suggest using torch>=1.5.0 for the best experience.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./../..')\n",
    "sys.path.append('./..')\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('./..')\n",
    "sys.path.append('./../..')\n",
    "from pathlib import Path\n",
    "import os\n",
    "from common_utils import utils\n",
    "from joblib import Parallel,delayed\n",
    "import multiprocessing as mp\n",
    "import pickle \n",
    "from torch import LongTensor as LT\n",
    "from torch import FloatTensor as FT\n",
    "from torch.nn import functional as F\n",
    "import dgl.function as fn\n",
    "from torch import nn\n",
    "DIR = 'us_import1'\n",
    "# Use training data for graph creation\n",
    "DATA_LOC = './../generated_data_v1/{}'.format(DIR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(DATA_LOC,'domain_dims.pkl'),'rb') as fh:\n",
    "    domain_dims = pickle.load(fh) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# First create a bipartite graph using training data  \n",
    "# 2 entity types : Consignee and Shipper\n",
    "# Prefixes : 'Consignee' 'Shipper'\n",
    "# -----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "attr_consignee_prefix = 'ConsigneePanjivaID'\n",
    "attr_shipper_prefix = 'ShipperPanjivaID'\n",
    "bipartite_domains = sorted([attr_consignee_prefix, attr_shipper_prefix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_df():\n",
    "    global attr_consignee_prefix\n",
    "    global attr_shipper_prefix\n",
    "    global DATA_LOC\n",
    "    global bipartite_domains\n",
    "    global DIR\n",
    "    \n",
    "    with open(os.path.join(DATA_LOC,'domain_dims.pkl'),'rb') as fh:\n",
    "        domain_dims = pickle.load(fh)\n",
    "        \n",
    "    # This data does not have serial ID but domain specific ID\n",
    "    data_df = pd.read_csv(os.path.join(DATA_LOC, 'train_data.csv'),index_col=None, low_memory=False)\n",
    "    data_df = data_df.drop_duplicates(subset=list(domain_dims.keys()))\n",
    "    # group by\n",
    "    g_df = data_df.groupby([attr_consignee_prefix, attr_shipper_prefix]).size().reset_index(name='weight')\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Create synthetic mappiing \n",
    "    # -----------------------------\n",
    "  \n",
    "    \n",
    "    synID = 0\n",
    "    cur = 0\n",
    "    col_syn_id = []\n",
    "    col_entity_id = []\n",
    "    col_domain_names = []\n",
    "    # ------------------\n",
    "    for d in sorted(bipartite_domains):\n",
    "        s = domain_dims[d]\n",
    "        col_entity_id.extend(list(range(s)))\n",
    "        col_domain_names.extend([d for _ in range(s)])\n",
    "        tmp = np.arange(s) + cur\n",
    "        tmp = tmp.tolist()\n",
    "        col_syn_id.extend(tmp)\n",
    "        cur += s\n",
    "\n",
    "    data = {'domain': col_domain_names, 'entity_id': col_entity_id, 'syn_id': col_syn_id}\n",
    "    synID_mapping_df = pd.DataFrame(data)\n",
    "    \n",
    "    # -------------------\n",
    "    # Replace entity_id with synthetic id \n",
    "    # -------------------\n",
    "    mapping_dict = {}\n",
    "    for domain in set(synID_mapping_df['domain']):\n",
    "        tmp =  synID_mapping_df.loc[(synID_mapping_df['domain'] == domain)]\n",
    "        syn_id = tmp['syn_id'].values.tolist()\n",
    "        entity_id = tmp['entity_id'].values.tolist()\n",
    "        mapping_dict[domain] = {k:v for k,v in zip(entity_id,syn_id)}\n",
    "        \n",
    "    def convert_aux(val, domain):\n",
    "        return mapping_dict[domain][val]\n",
    "\n",
    "    for domain in tqdm(bipartite_domains):\n",
    "        g_df[domain] = g_df[domain].parallel_apply(convert_aux, args=(domain,))\n",
    "        \n",
    "        \n",
    "    return g_df, synID_mapping_df, data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "g_df, synID_mapping_df, data_df = get_data_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ConsigneePanjivaID</th>\n",
       "      <th>ShipperPanjivaID</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4947</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5336</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5340</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5943</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>7339</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>7552</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>8142</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>8754</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>5267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>6015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ConsigneePanjivaID  ShipperPanjivaID  weight\n",
       "0                   0              4947       1\n",
       "1                   0              5336       6\n",
       "2                   0              5340      17\n",
       "3                   0              5943       7\n",
       "4                   0              7339       2\n",
       "5                   0              7552       2\n",
       "6                   0              8142       6\n",
       "7                   0              8754      14\n",
       "8                   1              5267       1\n",
       "9                   1              6015       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Read In metapath2vec features  \n",
    "# Those are stored as numpy arrays\n",
    "# ---------------------------------.\n",
    "mp2v_features = {}\n",
    "MP2V_features_LOC = os.path.join('./../createGraph_trade/saved_model_data', DIR)\n",
    "for attr in bipartite_domains:\n",
    "    file = os.path.join(MP2V_features_LOC, 'mp2v_{}.npy'.format(attr))\n",
    "    mp2v_features[attr] = np.load(file)\n",
    "    \n",
    "'''\n",
    "To feed data into DGL graph \n",
    "Create tensor of features\n",
    "'''\n",
    "input_emb_size = list(mp2v_features.values())[0].shape[1]\n",
    "num_entities = len(synID_mapping_df)\n",
    "input_features = np.zeros([num_entities, input_emb_size])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in bipartite_domains:\n",
    "    tmp = synID_mapping_df.loc[synID_mapping_df['domain']==d]\n",
    "    _syn_id = tmp['syn_id'].values.tolist()\n",
    "    _entity_id = tmp['entity_id'].values.tolist()\n",
    "    input_features[_syn_id] = mp2v_features[d][_entity_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52414888, -0.45322031,  1.91028905, ...,  1.29858494,\n",
       "         2.42626166,  2.1162219 ],\n",
       "       [ 0.06493711, -1.25776231,  2.04438019, ..., -0.46997485,\n",
       "         1.6251533 , -0.40729418],\n",
       "       [-0.8736642 ,  0.93814248, -0.60021335, ...,  0.48887551,\n",
       "        -0.15444292,  0.80477917],\n",
       "       ...,\n",
       "       [-0.90004742,  1.25017679,  1.40393507, ..., -0.14855796,\n",
       "         2.29815888, -0.50512475],\n",
       "       [-2.80783772,  4.58135986, -0.11078735, ..., -1.54375505,\n",
       "         1.16891015,  0.63428003],\n",
       "       [-0.71271372,  1.44254029,  0.34384501, ..., -0.98573357,\n",
       "         1.17151284, -0.87737447]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = g_df[attr_consignee_prefix].values.tolist()\n",
    "dst =  g_df[attr_shipper_prefix].values.tolist()\n",
    "\n",
    "nodes_src = src + dst\n",
    "nodes_dst = dst + src\n",
    "weights = g_df['weight'].values.tolist() +  g_df['weight'].values.tolist()\n",
    "# DGL is not undirected by default \n",
    "graph_obj = dgl.graph((nodes_src,nodes_dst))\n",
    "graph_obj.edata['weight'] = FT(weights)\n",
    "graph_obj.ndata['mp2v'] = FT(input_features) \n",
    "\n",
    "\n",
    "# graph_obj = dgl.to_bidirected(graph_obj, copy_ndata=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mp2v': tensor([[ 0.5241, -0.4532,  1.9103,  ...,  1.2986,  2.4263,  2.1162],\n",
       "        [ 0.0649, -1.2578,  2.0444,  ..., -0.4700,  1.6252, -0.4073],\n",
       "        [-0.8737,  0.9381, -0.6002,  ...,  0.4889, -0.1544,  0.8048],\n",
       "        ...,\n",
       "        [-0.6626,  0.8059, -1.8143,  ...,  0.8304, -0.6618,  1.1085],\n",
       "        [-0.9119,  1.6221, -0.9209,  ..., -0.3967,  0.4786, -0.1086],\n",
       "        [-0.8543, -0.6464, -2.5652,  ..., -0.3752,  0.0170, -0.8392]])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_obj.ndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "\n",
    "# ------------------------------\n",
    "'''\n",
    "Core GNN \n",
    "'''\n",
    "\n",
    "class GNN_1(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inp_dim,\n",
    "        out_dim,\n",
    "        input_feature_label='mp2v', \n",
    "        aggregator_type='mean',\n",
    "        bias=True,\n",
    "        norm=True,\n",
    "        activation=None\n",
    "    ):\n",
    "        super(GNN_1, self).__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.inp_dim = inp_dim\n",
    "        self.aggregator_type = aggregator_type\n",
    "        self.norm = norm\n",
    "        self.activation = activation\n",
    "        # ------\n",
    "        # Assuming a 2 layer GNN\n",
    "        # ------\n",
    "        self.num_layers = 2 \n",
    "        self.input_feature_label= input_feature_label\n",
    "        self.FC_w = nn.Linear(\n",
    "            self.inp_dim * 3, \n",
    "            self.out_dim, \n",
    "            bias=bias\n",
    "        )\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.aggregator_type = aggregator_type\n",
    "        return\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_uniform_(\n",
    "            self.FC_w.weight, \n",
    "            gain=gain\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def forward(self, graph):   \n",
    "        \n",
    "        for layer in range( self.num_layers ):\n",
    "            \n",
    "            # --------------------------------------\n",
    "            # If first layer store the result from h\n",
    "            # --------------------------------------\n",
    "            if layer == 0 :\n",
    "                print('Updating Layer 0')\n",
    "                # For the 1st layer initilize features with input feature\n",
    "                graph_obj.ndata['features'] = graph_obj.ndata[self.input_feature_label]\n",
    "                graph_obj.update_all(\n",
    "                    fn.copy_u('features', 'm'),\n",
    "                    fn.mean('m', 'h')\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                print('Updating Layer 1 a')\n",
    "                # ------------------------------\n",
    "                # Update along the given edge\n",
    "                # ------------------------------\n",
    "                graph_obj.update_all(\n",
    "                    fn.copy_u('features', 'm'),\n",
    "                    fn.mean('m', 'h')\n",
    "                )\n",
    "                print('Updating Layer 1b')\n",
    "                concat_ftrs = torch.cat([\n",
    "                    graph_obj.ndata['features'],\n",
    "                    graph_obj.ndata['h'],\n",
    "                    graph_obj.ndata[self.input_feature_label]\n",
    "                    ], \n",
    "                    dim=1\n",
    "                )\n",
    "                \n",
    "                ftrs = self.FC_w(concat_ftrs)\n",
    "                ftrs = F.tanh(ftrs)\n",
    "                if self.norm:\n",
    "                    ftrs = ftrs/torch.norm(ftrs, p=2)\n",
    "                graph_obj.ndata['features'] = ftrs \n",
    "        return\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_obj = GNN_1(\n",
    "    inp_dim = 128,\n",
    "    out_dim = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Triplet loss for training the GNN\n",
    "# --------------------------------------\n",
    "def triplet_loss( xt, xp, xn):\n",
    "    m = 0.25\n",
    "    v1 = 1 - F.cosine_similarity(xt, xp, dim=1)\n",
    "    v2 = 1 - F.cosine_similarity(xt, xn, dim=1)\n",
    "    d =  v1 - v2 + m\n",
    "    loss = torch.max(d, torch.zeros_like(d))\n",
    "    return torch.mean(loss, dim=0, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pos_neg_neighbors(\n",
    "    graph_obj\n",
    "):\n",
    " \n",
    "    # The source nodes \n",
    "    nodes = graph_obj.nodes().data.numpy()\n",
    "    edges = graph_obj.edges()\n",
    "    node_src = edges[0].data.numpy()\n",
    "    node_dest = edges[1].data.numpy()\n",
    "    \n",
    "    pos_nbrs = { _ : [] for _ in nodes }\n",
    "    neg_nbrs = { _ : [] for _ in nodes }\n",
    "    \n",
    "    for i,j  in zip(node_src, node_dest):\n",
    "        pos_nbrs[i].append(j)\n",
    "    \n",
    "    # Stor the list of neighbors as values in the dictionary\n",
    "    neg_nbr_dict = {}\n",
    "    pos_nbr_dict = {}\n",
    "    print(pos_nbrs[100])\n",
    " \n",
    "\n",
    "    for node in nodes:\n",
    "        pos_nbrs[node] = set(pos_nbrs[node])\n",
    "        neg_nbrs[node] = list(set(nodes).difference(pos_nbrs[node]))\n",
    "       \n",
    "        neg_nbr_dict[node] = list(neg_nbrs[node])\n",
    "        pos_nbr_dict[node] = list(pos_nbrs[node])\n",
    "        \n",
    "    return pos_nbr_dict,neg_nbr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_triplets(pos_nbr_dict,neg_nbr_dict):\n",
    "    nodes = list(pos_nbr_dict.keys())\n",
    "    pos_pairs = []\n",
    "    neg_pairs = []\n",
    "    triplets = []\n",
    "    for n in nodes:\n",
    "        t = [ n, \n",
    "            np.random.choice(pos_nbr_dict[n],size=1)[0], \n",
    "            np.random.choice(neg_nbr_dict[n],size=1)[0]\n",
    "        ]\n",
    "        triplets.append(t)\n",
    "    triplets = np.array(triplets)\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0, ..., 10010, 10010, 10011])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_obj.edges()[0].data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8565, 8667]\n"
     ]
    }
   ],
   "source": [
    "pos_nbr_dict,neg_nbr_dict = generate_pos_neg_neighbors(graph_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = generate_training_triplets(pos_nbr_dict,neg_nbr_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5241, -0.4532,  1.9103,  ...,  1.2986,  2.4263,  2.1162],\n",
       "        [ 0.0649, -1.2578,  2.0444,  ..., -0.4700,  1.6252, -0.4073],\n",
       "        [-0.8737,  0.9381, -0.6002,  ...,  0.4889, -0.1544,  0.8048],\n",
       "        ...,\n",
       "        [-0.6626,  0.8059, -1.8143,  ...,  0.8304, -0.6618,  1.1085],\n",
       "        [-0.9119,  1.6221, -0.9209,  ..., -0.3967,  0.4786, -0.1086],\n",
       "        [-0.8543, -0.6464, -2.5652,  ..., -0.3752,  0.0170, -0.8392]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_obj.ndata['mp2v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    graph_obj, \n",
    "    gnn_obj,\n",
    "    pos_nbr_dict,\n",
    "    neg_nbr_dict,\n",
    "    num_epochs = 10\n",
    "):\n",
    "    opt = torch.optim.Adam(\n",
    "        list(gnn_obj.parameters())\n",
    "    )\n",
    "    \n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    for e in pbar :\n",
    "        opt.zero_grad()\n",
    "        triplets = generate_training_triplets(pos_nbr_dict,neg_nbr_dict)\n",
    "       \n",
    "        gnn_obj(graph_obj)\n",
    "        idx_t = LT(triplets[:,0])\n",
    "        emb_t = graph_obj.ndata['features'][idx_t,:] \n",
    "       \n",
    "        idx_p = LT(triplets[:,1])\n",
    "        idx_n = LT(triplets[:,2])\n",
    "        emb_p = graph_obj.ndata['features'][idx_p,:] \n",
    "        emb_n = graph_obj.ndata['features'][idx_n,:] \n",
    "        loss_val = triplet_loss(emb_t, emb_p, emb_n)\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "#         print(' Epoch {} Loss {:4f}'.format(e,np.mean(loss_val.data.numpy())))\n",
    "        pbar.set_postfix({'Loss': '{:4f}'.format(np.mean(loss_val.data.numpy()))})\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:12<01:50, 12.26s/it, Loss=0.196880]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Layer 0\n",
      "Updating Layer 1 a\n",
      "Updating Layer 1b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:24<01:38, 12.34s/it, Loss=0.186077]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Layer 0\n",
      "Updating Layer 1 a\n",
      "Updating Layer 1b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:37<01:26, 12.35s/it, Loss=0.172316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Layer 0\n",
      "Updating Layer 1 a\n",
      "Updating Layer 1b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:49<01:14, 12.37s/it, Loss=0.158656]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Layer 0\n",
      "Updating Layer 1 a\n",
      "Updating Layer 1b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:02<01:01, 12.39s/it, Loss=0.147629]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Layer 0\n",
      "Updating Layer 1 a\n",
      "Updating Layer 1b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:14<00:49, 12.37s/it, Loss=0.137277]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Layer 0\n",
      "Updating Layer 1 a\n",
      "Updating Layer 1b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:26<00:37, 12.41s/it, Loss=0.132092]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Layer 0\n",
      "Updating Layer 1 a\n",
      "Updating Layer 1b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:39<00:24, 12.39s/it, Loss=0.123624]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Layer 0\n",
      "Updating Layer 1 a\n",
      "Updating Layer 1b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:52<00:12, 12.70s/it, Loss=0.114758]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Layer 0\n",
      "Updating Layer 1 a\n",
      "Updating Layer 1b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:04<00:00, 12.49s/it, Loss=0.108822]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Layer 0\n",
      "Updating Layer 1 a\n",
      "Updating Layer 1b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    graph_obj, \n",
    "    gnn_obj,\n",
    "    pos_nbr_dict,\n",
    "    neg_nbr_dict,\n",
    "    num_epochs = 10 \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
