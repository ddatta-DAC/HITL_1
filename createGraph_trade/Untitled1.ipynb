{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 363)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m363\u001b[0m\n\u001b[0;31m    return graph_data, edge_weights\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('./..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "from dgl.data.utils import save_graphs\n",
    "import pickle\n",
    "from dgl.data.utils import load_graphs\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import FloatTensor as FT\n",
    "from torch import LongTensor as LT\n",
    "import torch\n",
    "import dgl.function as fn\n",
    "from torch import FloatTensor as FT\n",
    "from torch import LongTensor as LT\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "import argparse\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import multiprocessing as mp\n",
    "cpu_count = mp.cpu_count()\n",
    "print('CPU count',cpu_count)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ----------------------------GLOALS-------------------------\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "\n",
    "MODEL_SAVE_DATA_LOC = None\n",
    "DIR = None\n",
    "W2V_EPOCHS = 100\n",
    "MODEL_SAVE_DATA_LOC = None\n",
    "SOURCE_DATA_LOC = './../generated_data_v1/{{}}/stage_graph'\n",
    "# -----------------------------------------------------------\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "\n",
    "def read_graph_data(DIR):\n",
    "    global SOURCE_DATA_LOC\n",
    "    loc = SOURCE_DATA_LOC.replace('{{}}', DIR)    \n",
    "    fname_e = 'edges.csv'\n",
    "    fname_n = 'nodes.csv'\n",
    "    \n",
    "    df_e = pd.read_csv(os.path.join(loc,fname_e),low_memory=False,index_col=None)\n",
    "    df_n = pd.read_csv(os.path.join(loc,fname_n),low_memory=False,index_col=None)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # replace the node id by synthetic id \n",
    "    # ----------------------------------------\n",
    "\n",
    "\n",
    "    print('Types of edges', set(df_e['e_type']))\n",
    "    edge_weights = {}\n",
    "    graph_data = {}\n",
    "    \n",
    "    for et in set(df_e['e_type']):\n",
    "        s,t = et.split('_')\n",
    "        et_R = '_'.join([t,s])\n",
    "        tmp = df_e.loc[df_e['e_type']==et]\n",
    "        n1 = tmp['source'].values.tolist()\n",
    "        n2 = tmp['target'].values.tolist()\n",
    "        weights = tmp['weight'].values.tolist()\n",
    "        _list = []\n",
    "        _list_R = []\n",
    "        \n",
    "        print(et, et_R)\n",
    "        for i,j in zip(n1,n2):\n",
    "            _list.append((i, j))\n",
    "            _list_R.append((j, i))\n",
    "        graph_data [(s, et, t)] = _list \n",
    "        graph_data [(t, et_R, s)] = _list_R  import dgl\n",
    "import torch as th\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('./..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "from dgl.data.utils import save_graphs\n",
    "import pickle\n",
    "from dgl.data.utils import load_graphs\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import FloatTensor as FT\n",
    "from torch import LongTensor as LT\n",
    "import torch\n",
    "import dgl.function as fn\n",
    "from torch import FloatTensor as FT\n",
    "from torch import LongTensor as LT\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "import argparse\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import multiprocessing as mp\n",
    "cpu_count = mp.cpu_count()\n",
    "print('CPU count',cpu_count)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ----------------------------GLOALS-------------------------\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "\n",
    "MODEL_SAVE_DATA_LOC = None\n",
    "DIR = None\n",
    "W2V_EPOCHS = 100\n",
    "MODEL_SAVE_DATA_LOC = None\n",
    "SOURCE_DATA_LOC = './../generated_data_v1/{{}}/stage_graph'\n",
    "# -----------------------------------------------------------\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "\n",
    "def read_graph_data(DIR):\n",
    "    global SOURCE_DATA_LOC\n",
    "    loc = SOURCE_DATA_LOC.replace('{{}}', DIR)    \n",
    "    fname_e = 'edges.csv'\n",
    "    fname_n = 'nodes.csv'\n",
    "    \n",
    "    df_e = pd.read_csv(os.path.join(loc,fname_e),low_memory=False,index_col=None)\n",
    "    df_n = pd.read_csv(os.path.join(loc,fname_n),low_memory=False,index_col=None)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # replace the node id by synthetic id \n",
    "    # ----------------------------------------\n",
    "\n",
    "\n",
    "    print('Types of edges', set(df_e['e_type']))\n",
    "    edge_weights = {}\n",
    "    graph_data = {}\n",
    "    \n",
    "    for et in set(df_e['e_type']):\n",
    "        s,t = et.split('_')\n",
    "        et_R = '_'.join([t,s])\n",
    "        tmp = df_e.loc[df_e['e_type']==et]\n",
    "        n1 = tmp['source'].values.tolist()\n",
    "        n2 = tmp['target'].values.tolist()\n",
    "        weights = tmp['weight'].values.tolist()\n",
    "        _list = []\n",
    "        _list_R = []\n",
    "        \n",
    "        print(et, et_R)\n",
    "        for i,j in zip(n1,n2):\n",
    "            _list.append((i, j))\n",
    "            _list_R.append((j, i))\n",
    "        graph_data [(s, et, t)] = _list \n",
    "        graph_data [(t, et_R, s)] = _list_R  \n",
    "        edge_weights[et] = weights\n",
    "        edge_weights[et_R] = weights\n",
    "        \n",
    "    return graph_data, edge_weights\n",
    "\n",
    "class loss_callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subed = 0\n",
    "            \n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
    "        self.epoch += 1\n",
    "# -----------------------------------------------------------\n",
    "# Metapath 2 vev\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def mp2vec(random_walks, epochs = 100):\n",
    "    cpu_count = mp.cpu_count()\n",
    "    model = Word2Vec(\n",
    "        random_walks, \n",
    "        size=128, \n",
    "        window=3, \n",
    "        negative = 10,\n",
    "        hs = 0,\n",
    "        min_count=1, \n",
    "        iter = epochs,\n",
    "        compute_loss=True,\n",
    "        null_word= '-1',\n",
    "        callbacks = [loss_callback()]\n",
    "    )\n",
    "    return model\n",
    "# -----------------------------------------------------------\n",
    "# Get metapaths\n",
    "# -----------------------------------------------------------\n",
    "def get_mp_list(graph_obj, multiplier=2):\n",
    "    with open('metapaths.txt', 'r') as fh:\n",
    "        mp_list = fh.readlines()\n",
    "        \n",
    "    mp_edges = []\n",
    "    mp_list = [_.strip('\\n') for _ in mp_list]\n",
    "    mp_list = [_.strip(' ') for _ in mp_list]\n",
    "    for mp in mp_list:\n",
    "        mp = mp.split(',')\n",
    "        _mp = list(mp)\n",
    "        _mp.reverse()\n",
    "        mp = mp + _mp[1:]\n",
    "        e_list = []\n",
    "        for i in range(len(mp)-1):\n",
    "            e = mp[i] + '_' + mp[i+1]\n",
    "            e_list.append(e)\n",
    "            if e not in graph_obj.etypes:\n",
    "                print('ERROR!!')\n",
    "        mp_edges.append(e_list * multiplier)\n",
    "    return mp_edges\n",
    "\n",
    "# -----------------------\n",
    "# Note :\n",
    "#  1. DGL can't process serialized ids \n",
    "#  2. RW needs to have node types preficed for w2v mocel\n",
    "# -----------------------\n",
    "def get_RW_list(graph_obj, metapaths):\n",
    "    start_node_types = [mp[0].split('_')[0] for mp in metapaths]\n",
    "    RW_list = []\n",
    "    \n",
    "    node_typeID2typename = {}\n",
    "    for e in  enumerate(graph_obj.ntypes): \n",
    "        node_typeID2typename[e[0]] = e[1]\n",
    "    \n",
    "    \n",
    "    def add_prefix(prefix, val):\n",
    "        return prefix + '_' + str(val)\n",
    "    \n",
    "    for ntype, mp in zip(start_node_types, metapaths): \n",
    "#         print(ntype, graph_obj.nodes(ntype).shape)\n",
    "        RW_mp = dgl.sampling.random_walk(\n",
    "            graph_obj,\n",
    "            metapath = mp,\n",
    "            nodes = graph_obj.nodes(ntype),\n",
    "            prob = 'weight'\n",
    "        )\n",
    "#         print(ntype, mp)\n",
    "        _random_walks = RW_mp[0].data.numpy()\n",
    "#         print(_random_walks[0])\n",
    "       \n",
    "        pattern = RW_mp[1].data.numpy().tolist()\n",
    "        pattern = [node_typeID2typename[_] for _ in pattern ]\n",
    "#         print(' > ', pattern)\n",
    "        vectorized_func = np.vectorize(add_prefix)\n",
    "        _random_walks = vectorized_func( pattern, _random_walks)\n",
    "        \n",
    "        RW_list.extend(_random_walks.tolist())\n",
    "    return RW_list\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Get and save the vectrs for each node\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def extract_feature_vectors(w2v_model):\n",
    "    vectors_dict = {}\n",
    "    \n",
    "    for token,vector in mp2vec_model.wv.vocab.items():\n",
    "        try:\n",
    "            _type,_id = token.split('_')\n",
    "        except:\n",
    "            print(' >> ',token, type(token))\n",
    "            continue\n",
    "        _id = int(_id)\n",
    "        if _id < 0 : continue\n",
    "        if _type not in vectors_dict.keys(): \n",
    "            vectors_dict[_type]= {}\n",
    "        vectors_dict[_type][_id] = w2v_model.wv[token]\n",
    "        \n",
    "    return vectors_dict\n",
    "        \n",
    "\n",
    "def save_vectors(node_vectors):\n",
    "    global DIR\n",
    "    global MODEL_SAVE_DATA_LOC\n",
    "    for n_type, _dict in node_vectors.items():\n",
    "        # sort the vectors by id \n",
    "        arr_vec = []\n",
    "        \n",
    "        arr_vec = [ _[1] for _ in sorted(_dict.items(),key = itemgetter(0))]\n",
    "#         for n_id in sorted(_dict.keys()):\n",
    "#             arr_vec.append(_dict[n_id])\n",
    "        arr_vec = np.array(arr_vec)\n",
    "        fname = 'mp2v_{}_.npy'.format(n_type)\n",
    "        fname = os.path.join(MODEL_SAVE_DATA_LOC,fname)\n",
    "        np.save(fname,arr_vec)\n",
    "        \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    edge_weights[et] = weights\n",
    "    edge_weights[et_R] = weights\n",
    "        \n",
    "    return graph_data, edge_weights\n",
    "\n",
    "class loss_callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subed = 0\n",
    "            \n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
    "        self.epoch += 1\n",
    "# -----------------------------------------------------------\n",
    "# Metapath 2 vev\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def mp2vec(random_walks, epochs = 100):\n",
    "    cpu_count = mp.cpu_count()\n",
    "    model = Word2Vec(\n",
    "        random_walks, \n",
    "        size=128, \n",
    "        window=3, \n",
    "        negative = 10,\n",
    "        hs = 0,\n",
    "        min_count=1, \n",
    "        iter = epochs,\n",
    "        compute_loss=True,\n",
    "        null_word= '-1',\n",
    "        callbacks = [loss_callback()]\n",
    "    )\n",
    "    return model\n",
    "# -----------------------------------------------------------\n",
    "# Get metapaths\n",
    "# -----------------------------------------------------------\n",
    "def get_mp_list(graph_obj, multiplier=2):\n",
    "    with open('metapaths.txt', 'r') as fh:\n",
    "        mp_list = fh.readlines()\n",
    "        \n",
    "    mp_edges = []\n",
    "    mp_list = [_.strip('\\n') for _ in mp_list]\n",
    "    mp_list = [_.strip(' ') for _ in mp_list]\n",
    "    for mp in mp_list:\n",
    "        mp = mp.split(',')\n",
    "        _mp = list(mp)\n",
    "        _mp.reverse()\n",
    "        mp = mp + _mp[1:]\n",
    "        e_list = []\n",
    "        for i in range(len(mp)-1):\n",
    "            e = mp[i] + '_' + mp[i+1]\n",
    "            e_list.append(e)\n",
    "            if e not in graph_obj.etypes:\n",
    "                print('ERROR!!')\n",
    "        mp_edges.append(e_list * multiplier)\n",
    "    return mp_edges\n",
    "\n",
    "# -----------------------\n",
    "# Note :\n",
    "#  1. DGL can't process serialized ids \n",
    "#  2. RW needs to have node types preficed for w2v mocel\n",
    "# -----------------------\n",
    "def get_RW_list(graph_obj, metapaths):\n",
    "    start_node_types = [mp[0].split('_')[0] for mp in metapaths]\n",
    "    RW_list = []\n",
    "    \n",
    "    node_typeID2typename = {}\n",
    "    for e in  enumerate(graph_obj.ntypes): \n",
    "        node_typeID2typename[e[0]] = e[1]\n",
    "    \n",
    "    \n",
    "    def add_prefix(prefix, val):\n",
    "        return prefix + '_' + str(val)\n",
    "    \n",
    "    for ntype, mp in zip(start_node_types, metapaths): \n",
    "#         print(ntype, graph_obj.nodes(ntype).shape)\n",
    "        RW_mp = dgl.sampling.random_walk(\n",
    "            graph_obj,\n",
    "            metapath = mp,\n",
    "            nodes = graph_obj.nodes(ntype),\n",
    "            prob = 'weight'\n",
    "        )\n",
    "#         print(ntype, mp)\n",
    "        _random_walks = RW_mp[0].data.numpy()\n",
    "#         print(_random_walks[0])\n",
    "       \n",
    "        pattern = RW_mp[1].data.numpy().tolist()\n",
    "        pattern = [node_typeID2typename[_] for _ in pattern ]\n",
    "#         print(' > ', pattern)\n",
    "        vectorized_func = np.vectorize(add_prefix)\n",
    "        _random_walks = vectorized_func( pattern, _random_walks)\n",
    "        \n",
    "        RW_list.extend(_random_walks.tolist())\n",
    "    return RW_list\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Get and save the vectrs for each node\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def extract_feature_vectors(w2v_model):\n",
    "    vectors_dict = {}\n",
    "    \n",
    "    for token,vector in mp2vec_model.wv.vocab.items():\n",
    "        try:\n",
    "            _type,_id = token.split('_')\n",
    "        except:\n",
    "            print(' >> ',token, type(token))\n",
    "            continue\n",
    "        _id = int(_id)\n",
    "        if _id < 0 : continue\n",
    "        if _type not in vectors_dict.keys(): \n",
    "            vectors_dict[_type]= {}\n",
    "        vectors_dict[_type][_id] = w2v_model.wv[token]\n",
    "        \n",
    "    return vectors_dict\n",
    "        \n",
    "\n",
    "def save_vectors(node_vectors):\n",
    "    global DIR\n",
    "    global MODEL_SAVE_DATA_LOC\n",
    "    for n_type, _dict in node_vectors.items():\n",
    "        # sort the vectors by id \n",
    "        arr_vec = []\n",
    "        \n",
    "        arr_vec = [ _[1] for _ in sorted(_dict.items(),key = itemgetter(0))]\n",
    "#         for n_id in sorted(_dict.keys()):\n",
    "#             arr_vec.append(_dict[n_id])\n",
    "        arr_vec = np.array(arr_vec)\n",
    "        fname = 'mp2v_{}_.npy'.format(n_type)\n",
    "        fname = os.path.join(MODEL_SAVE_DATA_LOC,fname)\n",
    "        np.save(fname,arr_vec)\n",
    "        \n",
    "    return\n",
    "\n",
    "\n",
    "def main():\n",
    "    global DIR\n",
    "    global W2V_EPOCHS\n",
    "    global MODEL_SAVE_DATA_LOC\n",
    "    \n",
    "    MODEL_SAVE_DATA_LOC = os.path.join('saved_model_data',DIR)\n",
    "    path_obj = Path(MODEL_SAVE_DATA_LOC)\n",
    "    path_obj.mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "    graph_data, edge_weights = read_graph_data(DIR)\n",
    "    graph_obj = dgl.heterograph(graph_data)\n",
    "    print('Node types, edge types', graph_obj.ntypes, graph_obj.etypes)\n",
    "    print('Graph ::', graph_obj)\n",
    "    for e_type in edge_weights.keys():\n",
    "        graph_obj[e_type].edata['weight'] = FT(edge_weights[e_type])\n",
    "\n",
    "    print('Node types, edge types', graph_obj.ntypes, graph_obj.etypes)\n",
    "    print('Graph ::', graph_obj)\n",
    "\n",
    "    print('Graph dgl device', graph_obj.device)\n",
    "    # DEBUG\n",
    "    # print(graph_obj['HSCode_ShipmentOrigin'].edata)\n",
    "    # print(graph_obj['HSCode_ShipmentOrigin'].edges())\n",
    "\n",
    "    metapaths = get_mp_list(graph_obj,multiplier=5)\n",
    "    random_walks = get_RW_list(graph_obj, metapaths)\n",
    "    mp2vec_model = mp2vec(random_walks,epochs=W2V_EPOCHS)\n",
    "    mp2vec_model.save(os.path.join(MODEL_SAVE_DATA_LOC,\"mp2vec.model\"))\n",
    "    node_vectors = extract_feature_vectors(mp2vec_model)\n",
    "    save_vectors(node_vectors)\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\n",
    "#     '--DIR', choices=['us_import1', 'us_import2', 'us_import3'],\n",
    "#     default='us_import1'\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     '--W2V_EPOCHS', \n",
    "#     default=150,\n",
    "#     type=int\n",
    "# )\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# DIR = args.DIR\n",
    "# W2V_EPOCHS = args.W2V_EPOCHS\n",
    "\n",
    "W2V_EPOCHS =10\n",
    "DIR = 'us_import1'\n",
    "main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
