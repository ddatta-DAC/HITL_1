{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "from GD import GD\n",
    "sys.path.append('./.')\n",
    "sys.path.append('./..')\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import pickle\n",
    "import copy\n",
    "import json\n",
    "from GD import GD\n",
    "from linear_model_v2 import linearClassifier_bEF\n",
    "import seaborn as sns\n",
    "from record import record_class\n",
    "import yaml\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from common_utils import utils\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "explantions_file_path = None\n",
    "embedding_data_path = None\n",
    "serialID_mapping_loc = None\n",
    "anomalies_pos_fpath = None\n",
    "anomalies_neg_fpath = None\n",
    "interaction_type = 'concat'\n",
    "DIR = None\n",
    "\n",
    "\n",
    "'''\n",
    "embedding_data_path  = './../../createGraph_trade/saved_model_data/{}'.format(DIR)\n",
    "serialID_mapping_loc = './../../generated_data_v1/{}/idMapping.csv'.format(DIR)\n",
    "anomalies_pos_fpath = './../../generated_data_v1/generated_anomalies/{}/pos_anomalies.csv'.format(DIR)\n",
    "anomalies_neg_fpath = './../../generated_data_v1/generated_anomalies/{}/neg_anomalies.csv'.format(DIR)\n",
    "explantions_f_path =  './../../generated_data_v1/generated_anomalies/{}/pos_anomalies_explanations.json'.format(DIR)\n",
    "'''\n",
    "\n",
    "def setup_config(DIR):\n",
    "    global explantions_file_path\n",
    "    global embedding_data_path\n",
    "    global serialID_mapping_loc\n",
    "    global anomalies_pos_fpath\n",
    "    global anomalies_neg_fpath\n",
    "    global domain_dims\n",
    "    global test_data_serialized_loc\n",
    "    with open('config.yaml', 'r') as fh:\n",
    "        config = yaml.safe_load(fh)\n",
    "\n",
    "    serialID_mapping_loc = config['serialID_mapping_loc'].format(DIR)\n",
    "    embedding_data_path = config['embedding_data_path'].format(DIR)\n",
    "    explantions_file_path = config['explantions_file_path'].format(DIR)\n",
    "    anomalies_pos_fpath = config['anomalies_pos_fpath'].format(DIR)\n",
    "    anomalies_neg_fpath = config['anomalies_neg_fpath'].format(DIR)\n",
    "    test_data_serialized_loc = config['test_data_serialized_loc'].format(DIR)\n",
    "\n",
    "    with open(config['domain_dims_file_path'].format(DIR), 'rb') as fh:\n",
    "        domain_dims = OrderedDict(pickle.load(fh))\n",
    "    return\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "def get_serialID_to_entityID():\n",
    "    global serialID_mapping_loc\n",
    "    global DIR\n",
    "    \n",
    "    idMapper_file = os.path.join(serialID_mapping_loc)\n",
    "    mapping_df = pd.read_csv(idMapper_file, index_col=None)\n",
    "    serialID_to_entityID = {}\n",
    "\n",
    "    for i, row in mapping_df.iterrows():\n",
    "        serialID_to_entityID[row['serial_id']] = row['entity_id']\n",
    "    return serialID_to_entityID\n",
    "\n",
    "# ---------------------------\n",
    "# Get records which are deemed nominal/normal\n",
    "# ---------------------------\n",
    "def obtain_normal_samples():\n",
    "    global test_data_serialized_loc\n",
    "    normal_data = pd.read_csv(\n",
    "        test_data_serialized_loc, index_col=None\n",
    "    )\n",
    "\n",
    "    _df = normal_data.sample(5000)\n",
    "    obj_list = []\n",
    "    data_x = []\n",
    "    for i in tqdm(range(_df.shape[0])):\n",
    "        obj = record_class(_df.iloc[i].to_dict(), -1)\n",
    "        obj.calc_features()\n",
    "        obj_list.append(obj)\n",
    "        data_x.append(obj.features)\n",
    "    \n",
    "    return data_x\n",
    "\n",
    "def get_trained_classifier(X, y, num_domains, emb_dim, num_epochs=10000):\n",
    "    global domain_dims\n",
    "    global interaction_type\n",
    "    classifier_obj = linearClassifier_bEF(\n",
    "        num_domains=num_domains,\n",
    "        emb_dim=emb_dim,\n",
    "        num_epochs=num_epochs,\n",
    "        L2_reg_lambda=0.0025,\n",
    "        force_reg=False,\n",
    "        interaction_type=interaction_type\n",
    "    )\n",
    "\n",
    "    classifier_obj.setup_binaryFeatures(\n",
    "        domain_dims,\n",
    "        binaryF_domains=['ConsigneePanjivaID', 'ShipperPanjivaID']\n",
    "    )\n",
    "\n",
    "    # classifier_obj.fit_on_pos(X, np.ones(X.shape[0]),n_epochs=10000)\n",
    "    classifier_obj.fit(X, y, log_interval=5000)\n",
    "    classifier_obj.fit_on_pos(X, y, n_epochs=num_epochs // 2, log_interval=1000)\n",
    "    return classifier_obj\n",
    "\n",
    "\n",
    "def fetch_entityID_arr_byList(data_df, id_list):\n",
    "    global domain_dims\n",
    "    domain_list = list(domain_dims.keys())\n",
    "    ID_COL = 'PanjivaRecordID'\n",
    "    data_df = data_df.copy(deep=True)\n",
    "    data_df = data_df.loc[data_df[ID_COL].isin(id_list)]\n",
    "    # Order of id_list has to be preserved!!!\n",
    "    X = []\n",
    "    for _id in id_list:\n",
    "        _tmp = data_df.loc[data_df[ID_COL] == _id][domain_list].iloc[0].values.tolist()\n",
    "        X.append(_tmp)\n",
    "    return np.array(X).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    global anomalies_pos_fpath\n",
    "    global anomalies_neg_fpath\n",
    "    global domain_dims\n",
    "    global explantions_file_path\n",
    "    global embedding_data_path\n",
    "    global serialID_mapping_loc\n",
    "\n",
    "    # ============================================\n",
    "\n",
    "    anom_pos_df = pd.read_csv(anomalies_pos_fpath, index_col=None)\n",
    "    anom_neg_df = pd.read_csv(anomalies_neg_fpath, index_col=None)\n",
    "    serialID_to_entityID = get_serialID_to_entityID()\n",
    "    print('Setting up record class embedding...',embedding_data_path)\n",
    "    record_class.__setup_embedding__(embedding_data_path, serialID_to_entityID, _normalize=True)\n",
    "    main_data_df = pd.concat(\n",
    "        [anom_pos_df,\n",
    "         anom_neg_df],\n",
    "        axis=0\n",
    "    )\n",
    "    # main_data_df has the records with entity ids\n",
    "\n",
    "\n",
    "    obj_list = []\n",
    "    for i in tqdm(range(anom_neg_df.shape[0])):\n",
    "        obj = record_class(anom_neg_df.iloc[i].to_dict(), -1)\n",
    "        obj.calc_features()\n",
    "        obj_list.append(obj)\n",
    "\n",
    "    for i in tqdm(range(anom_pos_df.shape[0])):\n",
    "        obj = record_class(anom_pos_df.iloc[i].to_dict(), 1)\n",
    "        obj.calc_features()\n",
    "        obj_list.append(obj)\n",
    "\n",
    "    # Read in the explantions\n",
    "    with open(explantions_file_path, 'rb') as fh:\n",
    "        explanations = json.load(fh)\n",
    "\n",
    "    explanations = {int(k): [sorted(_) for _ in v] for k, v in explanations.items()}\n",
    "    data_x = []\n",
    "    data_x_features = []\n",
    "    data_id = []\n",
    "    data_label = []\n",
    "    data_ID_to_matrix = {}\n",
    "\n",
    "    for _obj in obj_list:\n",
    "        data_x.append(_obj.x)\n",
    "        data_id.append(_obj.id)\n",
    "        data_label.append(_obj.label)\n",
    "        data_ID_to_matrix[_obj.id] = _obj.features\n",
    "        data_x_features.append(_obj.features)\n",
    "    data_x = np.stack(data_x)\n",
    "    data_label = np.array(data_label)\n",
    "    data_id = np.array(data_id)\n",
    "    return main_data_df, explanations , data_id, data_x, data_label, data_x_features, data_ID_to_matrix\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "def execute_with_input(\n",
    "    check_next = 25,\n",
    "    batch_size= 25\n",
    "):\n",
    "\n",
    "    serialID_to_entityID = get_serialID_to_entityID()\n",
    "    main_data_df, explanations, data_id, data_x, data_label, data_x_features, data_ID_to_matrix = get_data()\n",
    "    emb_dim = record_class.embedding['HSCode'].shape[1]\n",
    "\n",
    "    # -------------------------------------------\n",
    "    domain_idx = {e[0]: e[1] for e in enumerate(domain_dims.keys())}\n",
    "    domainInteraction_index = {}\n",
    "    num_domains = len(domain_dims)\n",
    "    k = 0\n",
    "    for i in range(num_domains):\n",
    "        for j in range(i + 1, num_domains):\n",
    "            domainInteraction_index['_'.join((domain_idx[i], domain_idx[j]))] = k\n",
    "            k += 1\n",
    "\n",
    "    idx = np.arange(len(data_id), dtype=int)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    data_x = data_x[idx]\n",
    "    data_label = data_label[idx]\n",
    "    data_id = data_id[idx]\n",
    "\n",
    "    X_0 = np.array(data_x_features)[idx.tolist()]  # Relevant anomalies\n",
    "    X_1 = obtain_normal_samples()  # Nominal\n",
    "\n",
    "\n",
    "    X_1 =np.array(X_1)\n",
    "\n",
    "    y_0 = np.ones(X_0.shape[0])\n",
    "    y_1 = -1 * np.ones(X_1.shape[0])\n",
    "    y = np.hstack([y_0, y_1])\n",
    "    X = np.vstack([X_0, X_1])\n",
    "    num_coeff = len(domainInteraction_index)\n",
    "\n",
    "    classifier_obj = get_trained_classifier(\n",
    "        X,\n",
    "        y,\n",
    "        num_domains,\n",
    "        emb_dim\n",
    "    )\n",
    "\n",
    "    W = classifier_obj.W.cpu().data.numpy()\n",
    "    emb_dim = W.shape[-1]\n",
    "\n",
    "    # classifier_obj.predict_score_op(X_0)\n",
    "    # Create a reference dataframe  :: data_reference_df\n",
    "    data_reference_df = pd.DataFrame(\n",
    "        data=np.vstack([data_id, data_label]).transpose(),\n",
    "        columns=['PanjivaRecordID', 'label']\n",
    "    )\n",
    "\n",
    "    data_reference_df['baseID'] = data_reference_df['PanjivaRecordID'].apply(lambda x: str(x)[:-3])\n",
    "    data_reference_df['expl_1'] = -1\n",
    "    data_reference_df['expl_2'] = -1\n",
    "    data_reference_df['original_score'] = 1\n",
    "\n",
    "    for i, row in data_reference_df.iterrows():\n",
    "        _id = int(row['PanjivaRecordID'])\n",
    "        if _id in explanations.keys():\n",
    "            entry = explanations[_id]\n",
    "            domain_1 = entry[0][0]\n",
    "            domain_2 = entry[0][1]\n",
    "            data_reference_df.loc[i, 'expl_1'] = domainInteraction_index['_'.join(sorted([domain_1, domain_2]))]\n",
    "            domain_1 = entry[1][0]\n",
    "            domain_2 = entry[1][1]\n",
    "            data_reference_df.loc[i, 'expl_2'] = domainInteraction_index['_'.join(sorted([domain_1, domain_2]))]\n",
    "        _x = data_ID_to_matrix[_id]\n",
    "        data_reference_df.loc[i, 'original_score'] = classifier_obj.predict_score_op(np.array([_x]))[0]\n",
    "\n",
    "    data_reference_df['cur_score'] = data_reference_df['original_score'].values\n",
    "\n",
    "\n",
    "    # To get random results\n",
    "    # Randomization\n",
    "    cur_df = data_reference_df.copy()\n",
    "    cur_df = cur_df.sample(frac=1).reset_index(drop=True)\n",
    "    cur_df = shuffle(cur_df).reset_index(drop=True)\n",
    "    \n",
    "    clf_obj = copy.deepcopy(classifier_obj)\n",
    "    \n",
    "    working_df=cur_df.copy(deep=True)\n",
    "    ref_data_df=main_data_df.copy(deep=True)\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    domain_list = list(domain_dims.keys())\n",
    "    total_posCount = len(working_df.loc[working_df['label'] == 1])\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    #  Main loop\n",
    "    # -------------------------------------------------\n",
    "    next_K_precision = []\n",
    "    prev_discovered_count = 0\n",
    "    BATCH_SIZE = batch_size\n",
    "    ID_COL = 'PanjivaRecordID'\n",
    "    discovered_df = pd.DataFrame( columns = list(working_df.columns))\n",
    "    \n",
    "    W = clf_obj.W.cpu().data.numpy()\n",
    "    GD_obj = GD(\n",
    "        num_coeff,\n",
    "        emb_dim,\n",
    "        interaction_type = interaction_type\n",
    "    )\n",
    "    GD_obj.set_original_W(W)\n",
    "    num_batches  = len(data_reference_df)//batch_size\n",
    "    zero_count = 0\n",
    "    for batch_idx in tqdm(num_batches):\n",
    "        print('Batch : {}'.format(batch_idx+1))\n",
    "        if batch_idx == 0:\n",
    "            lr = 0.25\n",
    "            max_iter = 1000\n",
    "        else:\n",
    "            lr = 1\n",
    "            max_iter = 500\n",
    "\n",
    "        cur = working_df.head(BATCH_SIZE).reset_index(drop=True)\n",
    "        if len(cur) < 2:\n",
    "            break\n",
    "       \n",
    "        _tail_count = len(working_df) - BATCH_SIZE\n",
    "        tmp = working_df.tail(_tail_count).reset_index(drop=True)\n",
    "        if len(tmp.loc[tmp['label']==1]) == 0 :\n",
    "            zero_count +=1\n",
    "            if zero_count > 5:\n",
    "                next_K_precision.append(0)\n",
    "                working_df = working_df.tail(_tail_count).reset_index(drop=True)\n",
    "                continue\n",
    "        else :\n",
    "            zero_count = 0    \n",
    "        # -----\n",
    "        # Count( of discovered in the current batch ( at the top; defined by batch size )\n",
    "        # -----\n",
    "        cum_cur_discovered = prev_discovered_count + len(cur.loc[cur['label'] == 1])\n",
    "        prev_discovered_count = cum_cur_discovered\n",
    "        _recall = float(cum_cur_discovered) / total_posCount\n",
    "        recall.append(_recall)\n",
    "\n",
    "        x_ij = []\n",
    "        x_entityIds = []\n",
    "\n",
    "        flags = []  # Whether a pos anomaly or not\n",
    "        terms = []  # Explanation terms\n",
    "        discovered_df = discovered_df.append(cur, ignore_index=True)\n",
    "        \n",
    "        for i, row in discovered_df.iterrows():\n",
    "            _mask = np.zeros(len(domainInteraction_index))\n",
    "            if row['label'] == 1:\n",
    "                _mask[row['expl_1']] = 1\n",
    "                _mask[row['expl_2']] = 1\n",
    "                flags.append(1)\n",
    "                terms.append((row['expl_1'], row['expl_2'],))\n",
    "            else:\n",
    "                flags.append(0)\n",
    "                terms.append(())\n",
    "            id_value = row['PanjivaRecordID']\n",
    "            x_ij.append(data_ID_to_matrix[id_value])\n",
    "\n",
    "            row_dict = ref_data_df.loc[(ref_data_df[ID_COL] == id_value)].iloc[0].to_dict()\n",
    "            x_entityIds.append([row_dict[d] for d in domain_list])\n",
    "\n",
    "        x_entityIds = np.array(x_entityIds)\n",
    "        x_ij = np.array(x_ij)\n",
    "\n",
    "        updated_W = GD_obj.update_weight(\n",
    "            flags,\n",
    "            terms,\n",
    "            x_ij,\n",
    "            lr = lr,\n",
    "            max_iter= max_iter\n",
    "        )\n",
    "        \n",
    "        # ----------------------------------------------------\n",
    "        # Update Model\n",
    "        # ----------------------------------------------------\n",
    "        clf_obj.update_W(updated_W)\n",
    "        clf_obj.update_binary_VarW(x_entityIds, flags)\n",
    "\n",
    "        _tail_count = len(working_df) - BATCH_SIZE\n",
    "        working_df = working_df.tail(_tail_count).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        # Obtain scores\n",
    "        x_ij_test = []\n",
    "        x_entityIds = fetch_entityID_arr_byList(\n",
    "            ref_data_df,\n",
    "            working_df['PanjivaRecordID'].values.tolist()\n",
    "        )\n",
    "        for _id in working_df['PanjivaRecordID'].values:\n",
    "            x_ij_test.append(data_ID_to_matrix[_id])\n",
    "\n",
    "        x_ij_test = np.array(x_ij_test)\n",
    "\n",
    "        new_scores = clf_obj.predict_bEF(x_entityIds, x_ij_test)\n",
    "\n",
    "        old_scores = working_df['cur_score'].values\n",
    "        _delta = new_scores - old_scores\n",
    "        working_df['delta'] = new_scores\n",
    "        working_df = working_df.sort_values(by='delta', ascending=False)\n",
    "        working_df = working_df.reset_index(drop=True)\n",
    "        \n",
    "        tmp = working_df.head(check_next)\n",
    "        _labels = tmp['label'].values\n",
    "        res = len(np.where(_labels == 1)[0])\n",
    "        _precison = res / check_next\n",
    "        next_K_precision.append(_precison)\n",
    "        \n",
    "    return next_K_precision\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
