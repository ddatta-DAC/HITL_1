{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "\n",
    "with open('DBLP_Triplets.pickle','rb') as fh :\n",
    "    data_obj = pickle.load(fh)\n",
    "\n",
    "author_label = pd.read_csv('author_label.txt',low_memory=False,header=None,sep='\\t',names=['ID','label','name'])\n",
    "author_label['ID'] =  author_label['ID'].apply(lambda x :'A' +  str(x))\n",
    "\n",
    "\n",
    "\n",
    "Counter(author_label['label'])\n",
    "\n",
    "\n",
    "valid_authors = list(author_label['ID'])\n",
    "print('# of authors with labels ', len(valid_authors))\n",
    "\n",
    "author_file = 'author.txt' \n",
    "author_df = pd.read_csv(\n",
    "    author_file,\n",
    "    low_memory=False,\n",
    "    header=None,\n",
    "    sep='\\t',\n",
    "    names=['ID','name']\n",
    ")\n",
    "author_df['ID'] =  author_df['ID'].apply(lambda x :'A' +  str(x))\n",
    "author_df =  author_df.loc[author_df['ID'].isin(author_label['ID'].values.tolist())]\n",
    "\n",
    "\n",
    "paper_file = 'paper.txt' \n",
    "paper_df = pd.read_csv(\n",
    "    paper_file,\n",
    "    low_memory=False,\n",
    "    header=None,\n",
    "    sep='\\t',\n",
    "    names=['ID','name']\n",
    ")\n",
    "paper_df['ID'] =  paper_df['ID'].apply(lambda x :'P' +  str(x))\n",
    "\n",
    "term_file = 'term.txt' \n",
    "term_df = pd.read_csv(\n",
    "    term_file,\n",
    "    low_memory=False,\n",
    "    header=None,\n",
    "    sep='\\t',\n",
    "    names=['ID','name']\n",
    ")\n",
    "\n",
    "term_df = term_df.loc[~term_df['name'].isin(set(stopwords.words('english')))]\n",
    "term_df['ID'] =  term_df['ID'].apply(lambda x :'T' +  str(x))\n",
    "\n",
    "conf_file = 'conf.txt' \n",
    "conf_df = pd.read_csv(\n",
    "    conf_file,\n",
    "    low_memory=False,\n",
    "    header=None,\n",
    "    sep='\\t',\n",
    "    names=['ID','name']\n",
    ")\n",
    "conf_df['ID'] =  conf_df['ID'].apply(lambda x :'C' +  str(x))\n",
    "\n",
    "\n",
    "\n",
    "paper_ID_list = paper_df['ID'].values\n",
    "author_ID_list = author_df['ID'].values\n",
    "conf_ID_list = conf_df['ID'].values\n",
    "term_ID_list = term_df['ID'].values\n",
    "\n",
    "print(len(paper_ID_list), len(author_ID_list), len(conf_ID_list), len(term_ID_list))\n",
    "\n",
    "PA = pd.read_csv('paper_author.txt', sep='\\t', names = ['P','A'])\n",
    "PC = pd.read_csv('paper_conf.txt', sep='\\t', names = ['P','C'])\n",
    "PT = pd.read_csv('paper_term.txt', sep='\\t', names = ['P','T'])\n",
    "PA['P']  =  PA['P'].apply(lambda x :'P' +  str(x))\n",
    "PT['P']  =  PA['P'].apply(lambda x :'P' +  str(x))\n",
    "PC['P']  =  PA['P'].apply(lambda x :'P' +  str(x))\n",
    "PA['A']  =  PA['A'].apply(lambda x :'A' +  str(x))\n",
    "PT['T']  =  PT['T'].apply(lambda x :'T' +  str(x))\n",
    "PC['C']  =  PC['C'].apply(lambda x :'C' +  str(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Create graph \n",
    "# --------------------------\n",
    "G = nx.Graph()\n",
    "\n",
    "for i,row in PA.iterrows():\n",
    "    p = row['P']\n",
    "    a = row['A']\n",
    "    if a in valid_authors:\n",
    "        G.add_edge(p,a)\n",
    "\n",
    "print(G.number_of_nodes(), G.number_of_edges())\n",
    "\n",
    "for i,row in PC.iterrows():\n",
    "    p = row['P']\n",
    "    c = row['C']\n",
    "    G.add_edge(p,c)  \n",
    "\n",
    "print(G.number_of_nodes(), G.number_of_edges())\n",
    "\n",
    "\n",
    "\n",
    "PT = PT.loc[PT['T'].isin(term_df['ID'].values.tolist())]\n",
    "_p = PT['P'].values.tolist()\n",
    "_t = PT['T'].values.tolist()\n",
    "arr = [ (i,j) for i,j in zip(_p,_t)]\n",
    "G.add_edges_from(arr)\n",
    "\n",
    "print(G.number_of_nodes(), G.number_of_edges())\n",
    "\n",
    "print('Author / label count', Counter(author_label['label']))\n",
    "\n",
    "label_0_ids =author_label.loc[author_label['label']==0]['ID'].values.tolist()\n",
    "label_1_ids =author_label.loc[author_label['label']==1]['ID'].values.tolist()\n",
    "label_2_ids =author_label.loc[author_label['label']==2]['ID'].values.tolist()\n",
    "label_3_ids =author_label.loc[author_label['label']==3]['ID'].values.tolist()\n",
    "\n",
    "G1 = G.copy()\n",
    "print(G1.number_of_nodes(),G1.number_of_edges())\n",
    "\n",
    "filter_terms = []\n",
    "for node in G1.nodes():\n",
    "    if node in list(term_df['ID']):\n",
    "        if nx.degree(G1,node) < 2 : \n",
    "            filter_terms.append(node)\n",
    "print(' Rare Terms to remove ',len(filter_terms))\n",
    "\n",
    "for n in filter_terms:\n",
    "    G1.remove_node(n)\n",
    "    \n",
    "isolates = [n for n in nx.isolates(G1) ]\n",
    "for n in isolates:\n",
    "    G1.remove_node(n)\n",
    "\n",
    "print(G1.number_of_nodes(), G1.number_of_edges())\n",
    "\n",
    "# Figure out the valid term ids\n",
    "list_terms = set(list(G1.nodes())).intersection(set(term_ID_list))\n",
    "list_authors = set(list(G1.nodes())).intersection(set(valid_authors))\n",
    "list_confs = set(list(G1.nodes())).intersection(set(conf_ID_list))\n",
    "list_papers = set(list(G1.nodes())).intersection(set(paper_ID_list))\n",
    "\n",
    "print( [len(list_terms),len(list_authors),len(list_confs),len(list_papers)])\n",
    "\n",
    "# =================================\n",
    "# Create synthetic ids for continuous ids in the riginal dataset\n",
    "# ==================================\n",
    "def get_df(id_list, label_df=None):\n",
    "    data = [( i,j) for i,j in enumerate(id_list)]\n",
    "    df = pd.DataFrame(np.array(data),columns=['synID','ID'])\n",
    "    print(len(df))\n",
    "    if label_df is not None:\n",
    "         df = df.merge(label_df, on=['ID'],how='inner')\n",
    "    print(len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "valid_ids = list(G1.nodes())\n",
    "C_df = get_df(list_confs,None)\n",
    "T_df = get_df(list_terms)\n",
    "P_df = get_df(list_papers)\n",
    "author_label_df = author_label[['ID','label']]\n",
    "A_df = get_df(list_authors, author_label_df)\n",
    "\n",
    "C_df['type'] = 'C'\n",
    "P_df['type'] = 'P'\n",
    "T_df['type'] = 'T'\n",
    "A_df['type'] = 'A'\n",
    "\n",
    "C_df['label'] = None\n",
    "P_df['label'] = None\n",
    "T_df['label'] = None\n",
    "# Export the edge lists\n",
    "G1.number_of_edges()\n",
    "\n",
    "all_nodes_df = A_df.copy()\n",
    "all_nodes_df = all_nodes_df.append(C_df,ignore_index=True)\n",
    "all_nodes_df = all_nodes_df.append(P_df,ignore_index=True)\n",
    "all_nodes_df = all_nodes_df.append(T_df,ignore_index=True)\n",
    "\n",
    "edges_df = pd.DataFrame(data_obj[2],columns=['n1','n2','etype'])\n",
    "all_ids = list(all_nodes_df['ID'])\n",
    "edges_df = edges_df.loc[(edges_df['n1'].isin(all_ids)) & (edges_df['n2'].isin(all_ids))]\n",
    "\n",
    "edges_df.to_csv('dblp_edges.csv',index=False)\n",
    "all_nodes_df.to_csv('dblp_nodes.csv',index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
