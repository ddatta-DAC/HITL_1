{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from sklearn.preprocessing import normalize\n",
    "sys.path.append('./..')\n",
    "sys.path.append('./../..')\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except:\n",
    "    pass\n",
    "import json\n",
    "from onlineGD import onlineGD\n",
    "import linear_model\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--DIR', choices=['us_import1', 'us_import2', 'us_import3','us_import4'],\n",
    "    default=None\n",
    ")\n",
    "\n",
    "\n",
    "DIR = 'us_import4'\n",
    "\n",
    "# ============================================\n",
    "\n",
    "\n",
    "embedding_path  = './../../createGraph_trade/saved_model_data/{}'.format(DIR)\n",
    "serialID_mapping_loc = './../../generated_data_v1/{}/idMapping.csv'.format(DIR)\n",
    "idMapper_file = os.path.join(serialID_mapping_loc)\n",
    "mapping_df = pd.read_csv(idMapper_file, index_col=None)\n",
    "serialID_to_entityID = {}\n",
    "\n",
    "for i, row in mapping_df.iterrows():\n",
    "    serialID_to_entityID[row['serial_id']] = row['entity_id']\n",
    "\n",
    "anomalies_pos_fpath = './../../generated_data_v1/generated_anomalies/{}/pos_anomalies.csv'.format(DIR)\n",
    "anomalies_neg_fpath = './../../generated_data_v1/generated_anomalies/{}/neg_anomalies.csv'.format(DIR)\n",
    "anom_pos_df = pd.read_csv(anomalies_pos_fpath, index_col=None)\n",
    "anom_neg_df = pd.read_csv(anomalies_neg_fpath, index_col=None)\n",
    "\n",
    "\n",
    "\n",
    "class record_obj:\n",
    "    embedding = None\n",
    "    serialID_to_entityID = None\n",
    "    @staticmethod\n",
    "    def __setup_embedding__(embedding_path, serialID_to_entityID, _normalize = True):\n",
    "        record_obj.embedding = {}\n",
    "        record_obj.serialID_to_entityID = serialID_to_entityID\n",
    "        files =  glob.glob(os.path.join(embedding_path,'**.npy'))\n",
    "        for f in sorted(files):\n",
    "            emb = np.load(f)\n",
    "            domain = f.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "            if _normalize:\n",
    "                emb = normalize(emb,axis=1)\n",
    "            record_obj.embedding[domain] = emb\n",
    "        return\n",
    "    \n",
    "    def __init__(self, _record, _label):\n",
    "        id_col = 'PanjivaRecordID'\n",
    "        self.id = _record[id_col]\n",
    "        domains = list(record_obj.embedding.keys())\n",
    "        self.x = []\n",
    "        self.label = _label\n",
    "        for d,e in _record.items():\n",
    "            if d == id_col: continue\n",
    "            non_serial_id = serialID_to_entityID[e]\n",
    "            self.x.append(record_obj.embedding[d][non_serial_id])\n",
    "        self.x = np.array(self.x)\n",
    "\n",
    "record_obj.__setup_embedding__(embedding_path, serialID_to_entityID, _normalize=True)\n",
    "\n",
    "emb_dim = record_obj.embedding['HSCode'].shape[1]\n",
    "\n",
    "\n",
    "def obtain_normal_samples(DIR):\n",
    "    normal_data = pd.read_csv(\n",
    "        './../../generated_data_v1/{}/stage_2/test_serialized.csv'.format(DIR), index_col= None\n",
    "    )\n",
    "    \n",
    "    _df =  normal_data.sample(5000)\n",
    "    obj_list = []\n",
    "    for i in tqdm(range(_df.shape[0])):\n",
    "        obj = record_obj(_df.iloc[i].to_dict(),-1)\n",
    "        obj_list.append(obj)\n",
    "    data_x = []\n",
    "    for _obj in obj_list:\n",
    "        data_x.append(_obj.x)\n",
    "    data_x = np.stack(data_x)\n",
    "    return data_x\n",
    "\n",
    "obj_list = []\n",
    "for i in tqdm(range(anom_neg_df.shape[0])):\n",
    "    obj = record_obj(anom_neg_df.iloc[i].to_dict(),-1)\n",
    "    obj_list.append(obj)\n",
    "    \n",
    "for i in tqdm(range(anom_pos_df.shape[0])):\n",
    "    obj = record_obj(anom_pos_df.iloc[i].to_dict(),1)\n",
    "    obj_list.append(obj)\n",
    "\n",
    "# Read in the explantions\n",
    "\n",
    "explantions_f_path =  './../../generated_data_v1/generated_anomalies/{}/pos_anomalies_explanations.json'.format(DIR)\n",
    "with open(explantions_f_path,'rt') as fh:\n",
    "    explanations = json.load(fh)\n",
    "explanations = { int(k): [sorted (_) for _ in v] for k,v in explanations.items()}\n",
    "\n",
    "domain_dims = None\n",
    "with open('./../../generated_data_v1/{}/domain_dims.pkl'.format(DIR), 'rb') as fh :\n",
    "    domain_dims = pickle.load(fh)\n",
    "\n",
    "num_domains = len(domain_dims)\n",
    "domain_idx = { e[0]:e[1] for e in enumerate(domain_dims.keys())}\n",
    "domain_list = list(domain_dims.keys())\n",
    "\n",
    "domainInteraction_index = {}\n",
    "k = 0 \n",
    "for i in range(num_domains):\n",
    "    for j in range(i+1,num_domains):\n",
    "        domainInteraction_index['_'.join((domain_idx[i] , domain_idx[j]))] = k\n",
    "        k+=1\n",
    "\n",
    "\n",
    "data_x = []\n",
    "data_id = []\n",
    "data_label = []\n",
    "data_ID_to_matrix = {}\n",
    "for _obj in obj_list:\n",
    "    data_x.append(_obj.x)\n",
    "    data_id.append(_obj.id)\n",
    "    data_label.append(_obj.label)\n",
    "    data_ID_to_matrix[_obj.id] = _obj.x\n",
    "data_x = np.stack(data_x)\n",
    "data_label = np.array(data_label)\n",
    "data_id = np.array(data_id)\n",
    "\n",
    "\n",
    "idx = np.arange(len(data_id),dtype=int)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "data_x = data_x[idx]\n",
    "data_label = data_label[idx]\n",
    "data_id = data_id[idx]\n",
    "\n",
    "\n",
    "X_0 = data_x \n",
    "X_1 = obtain_normal_samples(DIR)\n",
    "\n",
    "y_0 = np.ones(X_0.shape[0])\n",
    "y_1 = -1 * np.ones(X_1.shape[0])\n",
    "y = np.hstack([y_0,y_1])\n",
    "X = np.vstack([X_0,X_1])\n",
    "\n",
    "\n",
    "def get_trained_classifier( X,y , num_domains, emb_dim, num_epochs=10000):\n",
    "    classifier_obj = linear_model.linearClassifier(\n",
    "        num_domains = num_domains , emb_dim = emb_dim, num_epochs=num_epochs\n",
    "    )\n",
    "\n",
    "    # classifier_obj.fit_on_pos(X, np.ones(X.shape[0]),n_epochs=10000)\n",
    "    classifier_obj.fit(X, y)\n",
    "    classifier_obj.fit_on_pos( X, y, n_epochs=5000)\n",
    "    return classifier_obj\n",
    "\n",
    "classifier_obj = get_trained_classifier(X,y , num_domains, emb_dim)\n",
    "      \n",
    "\n",
    "num_coeff = len(domainInteraction_index)\n",
    "W = classifier_obj.W.cpu().data.numpy()\n",
    "emb_dim = W.shape[-1]\n",
    "\n",
    "# classifier_obj.predict_score_op(X_0)\n",
    "\n",
    "# Create a referece dataframe  :: data_reference_df\n",
    "\n",
    "working_df = pd.DataFrame(\n",
    "    data = np.vstack([data_id, data_label]).transpose(), \n",
    "    columns=['PanjivaRecordID', 'label'] \n",
    ")\n",
    "\n",
    "working_df['baseID'] = working_df['PanjivaRecordID'].apply(lambda x : str(x)[:-3])\n",
    "working_df['expl_1'] = -1\n",
    "working_df['expl_2'] = -1\n",
    "working_df['original_score'] = 1\n",
    "\n",
    "for i,row in working_df.iterrows():\n",
    "    _id = int(row['PanjivaRecordID'])\n",
    "    if _id in explanations.keys():\n",
    "        entry = explanations[_id]\n",
    "        domain_1 = entry[0][0]\n",
    "        domain_2 = entry[0][1]\n",
    "        working_df.loc[i,'expl_1']= domainInteraction_index['_'.join(sorted( [domain_1, domain_2]))]\n",
    "        domain_1 = entry[1][0]\n",
    "        domain_2 = entry[1][1]\n",
    "        working_df.loc[i,'expl_2'] = domainInteraction_index['_'.join(sorted( [domain_1, domain_2]))]\n",
    "    _x = data_ID_to_matrix[_id]\n",
    "    working_df.loc[i,'original_score'] = classifier_obj.predict_score_op(np.array([_x]))[0]\n",
    "\n",
    "working_df['cur_score']  = working_df['original_score'].values\n",
    "data_reference_df = working_df.copy()\n",
    "\n",
    "def execute(\n",
    "    clf_obj,\n",
    "    working_df,\n",
    "    domainInteraction_index,\n",
    "    check_next = 20,\n",
    "    batch_size = 10\n",
    "):\n",
    "\n",
    "    BATCH_SIZE = batch_size\n",
    "    working_df['delta'] = 0\n",
    "    obj = onlineGD(num_coeff,emb_dim)\n",
    "    W = clf_obj.W.cpu().data.numpy()\n",
    "    obj.set_original_W(W)\n",
    "\n",
    "    num_batches = len(working_df.loc[working_df['label']==1])//BATCH_SIZE + 5\n",
    "    acc = []\n",
    "    for b in range(num_batches):\n",
    "        cur = working_df.head(BATCH_SIZE)\n",
    "        flags = [] # Whether a pos anaomaly or not\n",
    "        terms = [] # Explanation terms\n",
    "\n",
    "        x = [] \n",
    "        for i,row in cur.iterrows():\n",
    "            _mask = np.zeros(len(domainInteraction_index))\n",
    "            if row['label'] == 1:\n",
    "                _mask[row['expl_1']] = 1\n",
    "                _mask[row['expl_2']] = 1\n",
    "                flags.append(1)\n",
    "                terms.append((row['expl_1'],row['expl_2'],))\n",
    "            else:\n",
    "                flags.append(0)\n",
    "                terms.append(())\n",
    "            x.append(data_ID_to_matrix[row['PanjivaRecordID']])\n",
    "        if len(x) < 2:\n",
    "            break\n",
    "        x = np.array(x)\n",
    "    \n",
    "        final_gradient, _W = obj.update_weight(\n",
    "            flags,\n",
    "            terms,\n",
    "            x\n",
    "        )\n",
    "\n",
    "        # Update weights\n",
    "        clf_obj.update_W(_W)\n",
    "        working_df = working_df.iloc[BATCH_SIZE:]\n",
    "        # Obtain scores\n",
    "        x_test = []\n",
    "        for _id in  working_df['PanjivaRecordID'].values:\n",
    "            x_test.append( data_ID_to_matrix[_id] )\n",
    "        x_test = np.array(x_test)\n",
    "        new_scores =  clf_obj.predict_score_op(x_test)\n",
    "        old_scores = working_df['cur_score'].values\n",
    "        _delta = new_scores - old_scores\n",
    "        working_df['delta'] = _delta\n",
    "        working_df = working_df.sort_values(by='delta', ascending =False)\n",
    "        working_df = working_df.reset_index(drop=True)\n",
    "        tmp = working_df.head(check_next)\n",
    "        _labels = tmp['label'].values\n",
    "\n",
    "        res = len(np.where(_labels==1)[0])\n",
    "        _acc = res/check_next\n",
    "        acc.append(_acc)\n",
    "    return acc\n",
    "\n",
    "def execute_no_input(\n",
    "    working_df,\n",
    "    check_next = 20,\n",
    "    batch_size = 10\n",
    "):\n",
    "    \n",
    "    BATCH_SIZE = batch_size\n",
    "    working_df['delta'] = 0\n",
    "\n",
    "    num_batches = len(working_df.loc[working_df['label']==1])//BATCH_SIZE + 5\n",
    "    acc = []\n",
    "    for b in range(num_batches):\n",
    "        working_df = working_df.iloc[BATCH_SIZE:]\n",
    "        working_df = working_df.reset_index(drop=True)\n",
    "        tmp = working_df.head(check_next)\n",
    "        _labels = tmp['label'].values\n",
    "\n",
    "        res = len(np.where(_labels==1)[0])\n",
    "        _acc = res/check_next\n",
    "        acc.append(_acc)\n",
    "    return acc\n",
    "    \n",
    "\n",
    "num_runs = 20\n",
    "cumulative_results = pd.DataFrame(columns=['idx','acc'])\n",
    "results_no_input = pd.DataFrame(columns=['idx','acc'])\n",
    "\n",
    "\n",
    "for i in range(num_runs):\n",
    "    cur_df = data_reference_df.copy()\n",
    "    cur_df = cur_df.sample(frac=1).reset_index(drop=True)\n",
    "    acc = execute(\n",
    "            clf_obj = copy.deepcopy(classifier_obj),\n",
    "            working_df = cur_df,\n",
    "            domainInteraction_index = domainInteraction_index,\n",
    "            check_next = 50,\n",
    "            batch_size = 10\n",
    "    )\n",
    "    _tmpdf = pd.DataFrame( [(e[0],e[1]) for e in enumerate(acc)], columns=['idx','acc'] )\n",
    "    cumulative_results = cumulative_results.append(\n",
    "       _tmpdf, ignore_index=True\n",
    "    )\n",
    "    \n",
    "    acc = execute_no_input(\n",
    "            working_df = cur_df,\n",
    "            check_next = 50,\n",
    "            batch_size = 10\n",
    "    )\n",
    "    _tmpdf = pd.DataFrame( [(e[0],e[1]) for e in enumerate(acc)], columns=['idx','acc'] )\n",
    "    results_no_input = results_no_input.append(\n",
    "       _tmpdf, ignore_index=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.title('Accuracy in next 50 samples| Iteration(batch) : 10 samples')\n",
    "plt.xlabel('Batch index',fontsize=14)\n",
    "plt.ylabel('Accuracy in next 50 samples', fontsize=14)\n",
    "sns.lineplot(data=cumulative_results, x=\"idx\", y=\"acc\",markers=True, label = 'Input provided')\n",
    "sns.lineplot(data=results_no_input, x=\"idx\", y=\"acc\",markers=True, label='No Input')\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid()\n",
    "plt.savefig('{}_results_v1.png'.format(DIR))\n",
    "try:\n",
    "    plt.show()\n",
    "except:\n",
    "    pass\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
