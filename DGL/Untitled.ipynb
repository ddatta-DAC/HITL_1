{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('./..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "from dgl.data.utils import save_graphs\n",
    "import pickle\n",
    "from dgl.data.utils import load_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor as FT\n",
    "from torch import LongTensor as LT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_DBLP_data():\n",
    "    \n",
    "    loc = './../DBLP'\n",
    "    fname_e = 'dblp_edges.csv'\n",
    "    fname_n = 'dblp_nodes.csv'\n",
    "    df_e = pd.read_csv(os.path.join(loc,fname_e),low_memory=False)\n",
    "    df_n = pd.read_csv(os.path.join(loc,fname_n),low_memory=False)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # replace the node id by synthetic id \n",
    "    # ----------------------------------------\n",
    "    def replace_id(val):      \n",
    "        num = val[1:]\n",
    "        t = val[0]\n",
    "        return df_n.loc[\n",
    "            (df_n['type']==t) & (df_n['ID']==val)\n",
    "        ]['synID'].values.tolist()\n",
    "        \n",
    "    df_e['n1'] = df_e['n1'].parallel_apply(replace_id)\n",
    "    df_e['n2'] = df_e['n2'].parallel_apply(replace_id)\n",
    "    \n",
    "    graph_data = {}\n",
    "    print('Types of edges', set(df_e['etype']))\n",
    "    \n",
    "    for et in set(df_e['etype']):\n",
    "        et_R = et[::-1]\n",
    "        tmp = df_e.loc[df_e['etype']==et]\n",
    "        n1 = tmp['n1'].values.tolist()\n",
    "        n2 = tmp['n2'].values.tolist()\n",
    "        _list = []\n",
    "        _list_R = []\n",
    "        for i,j in zip(n1,n2):\n",
    "            _list.append((i[0], j[0]))\n",
    "            _list_R.append((j[0], i[0]))\n",
    "        graph_data [(et[0], et, et[1])] = _list \n",
    "        graph_data [(et[1], et_R, et[0])] = _list_R  \n",
    "    return graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of edges {'PA', 'PC', 'PT'}\n",
      "Node types, edge types ['A', 'C', 'P', 'T'] ['AP', 'CP', 'PA', 'PC', 'PT', 'TP']\n",
      "Graph :: Graph(num_nodes={'A': 4057, 'C': 20, 'P': 14328, 'T': 3590},\n",
      "      num_edges={('A', 'AP', 'P'): 19645, ('C', 'CP', 'P'): 14328, ('P', 'PA', 'A'): 19645, ('P', 'PC', 'C'): 14328, ('P', 'PT', 'T'): 81823, ('T', 'TP', 'P'): 81823},\n",
      "      metagraph=[('A', 'P', 'AP'), ('P', 'A', 'PA'), ('P', 'C', 'PC'), ('P', 'T', 'PT'), ('C', 'P', 'CP'), ('T', 'P', 'TP')])\n"
     ]
    }
   ],
   "source": [
    "graph_data = read_DBLP_data()\n",
    "g = dgl.heterograph(graph_data)\n",
    "print('Node types, edge types', g.ntypes, g.etypes)\n",
    "print('Graph ::', g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FILE = \"./dblp_graph_obj.dgl\"\n",
    "save_graphs(SAVE_FILE, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g,_ = load_graphs(SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_obj = g[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node types, edge types ['A', 'C', 'P', 'T'] ['AP', 'CP', 'PA', 'PC', 'PT', 'TP']\n",
      "Graph :: Graph(num_nodes={'A': 4057, 'C': 20, 'P': 14328, 'T': 3590},\n",
      "      num_edges={('A', 'AP', 'P'): 19645, ('C', 'CP', 'P'): 14328, ('P', 'PA', 'A'): 19645, ('P', 'PC', 'C'): 14328, ('P', 'PT', 'T'): 81823, ('T', 'TP', 'P'): 81823},\n",
      "      metagraph=[('A', 'P', 'AP'), ('P', 'A', 'PA'), ('P', 'C', 'PC'), ('P', 'T', 'PT'), ('C', 'P', 'CP'), ('T', 'P', 'TP')])\n"
     ]
    }
   ],
   "source": [
    "print('Node types, edge types', graph_obj.ntypes, graph_obj.etypes)\n",
    "print('Graph ::', graph_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "metapaths = {\n",
    "    'C' : ['CP', 'PC' ],\n",
    "    'T' : ['TP', 'PT' ],\n",
    "    'P' : ['PT', 'TP', 'PC' , 'CP']\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RW_list(graph_obj, metapaths, prefix=True):\n",
    "    def add_prefix(prefix, val):\n",
    "        return prefix + str(val)\n",
    "    \n",
    "    node_typeID2typename = {}\n",
    "    for e in  enumerate(graph_obj.ntypes): \n",
    "        node_typeID2typename[e[0]]= e[1]\n",
    "    \n",
    "    RW_list =[]\n",
    "    for ntype, mp in metapaths.items():\n",
    "        mp =  mp * 10\n",
    "        RW_mp = dgl.sampling.random_walk(\n",
    "            graph_obj,\n",
    "            metapath= mp,\n",
    "            nodes = graph_obj.nodes(ntype),\n",
    "        )\n",
    "        print(RW_mp[1])\n",
    "        _random_walks = RW_mp[0].data.numpy()\n",
    "        if prefix:\n",
    "            pattern = RW_mp[1].data.numpy().tolist()\n",
    "            pattern = [node_typeID2typename[_] for _ in pattern ]\n",
    "            print(pattern)\n",
    "            vectorized_func = np.vectorize(add_prefix)\n",
    "            _random_walks = vectorized_func( patterlengthn, _random_walks)\n",
    "           \n",
    "        RW_list.extend(_random_walks.tolist())\n",
    "        \n",
    "    return RW_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1])\n",
      "['C', 'P', 'C', 'P', 'C', 'P', 'C', 'P', 'C', 'P', 'C', 'P', 'C', 'P', 'C', 'P', 'C', 'P', 'C', 'P', 'C']\n",
      "tensor([3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3])\n",
      "['T', 'P', 'T', 'P', 'T', 'P', 'T', 'P', 'T', 'P', 'T', 'P', 'T', 'P', 'T', 'P', 'T', 'P', 'T', 'P', 'T']\n",
      "tensor([2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1,\n",
      "        2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2])\n",
      "['P', 'T', 'P', 'C', 'P', 'T', 'P', 'C', 'P', 'T', 'P', 'C', 'P', 'T', 'P', 'C', 'P', 'T', 'P', 'C', 'P', 'T', 'P', 'C', 'P', 'T', 'P', 'C', 'P', 'T', 'P', 'C', 'P', 'T', 'P', 'C', 'P', 'T', 'P', 'C', 'P']\n"
     ]
    }
   ],
   "source": [
    "random_walks = get_RW_list(graph_obj, metapaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import multiprocessing as mp\n",
    "\n",
    "cpu_count = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subed = 0\n",
    "            \n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
    "        self.epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2vec(random_walks, epochs = 100):\n",
    "    cpu_count = mp.cpu_count()\n",
    "    model = Word2Vec(\n",
    "        random_walks, \n",
    "        size=128, \n",
    "        window=3, \n",
    "        negative = 10,\n",
    "        sg = 1,\n",
    "        min_count=1, \n",
    "        iter = epochs,\n",
    "        compute_loss=True,\n",
    "        callbacks= [loss_callback()]\n",
    "    )\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 2959041.25\n",
      "Loss after epoch 1: 1616846.75\n",
      "Loss after epoch 2: 1539773.5\n",
      "Loss after epoch 3: 1494218.5\n",
      "Loss after epoch 4: 1329412.0\n",
      "Loss after epoch 5: 1068293.0\n",
      "Loss after epoch 6: 1019163.0\n",
      "Loss after epoch 7: 1126825.0\n",
      "Loss after epoch 8: 1063675.0\n",
      "Loss after epoch 9: 925662.0\n",
      "Loss after epoch 10: 1026505.0\n",
      "Loss after epoch 11: 965177.0\n",
      "Loss after epoch 12: 907416.0\n",
      "Loss after epoch 13: 795062.0\n",
      "Loss after epoch 14: 781654.0\n",
      "Loss after epoch 15: 731722.0\n",
      "Loss after epoch 16: 750374.0\n",
      "Loss after epoch 17: 710116.0\n",
      "Loss after epoch 18: 652882.0\n",
      "Loss after epoch 19: 711134.0\n",
      "Loss after epoch 20: 694964.0\n",
      "Loss after epoch 21: 627070.0\n",
      "Loss after epoch 22: 689686.0\n",
      "Loss after epoch 23: 614478.0\n",
      "Loss after epoch 24: 646726.0\n",
      "Loss after epoch 25: 669510.0\n",
      "Loss after epoch 26: 611840.0\n",
      "Loss after epoch 27: 662960.0\n",
      "Loss after epoch 28: 653640.0\n",
      "Loss after epoch 29: 650250.0\n",
      "Loss after epoch 30: 567358.0\n",
      "Loss after epoch 31: 658266.0\n",
      "Loss after epoch 32: 578638.0\n",
      "Loss after epoch 33: 569578.0\n",
      "Loss after epoch 34: 636776.0\n",
      "Loss after epoch 35: 636152.0\n",
      "Loss after epoch 36: 627532.0\n",
      "Loss after epoch 37: 611364.0\n",
      "Loss after epoch 38: 410936.0\n",
      "Loss after epoch 39: 384344.0\n",
      "Loss after epoch 40: 431892.0\n",
      "Loss after epoch 41: 431756.0\n",
      "Loss after epoch 42: 368744.0\n",
      "Loss after epoch 43: 379112.0\n",
      "Loss after epoch 44: 421868.0\n",
      "Loss after epoch 45: 380308.0\n",
      "Loss after epoch 46: 373356.0\n",
      "Loss after epoch 47: 369540.0\n",
      "Loss after epoch 48: 369740.0\n",
      "Loss after epoch 49: 366004.0\n",
      "Loss after epoch 50: 412616.0\n",
      "Loss after epoch 51: 404188.0\n",
      "Loss after epoch 52: 363652.0\n",
      "Loss after epoch 53: 362636.0\n",
      "Loss after epoch 54: 409768.0\n",
      "Loss after epoch 55: 411576.0\n",
      "Loss after epoch 56: 358604.0\n",
      "Loss after epoch 57: 376500.0\n",
      "Loss after epoch 58: 393492.0\n",
      "Loss after epoch 59: 384600.0\n",
      "Loss after epoch 60: 354484.0\n",
      "Loss after epoch 61: 381188.0\n",
      "Loss after epoch 62: 397624.0\n",
      "Loss after epoch 63: 350124.0\n",
      "Loss after epoch 64: 401152.0\n",
      "Loss after epoch 65: 391596.0\n",
      "Loss after epoch 66: 346192.0\n",
      "Loss after epoch 67: 345984.0\n",
      "Loss after epoch 68: 391400.0\n",
      "Loss after epoch 69: 340516.0\n",
      "Loss after epoch 70: 344268.0\n",
      "Loss after epoch 71: 373928.0\n",
      "Loss after epoch 72: 377780.0\n",
      "Loss after epoch 73: 376484.0\n",
      "Loss after epoch 74: 344104.0\n",
      "Loss after epoch 75: 372552.0\n",
      "Loss after epoch 76: 370440.0\n",
      "Loss after epoch 77: 363572.0\n",
      "Loss after epoch 78: 368984.0\n",
      "Loss after epoch 79: 340872.0\n",
      "Loss after epoch 80: 360916.0\n",
      "Loss after epoch 81: 366788.0\n",
      "Loss after epoch 82: 349408.0\n",
      "Loss after epoch 83: 362356.0\n",
      "Loss after epoch 84: 334600.0\n",
      "Loss after epoch 85: 343320.0\n",
      "Loss after epoch 86: 354704.0\n",
      "Loss after epoch 87: 327328.0\n",
      "Loss after epoch 88: 325236.0\n",
      "Loss after epoch 89: 348508.0\n",
      "Loss after epoch 90: 356700.0\n",
      "Loss after epoch 91: 321312.0\n",
      "Loss after epoch 92: 321728.0\n",
      "Loss after epoch 93: 320548.0\n",
      "Loss after epoch 94: 319440.0\n",
      "Loss after epoch 95: 351856.0\n",
      "Loss after epoch 96: 337316.0\n",
      "Loss after epoch 97: 346072.0\n",
      "Loss after epoch 98: 354868.0\n",
      "Loss after epoch 99: 338036.0\n"
     ]
    }
   ],
   "source": [
    "model = node2vec(random_walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"node2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_vectors(w2v_model):\n",
    "    vectors_dict = {} \n",
    "    for node, vec  in  w2v_model.wv.vocab.items():\n",
    "        _type = node[0]\n",
    "        _id = int(node[1:])\n",
    "        if _id < 0 : continue\n",
    "     \n",
    "        if _type not in vectors_dict.keys(): vectors_dict[_type]= {}\n",
    "        vectors_dict[_type][_id] = w2v_model.wv[node]\n",
    "    return vectors_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectors_dict =  get_node_vectors(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors_dict['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the node2vec vectors\n",
    "with open(\"node2vec_vectors_dblp.pkl\",'wb') as fh:\n",
    "    pickle.dump(vectors_dict, fh, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'A': 4057, 'C': 20, 'P': 14328, 'T': 3590},\n",
       "      num_edges={('A', 'AP', 'P'): 19645, ('C', 'CP', 'P'): 14328, ('P', 'PA', 'A'): 19645, ('P', 'PC', 'C'): 14328, ('P', 'PT', 'T'): 81823, ('T', 'TP', 'P'): 81823},\n",
       "      metagraph=[('A', 'P', 'AP'), ('P', 'A', 'PA'), ('P', 'C', 'PC'), ('P', 'T', 'PT'), ('C', 'P', 'CP'), ('T', 'P', 'TP')])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     1,     2,  ..., 14325, 14326, 14327])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_obj.nodes('P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 128)\n",
      "(14328, 128)\n",
      "(3590, 128)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------\n",
    "# Assign the vectors to nodes \n",
    "# ---------------------------------\n",
    "for _type,vec_dict in vectors_dict.items():\n",
    "    # Sort by keys\n",
    "    vec_dict = {k: vec_dict[k] for k in sorted(vec_dict)}\n",
    "    arr = np.array(list(vec_dict.values()))\n",
    "    print(arr.shape)\n",
    "    graph_obj.nodes[_type].data['mp2v'] = FT(arr)\n",
    "    graph_obj.nodes[_type].data['n2v'] = FT(np.zeros(len(vec_dict)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeSpace(data={'n2v': tensor([0., 0., 0.,  ..., 0., 0., 0.]), 'mp2v': tensor([[-0.2676,  0.0374,  0.7205,  ..., -0.0482,  0.5645, -0.7812],\n",
       "        [-1.2595,  0.8070,  0.5722,  ..., -0.6487, -0.1616, -0.9677],\n",
       "        [-0.7353,  0.8474,  0.1744,  ...,  0.0428, -0.2648, -0.4524],\n",
       "        ...,\n",
       "        [-0.6150,  0.7447,  0.0518,  ..., -0.0323,  0.8357, -0.2118],\n",
       "        [ 0.1136,  1.6048,  0.6605,  ..., -0.7495, -0.6699,  0.0742],\n",
       "        [-1.1061, -0.3933, -0.3096,  ..., -1.1272,  0.2970,  0.7285]])})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_obj.nodes['P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
