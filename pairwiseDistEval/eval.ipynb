{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--DIR {us_import1,us_import2,us_import3,us_import4}]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/ddatta/.local/share/jupyter/runtime/kernel-5adc8ce9-3bc7-462d-b894-5e632b7e7afc.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/hitl_1/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./..')\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pickle\n",
    "import glob\n",
    "from itertools import combinations\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "from collections import  Counter\n",
    "import argparse\n",
    "\n",
    "DIR = None\n",
    "DATA_LOC = None\n",
    "train_data_loc = None\n",
    "test_data_loc = None\n",
    "anomaly_data_loc = None\n",
    "domain_dims = None\n",
    "dist_df_dict = None\n",
    "\n",
    "\n",
    "def set_up(_DIR):\n",
    "    \n",
    "    global DIR, DATA_LOC, train_data_loc, test_data_loc, anomaly_data_loc, domain_dims, dist_df_dict\n",
    " \n",
    "    with open('config.yaml', 'r') as fh:\n",
    "        config = yaml.safe_load(fh)\n",
    "        \n",
    "    DATA_LOC = config['DATA_LOC']\n",
    "    pairWiseDist_dir = os.path.join(config['pairWiseDist_dir'],DIR)\n",
    "    anomaly_data_loc = os.path.join(config['anomaly_data_loc'],DIR)\n",
    "    train_data_loc = os.path.join(DATA_LOC, DIR, 'train_data.csv')\n",
    "    test_data_loc = os.path.join(DATA_LOC, DIR, 'test_data.csv')\n",
    "    with open(os.path.join(DATA_LOC, DIR, 'domain_dims.pkl'.format(DIR)), 'rb')  as fh:\n",
    "        domain_dims = pickle.load(fh)\n",
    "    dist_df_dict = {}\n",
    "    for _file in glob.glob(os.path.join(config['pairWiseDist_dir'],DIR,'**.csv')):\n",
    "         \n",
    "        _filename = os.path.split(_file)[-1].split('.')[0]\n",
    "        _parts = _filename.split('_')\n",
    "        key = (_parts[1],_parts[2])\n",
    "     \n",
    "        dist_df_dict[key] = pd.read_csv(_file,index_col=None)    \n",
    "    return\n",
    "\n",
    "def analyze_record(\n",
    "    record,\n",
    "    top = 5\n",
    "):\n",
    "    global reference_df\n",
    "    global domain_dims\n",
    "    global dist_df_dict \n",
    "    tqdm._instances.clear()\n",
    "    try:\n",
    "        ref_row = reference_df.loc[reference_df['PanjivaRecordID']==record['PanjivaRecordID']].iloc[0]\n",
    "    except:\n",
    "        _id =  int( str(record['PanjivaRecordID']) + '0' )\n",
    "        ref_row = reference_df.loc[reference_df['PanjivaRecordID']==_id].iloc[0]\n",
    "        \n",
    "    wrong_domains = {}\n",
    "    for d in domain_dims.keys():\n",
    "        if ref_row[d]!= record[d]:\n",
    "            wrong_domains[d] = record[d]\n",
    "    \n",
    "    # format domain1,domain2, dist\n",
    "    record_entDist = []\n",
    "    for pair in combinations(list(domain_dims.keys()),2):\n",
    "        pair = sorted(pair)\n",
    "        d1,d2 = pair[0],pair[1]\n",
    "        key = (d1,d2)\n",
    "        tmp_df = dist_df_dict[key]\n",
    "        e1 = int(record[d1])\n",
    "        e2 = int(record[d2])\n",
    "        \n",
    "        _dist = tmp_df.loc[(tmp_df[d1]==e1)&(tmp_df[d2]==e2)]['dist'].values[0]\n",
    "        record_entDist.append([d1,d2,_dist]) \n",
    "        \n",
    "    result = list(sorted(record_entDist, key = lambda x: x[2], reverse=True))\n",
    "    wrong_domain_list = set(list(wrong_domains.keys()))\n",
    "    for item in result[:top]:\n",
    "        if len(wrong_domain_list.intersection(set(item[:2]))) > 0 : \n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    global DIR, DATA_LOC, train_data_loc, test_data_loc, anomaly_data_loc, domain_dims, dist_df_dict\n",
    "    pos_anomalies_0 = pd.read_csv(os.path.join(anomaly_data_loc, 'pos_anomalies.csv') ,index_col=None).reset_index(drop=True)\n",
    "    neg_anomalies = pd.read_csv(os.path.join(anomaly_data_loc, 'neg_anomalies.csv') ,index_col=None)\n",
    "    reference_df = pd.read_csv(train_data_loc, index_col=None).reset_index(drop=True)\n",
    "    neg_anomalies['PanjivaRecordID'] = neg_anomalies['PanjivaRecordID'].apply(lambda x : int(str(x)[:-3]) ).reset_index(drop=True)\n",
    "    pos_anomalies['PanjivaRecordID'] = pos_anomalies_0['PanjivaRecordID'].apply(lambda x : int(re.subn('00\\d{1,2}$', '', str(x))[0]) )\n",
    "\n",
    "    anomalies = neg_anomalies.append(pos_anomalies, ignore_index=True)\n",
    "    anomalies = anomalies.reset_index(drop=True)\n",
    "    top_Values= [ 1,2,3,4,5 ]\n",
    "    results = {}\n",
    "    for top in top_Values:\n",
    "        res_neg = Parallel(n_jobs=mp.cpu_count())(\n",
    "            delayed(analyze_record)(neg_anomalies.iloc[i],top)  for i in tqdm(range(neg_anomalies.shape[0])))\n",
    "\n",
    "        res_pos = Parallel(n_jobs = mp.cpu_count())(\n",
    "            delayed(analyze_record)(pos_anomalies.iloc[i])  for i in tqdm(range(pos_anomalies.shape[0])))\n",
    "\n",
    "        res = res_neg + res_pos\n",
    "        _dict = Counter(res)\n",
    "        acc = _dict[True]/(_dict[True]+_dict[False])\n",
    "        results[top] = acc\n",
    "    result_path = os.path.join(DIR,'results')\n",
    "    Path(result_path).mkdir(exist_ok=True, parents=True)  \n",
    "    \n",
    "    with open(result_path, \"w\") as fh:  \n",
    "        json.dump(results, fh) \n",
    "    return results\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--DIR',\n",
    "    choices=['us_import1', 'us_import2', 'us_import3', 'us_import4'],\n",
    "    default=None\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "args = parser.parse_args()\n",
    "DIR = args.DIR\n",
    "set_up(DIR)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dictionary ={  \n",
    "    \"name\" : \"sathiyajith\",  \n",
    "    \"rollno\" : 56,  \n",
    "    \"cgpa\" : 8.6,  \n",
    "    \"phonenumber\" : \"9976770500\"\n",
    "}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
