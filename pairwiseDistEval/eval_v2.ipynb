{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./..')\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pickle\n",
    "import glob\n",
    "from itertools import combinations\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "from collections import  Counter\n",
    "import argparse\n",
    "from operator import itemgetter\n",
    "import json\n",
    "DIR = None\n",
    "DATA_LOC = None\n",
    "train_data_loc = None\n",
    "test_data_loc = None\n",
    "anomaly_data_loc = None\n",
    "domain_dims = None\n",
    "dist_df_dict = None\n",
    "reference_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up(_DIR):\n",
    "    \n",
    "    global DIR, DATA_LOC, train_data_loc, test_data_loc, anomaly_data_loc, domain_dims, dist_df_dict, reference_df\n",
    " \n",
    "    with open('config.yaml', 'r') as fh:\n",
    "        config = yaml.safe_load(fh)\n",
    "        \n",
    "    DATA_LOC = config['DATA_LOC']\n",
    "    pairWiseDist_dir = os.path.join(config['pairWiseDist_dir'],DIR)\n",
    "    anomaly_data_loc = os.path.join(config['anomaly_data_loc'],DIR)\n",
    "    train_data_loc = os.path.join(DATA_LOC, DIR, 'train_data.csv')\n",
    "    test_data_loc = os.path.join(DATA_LOC, DIR, 'test_data.csv')\n",
    "    with open(os.path.join(DATA_LOC, DIR, 'domain_dims.pkl'.format(DIR)), 'rb')  as fh:\n",
    "        domain_dims = pickle.load(fh)\n",
    "    dist_df_dict = {}\n",
    "    for _file in glob.glob(os.path.join(config['pairWiseDist_dir'],DIR,'**.csv')):\n",
    "         \n",
    "        _filename = os.path.split(_file)[-1].split('.')[0]\n",
    "        _parts = _filename.split('_')\n",
    "        key = (_parts[1],_parts[2])\n",
    "     \n",
    "        dist_df_dict[key] = pd.read_csv(_file,index_col=None)    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_record(\n",
    "    record\n",
    "):\n",
    "    global reference_df\n",
    "    global domain_dims\n",
    "    global dist_df_dict \n",
    "    tqdm._instances.clear()\n",
    "    try:\n",
    "        ref_row = reference_df.loc[reference_df['PanjivaRecordID']==record['PanjivaRecordID']].iloc[0]\n",
    "    except:\n",
    "        _id =  int( str(record['PanjivaRecordID']) + '0' )\n",
    "        ref_row = reference_df.loc[reference_df['PanjivaRecordID']==_id].iloc[0]\n",
    "        \n",
    "    wrong_domains = {}\n",
    "    for d in domain_dims.keys():\n",
    "        if ref_row[d]!= record[d]:\n",
    "            wrong_domains[d] = record[d]\n",
    "    wrong_domains_list = list(wrong_domains.keys())\n",
    "    \n",
    "    # format domain1,domain2, dist\n",
    "    record_entDist = []\n",
    "    for pair in combinations(list(domain_dims.keys()),2):\n",
    "        pair = sorted(pair)\n",
    "        d1,d2 = pair[0],pair[1]\n",
    "        key = (d1,d2)\n",
    "        tmp_df = dist_df_dict[key]\n",
    "        e1 = int(record[d1])\n",
    "        e2 = int(record[d2])\n",
    "        \n",
    "        _dist = tmp_df.loc[(tmp_df[d1]==e1)&(tmp_df[d2]==e2)]['dist'].values[0]\n",
    "        record_entDist.append([d1,d2,_dist]) \n",
    "        \n",
    "    record_entDist = list(sorted(record_entDist, key = lambda x: x[2], reverse=True))\n",
    "    wrong_domain_list = list(wrong_domains.keys())\n",
    "    print('>',wrong_domain_list)\n",
    "    print(record_entDist)\n",
    "    seen = set()\n",
    "    count = 0 \n",
    "    cur = 1\n",
    "    for item in record_entDist:\n",
    "        print(item)\n",
    "        _item = []\n",
    "        if item[0] in wrong_domain_list: \n",
    "            _item =  item[0]\n",
    "            seen = seen.union(set([_item]))\n",
    "        if  item[1] in wrong_domain_list:\n",
    "            _item =  item[1]\n",
    "            seen = seen.union(set([_item]))\n",
    "        print(seen)\n",
    "        if len(seen) == len(wrong_domain_list):\n",
    "            break\n",
    "        cur +=1\n",
    "    print(cur)   \n",
    "    return cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_anomalies = pd.read_csv(os.path.join(anomaly_data_loc, 'pos_anomalies.csv') ,index_col=None).reset_index(drop=True)\n",
    "neg_anomalies = pd.read_csv(os.path.join(anomaly_data_loc, 'neg_anomalies.csv') ,index_col=None)\n",
    "reference_df = pd.read_csv(train_data_loc, index_col=None).reset_index(drop=True)\n",
    "neg_anomalies['PanjivaRecordID'] = neg_anomalies['PanjivaRecordID'].apply(lambda x : int(str(x)[:-3]) ).reset_index(drop=True)\n",
    "pos_anomalies['PanjivaRecordID'] = pos_anomalies['PanjivaRecordID'].apply(lambda x : int(re.subn('00\\d{1,2}$', '', str(x))[0]) )\n",
    "\n",
    "anomalies = neg_anomalies.append(pos_anomalies, ignore_index=True)\n",
    "anomalies = anomalies.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    global DIR, DATA_LOC, train_data_loc, test_data_loc, anomaly_data_loc, domain_dims, dist_df_dict, reference_df\n",
    "    pos_anomalies = pd.read_csv(os.path.join(anomaly_data_loc, 'pos_anomalies.csv') ,index_col=None).reset_index(drop=True)\n",
    "    neg_anomalies = pd.read_csv(os.path.join(anomaly_data_loc, 'neg_anomalies.csv') ,index_col=None)\n",
    "    reference_df = pd.read_csv(train_data_loc, index_col=None).reset_index(drop=True)\n",
    "    neg_anomalies['PanjivaRecordID'] = neg_anomalies['PanjivaRecordID'].apply(lambda x : int(str(x)[:-3]) ).reset_index(drop=True)\n",
    "    pos_anomalies['PanjivaRecordID'] = pos_anomalies['PanjivaRecordID'].apply(lambda x : int(re.subn('00\\d{1,2}$', '', str(x))[0]) )\n",
    "\n",
    "    anomalies = neg_anomalies.append(pos_anomalies, ignore_index=True)\n",
    "    anomalies = anomalies.reset_index(drop=True)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    res_neg = Parallel(n_jobs=mp.cpu_count())(\n",
    "        delayed(analyze_record)(neg_anomalies.iloc[i])  for i in tqdm(range(neg_anomalies.shape[0])))\n",
    "\n",
    "    res_pos = Parallel(n_jobs = mp.cpu_count())(\n",
    "        delayed(analyze_record)(pos_anomalies.iloc[i])  for i in tqdm(range(pos_anomalies.shape[0])))\n",
    "\n",
    "    res = res_neg + res_pos\n",
    "    res_dict = Counter(res)\n",
    "    \n",
    "    res = list(sorted(res_dict.items(), key = itemgetter(0), reverse=False))\n",
    "    total_count = sum([_[1] for _ in res])\n",
    "    c = 0\n",
    "    recall_dict = {}\n",
    "    for item in res:\n",
    "        c+= item[1]\n",
    "        recall_dict[item[0]] = c/total_count\n",
    "    \n",
    "    Path('results_3').mkdir(exist_ok=True, parents=True)\n",
    "    result_path = os.path.join('results_3',DIR)\n",
    "    Path(result_path).mkdir(exist_ok=True, parents=True)  \n",
    "    result_path = os.path.join('results_3',DIR,'results.json')\n",
    "    with open(result_path, \"w\") as fh:  \n",
    "        json.dump(recall_dict, fh) \n",
    "    return result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2940/2940 [02:05<00:00, 23.49it/s]\n",
      "100%|██████████| 980/980 [00:35<00:00, 27.66it/s]\n"
     ]
    }
   ],
   "source": [
    "DIR ='us_import1'\n",
    "set_up(DIR)\n",
    "res_dict = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2970/2970 [03:28<00:00, 14.21it/s]\n",
      "100%|██████████| 990/990 [01:06<00:00, 14.84it/s]\n"
     ]
    }
   ],
   "source": [
    "DIR ='us_import2'\n",
    "set_up(DIR)\n",
    "res_dict = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results_3/us_import1/results.json'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [03:19<00:00, 15.02it/s]\n",
      "100%|██████████| 1000/1000 [00:50<00:00, 19.89it/s]\n"
     ]
    }
   ],
   "source": [
    "DIR ='us_import3'\n",
    "set_up(DIR)\n",
    "res_dict = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [03:23<00:00, 14.77it/s]\n",
      "100%|██████████| 1000/1000 [01:08<00:00, 14.51it/s]\n"
     ]
    }
   ],
   "source": [
    "DIR ='us_import4'\n",
    "set_up(DIR)\n",
    "res_dict = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--DIR {us_import1,us_import2,us_import3,us_import4}]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/ddatta/.local/share/jupyter/runtime/kernel-5adc8ce9-3bc7-462d-b894-5e632b7e7afc.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/hitl_1/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--DIR',\n",
    "    choices=['us_import1', 'us_import2', 'us_import3', 'us_import4'],\n",
    "    default=None\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "args = parser.parse_args()\n",
    "DIR = args.DIR\n",
    "set_up(DIR)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dictionary ={  \n",
    "    \"name\" : \"sathiyajith\",  \n",
    "    \"rollno\" : 56,  \n",
    "    \"cgpa\" : 8.6,  \n",
    "    \"phonenumber\" : \"9976770500\"\n",
    "}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
