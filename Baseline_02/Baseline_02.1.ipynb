{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./..')\n",
    "sys.path.append('./../..')\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "import glob \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "try:\n",
    "    from common_utils import AD_result_fetcher\n",
    "except:\n",
    "    from .common_utils import AD_result_fetcher\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------\n",
    "## (Modified) implementatation of : Improve Blackbox Sequential Anomaly dtetctor Relevancy with Limited Human Feedback\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'us_import1'\n",
    "\n",
    "PERCENTILE_THRESHOLD = 100 - 5\n",
    "\n",
    "UPDATE_scoreUB_percentile =  10 \n",
    "UPDATE_scoreUB = 0.5\n",
    "id_col = 'PanjivaRecordID'\n",
    "attribute_SHIPPER = 'ShipperPanjivaID'\n",
    "attribute_CONSIGNEE = 'ConsigneePanjivaID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22935 22648\n",
      "95 0.4913615251753746\n"
     ]
    }
   ],
   "source": [
    "labelled_results = AD_result_fetcher.read_in_AD_result(DIR)\n",
    "score_threshold = np.percentile(labelled_results['score'],PERCENTILE_THRESHOLD)\n",
    "print(PERCENTILE_THRESHOLD, score_threshold)\n",
    "labelled_results = labelled_results.sort_values(by=['score'],ascending=False)\n",
    "\n",
    "working_df = labelled_results.copy()\n",
    "working_df['dynamic_score'] = labelled_results['score'].values\n",
    "# ------------------\n",
    "# Type conversion : to ensure no bugs\n",
    "# ------------------\n",
    "\n",
    "working_df['PanjivaRecordID'] = working_df['PanjivaRecordID'].astype(int)\n",
    "working_df['ConsigneePanjivaID'] = working_df['ConsigneePanjivaID'].astype(int)\n",
    "working_df['ShipperPanjivaID'] = working_df['ShipperPanjivaID'].astype(int)\n",
    "working_df = working_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_dims(DIR):\n",
    "    file_path = './../generated_data_v1/{}/domain_dims.pkl'.format(DIR)\n",
    "    with open(file_path,'rb') as fh:\n",
    "        dd = pickle.load(fh)\n",
    "    return dd\n",
    "\n",
    "def get_serial_mappingId(DIR):\n",
    "    file_path = './../generated_data_v1/{}/idMapping.csv'.format(DIR)\n",
    "    df = pd.read_csv(file_path,index_col=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4539567681357035\n",
      "Number of positive anomalies :  0\n"
     ]
    }
   ],
   "source": [
    "# The lowest score till which a new score should be calculated\n",
    "UPDATE_scoreUB = np.percentile(labelled_results['score'],100-UPDATE_scoreUB_percentile)\n",
    "print(UPDATE_scoreUB)\n",
    "\n",
    "print('Number of positive anomalies : ', len(labelled_results.loc[labelled_results['label']==1]))\n",
    "score_threshold = np.percentile(labelled_results['score'],PERCENTILE_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4002390717618687"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Select entitties from records with a MIN_ANOM_SCORE\n",
    "# Set to top 25 percentile\n",
    "# --------------------------------------------\n",
    "ENTITY_MIN_ANOM_SCORE = np.percentile(labelled_results['score'],100-25)\n",
    "ENTITY_MIN_ANOM_SCORE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_embedding(DIR, serialized=True):\n",
    "    global attribute_SHIPPER\n",
    "    global attribute_CONSIGNEE\n",
    "    valid_domains = [attribute_CONSIGNEE, attribute_SHIPPER]\n",
    "    \n",
    "    embedding_data_loc = './../createGraph_trade/saved_model_data/{}'.format(DIR)\n",
    "    files = sorted(glob.glob(os.path.join(embedding_data_loc,'**.npy')))\n",
    " \n",
    "    domain_dims = get_domain_dims(DIR)\n",
    "   \n",
    "   \n",
    "    serialIdMapping = get_serial_mappingId(DIR)\n",
    "    # ===========================================================================================================\n",
    "    # Create synthetic ids so that we can map from the entities of attribute_SHIPPER and attribute_CONSIGNEE only\n",
    "    # -----------------------------------------------------------------------------------------------------------\n",
    "    serialID_2_syntheticID = {}\n",
    "    \n",
    "    cur = 0\n",
    "    for domain in valid_domains:\n",
    "       \n",
    "        _df = serialIdMapping.loc[serialIdMapping['domain']==domain]\n",
    "        for e in enumerate(_df['serial_id'].values.tolist(),0):\n",
    "            _syn_id = e[0]+cur\n",
    "            serialID_2_syntheticID[e[1]] = _syn_id \n",
    "         \n",
    "        cur += len(_df)\n",
    "        print(max(serialID_2_syntheticID.keys()))\n",
    "        \n",
    "    num_entities = len(serialID_2_syntheticID) \n",
    "    combined_emb = None \n",
    "    for domain in valid_domains:\n",
    "        file = sorted(glob.glob(os.path.join(embedding_data_loc,'**{}.npy'.format(domain))))[0]\n",
    "        emb_data = np.load(file) \n",
    "        emb_dim  = emb_data.shape[1]\n",
    "        # Initialize :\n",
    "        if combined_emb is None: \n",
    "            combined_emb = np.zeros([num_entities,emb_dim])\n",
    "        _df = serialIdMapping.loc[serialIdMapping['domain'] == domain]\n",
    "        for i, row in _df.iterrows():\n",
    "            _serial_id = row['serial_id']\n",
    "            try:\n",
    "                _syn_id = serialID_2_syntheticID[_serial_id]\n",
    "                combined_emb[_syn_id] = emb_data[row['entity_id']]\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    return combined_emb, serialID_2_syntheticID\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4895\n",
      "11135\n"
     ]
    }
   ],
   "source": [
    "embedding, serialID_2_syntheticID = fetch_embedding(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Perform dimensionality reduction\n",
    "# ==================================\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "def reduce_dimension(data, target_dim=16):\n",
    "    svd_obj = TruncatedSVD (n_components = target_dim)\n",
    "    svd_obj.fit(data)\n",
    "    xformed = svd_obj.transform(data)\n",
    "    return xformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering( X, num_clusters = 2):\n",
    "#     model = MiniBatchKMeans(\n",
    "#         init='k-means++', \n",
    "#         n_clusters=num_clusters, \n",
    "#         verbose=0\n",
    "#     )\n",
    "    model = KMeans(\n",
    "        init='k-means++', \n",
    "        n_clusters=num_clusters, \n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    t0 = time()\n",
    "    model.fit(X)\n",
    "    t1 = time() - t0\n",
    "    print('Time taken {:.4f}'.format(t1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_custers( scored_df, embedding, serialID_2_syntheticID , ENTITY_MIN_ANOM_SCORE, reduced_dim_clustering=True, reduced_dim=16):\n",
    "    global attribute_CONSIGNEE\n",
    "    global attribute_SHIPPER\n",
    "    \n",
    "    valid_domains = [attribute_CONSIGNEE, attribute_SHIPPER]\n",
    "    \n",
    "    df = scored_df.loc[scored_df['score']>=ENTITY_MIN_ANOM_SCORE]\n",
    "    valid_entities = []\n",
    "    for d in valid_domains:\n",
    "        valid_entities.extend(df[d].values.tolist())\n",
    "      \n",
    "    valid_entities = list(set(sorted(valid_entities)))\n",
    "    # -----------------------------------------------------\n",
    "    # convert from serial_IDs to  clustering_syn_ID\n",
    "    # serial_IDs -> syntheticID -> access embedding value\n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    serialID_2_tmpSynID = {e[1]:e[0] for e in enumerate(valid_entities)}\n",
    "    clustering_data = np.zeros((len(valid_entities), embedding.shape[1]))\n",
    "    \n",
    "    for idx in valid_entities:\n",
    "        syn_id = serialID_2_syntheticID[idx]\n",
    "        tmpSynID = serialID_2_tmpSynID[idx]\n",
    "        clustering_data[tmpSynID] = embedding[syn_id]\n",
    "    \n",
    "    print(clustering_data.shape)\n",
    "    # Perform clustering\n",
    "    if reduced_dim_clustering:\n",
    "        clustering_data = reduce_dimension(clustering_data, target_dim=reduced_dim)\n",
    "    \n",
    "   \n",
    "    max_silhoutte = -10\n",
    "    clustering_model_obj = None\n",
    "    valid_K_values = list(range(2,11))\n",
    "   \n",
    "    for k in valid_K_values:\n",
    "        X = clustering_data.copy()\n",
    "        _model_obj = perform_clustering( X, num_clusters = k)\n",
    "        xformed = _model_obj.transform(X)\n",
    "        silhoutte = silhouette_score(xformed,_model_obj.labels_ )\n",
    "        print(' K = {}, silhoutte score {:.4f}'.format(k,silhoutte))\n",
    "       \n",
    "        if silhoutte > max_silhoutte:\n",
    "            max_silhoutte = silhoutte\n",
    "            clustering_model_obj = _model_obj\n",
    "      \n",
    "    # Create a mapping from serial_ID to cluster_id\n",
    "    tmpSynID_2_serialID = {v:k for k,v in serialID_2_tmpSynID.items()}\n",
    "    \n",
    "    num_clusters = len(set(clustering_model_obj.labels_))\n",
    "    serialID_clusterID_map = {}\n",
    "    labels = np.array(clustering_model_obj.labels_)\n",
    "    print(labels)\n",
    "    for idx in range(labels.shape[0]):\n",
    "        serial_id = tmpSynID_2_serialID[idx] \n",
    "        serialID_clusterID_map[serial_id] = clustering_model_obj.labels_[idx]\n",
    "        \n",
    "    return serialID_clusterID_map\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3835, 128)\n",
      "Time taken 0.0895\n",
      " K = 2, silhoutte score 0.3487\n",
      "Time taken 0.1039\n",
      " K = 3, silhoutte score 0.3376\n",
      "Time taken 0.1374\n",
      " K = 4, silhoutte score 0.2832\n",
      "Time taken 0.1741\n",
      " K = 5, silhoutte score 0.2632\n",
      "Time taken 0.1931\n",
      " K = 6, silhoutte score 0.2574\n",
      "Time taken 0.1659\n",
      " K = 7, silhoutte score 0.3094\n",
      "Time taken 0.3504\n",
      " K = 8, silhoutte score 0.3051\n",
      "Time taken 0.2439\n",
      " K = 9, silhoutte score 0.3188\n",
      "Time taken 0.2979\n",
      " K = 10, silhoutte score 0.3160\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "serialID_clusterID_map = obtain_custers( labelled_results, embedding, serialID_2_syntheticID , ENTITY_MIN_ANOM_SCORE, reduced_dim_clustering=True, reduced_dim=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "def calculate_cluster_relevancy(df_cur, serialID_clusterID_map, label_top_k=10 ):\n",
    "\n",
    "    global id_col\n",
    "    df_cur = df_cur.sort_values(by='score',ascending=False)\n",
    "    seen_ids = []\n",
    "    count = 0\n",
    "    labelled_df = None\n",
    "    idx = 0\n",
    "    for i,row in df_cur.iterrows():\n",
    "        _id = int(row[id_col])\n",
    "        idx = i\n",
    "        seen_ids.append(_id)\n",
    "        \n",
    "        if row['label']==1: \n",
    "            count +=1\n",
    "            if count > label_top_k : \n",
    "                break\n",
    "        \n",
    "    seen_ids = set(seen_ids)\n",
    "    labelled_df = df_cur.iloc[:idx,:]\n",
    "    unlabelled_df = df_cur.iloc[idx:,:]\n",
    "    \n",
    "    print('# of records revealed to find {} instances at the top :: {}'.format(\n",
    "        label_top_k, \n",
    "        len(labelled_df))\n",
    "    )\n",
    "    # ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of records revealed to find 10 instances at the top :: 20856\n"
     ]
    }
   ],
   "source": [
    "calculate_cluster_relevancy(working_df.copy(), serialID_clusterID_map )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "xformed = reduce_dimension(reduced_dimX,2)\n",
    "\n",
    "colors = ['r','b','g','c','m']\n",
    "for l in set(clustering_model_obj.labels_):\n",
    "    t = xformed[clustering_model_obj.labels_==l]\n",
    "    plt.scatter(t[:,0],t[:,1],color=colors[l],s=1, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10725698295519771"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(xformed,clustering_model_obj.labels_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
