{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from  networkx import bipartite\n",
    "sys.path.append('./../..')\n",
    "sys.path.append('./..')\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from pandarallel import pandarallel\n",
    "from gensim.models import KeyedVectors\n",
    "import multiprocessing as mp\n",
    "pandarallel.initialize()\n",
    "import re\n",
    "from IPython.core.display import display, HTML\n",
    "import yaml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "sys.path.append('./..')\n",
    "sys.path.append('./../..')\n",
    "from common_utils import utils\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "id_col = 'PanjivaRecordID'\n",
    "import networkx as nx\n",
    "import operator\n",
    "import collections\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from networkx.algorithms import community\n",
    "import json\n",
    "# -------------------------------------------------\n",
    "config_ANOM_PERC = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_SIMTYPE = 4  # This means how many similar \"types\" of a transaction should be generated should be there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CONFIG = None\n",
    "DIR_LOC = None\n",
    "CONFIG = None\n",
    "CONFIG_FILE = 'config.yaml'\n",
    "save_dir = None\n",
    "id_col = 'PanjivaRecordID'\n",
    "use_cols = None\n",
    "freq_bound = None\n",
    "attribute_columns = None\n",
    "DIR = None\n",
    "CUT_OFF = None\n",
    "company_attributes = ['ConsigneePanjivaID','ShipperPanjivaID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# To find Betwenness Centrality\n",
    "# -------------------\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Divide a list of nodes `l` in `n` chunks\"\"\"\n",
    "    l_c = iter(l)\n",
    "    while 1:\n",
    "        x = tuple(itertools.islice(l_c, n))\n",
    "        if not x:\n",
    "            return\n",
    "        yield x\n",
    "\n",
    "\n",
    "def betweenness_centrality_parallel(G, processes=None):\n",
    "    \"\"\"Parallel betweenness centrality  function\"\"\"\n",
    "    p = Pool(processes=processes)\n",
    "    node_divisor = len(p._pool) * 4\n",
    "    node_chunks = list(chunks(G.nodes(), int(G.order() / node_divisor)))\n",
    "    num_chunks = len(node_chunks)\n",
    "    bt_sc = p.starmap(\n",
    "        nx.betweenness_centrality_subset,\n",
    "        zip(\n",
    "            [G] * num_chunks,\n",
    "            node_chunks,\n",
    "            [list(G)] * num_chunks,\n",
    "            [True] * num_chunks,\n",
    "            [None] * num_chunks,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Reduce the partial solutions\n",
    "    bt_c = bt_sc[0]\n",
    "    for bt in bt_sc[1:]:\n",
    "        for n in bt:\n",
    "            bt_c[n] += bt[n]\n",
    "    return bt_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_config(_DIR=None):\n",
    "    global DIR\n",
    "    global CONFIG\n",
    "    global CONFIG_FILE\n",
    "    global use_cols\n",
    "    global num_neg_samples\n",
    "    global DATA_SOURCE\n",
    "    global DIR_LOC\n",
    "    global save_dir\n",
    "    global id_col\n",
    "    global attribute_columns\n",
    "\n",
    "    DATA_SOURCE = './../generated_data_v1/'\n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "\n",
    "    if _DIR is not None:\n",
    "        DIR = _DIR\n",
    "        CONFIG['DIR'] = _DIR\n",
    "    else:\n",
    "        DIR = CONFIG['DIR']\n",
    "\n",
    "    DIR_LOC = re.sub('[0-9]', '', DIR)\n",
    "    DATA_SOURCE = os.path.join(DATA_SOURCE, DIR)\n",
    "    _pathobj = Path(DATA_SOURCE)\n",
    "    _pathobj.mkdir(exist_ok=True, parents=True)\n",
    "    save_dir = 'stage_2'\n",
    "    save_dir = os.path.join(\n",
    "        DATA_SOURCE,\n",
    "        save_dir\n",
    "    )\n",
    "    \n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    use_cols = CONFIG[DIR]['use_cols']\n",
    "    _cols = list(use_cols)\n",
    "    _cols.remove(id_col)\n",
    "    attribute_columns = list(sorted(_cols))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'us_import4'\n",
    "set_up_config(DIR)\n",
    "\n",
    "Anomaly_Output_path = './../generated_data_v1/generated_anomalies/{}'.format(DIR)\n",
    "path = Path(Anomaly_Output_path)\n",
    "path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\n",
    "    os.path.join(DATA_SOURCE, 'train_data.csv'),\n",
    "    low_memory=False,\n",
    "    index_col=None\n",
    ")\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "    os.path.join(DATA_SOURCE, 'test_data.csv'),\n",
    "    low_memory=False,\n",
    "    index_col=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_degree_distribution(graph_obj):\n",
    "    degree_sequence = sorted([d for n, d in graph_obj.degree()], reverse=True)\n",
    "    try:\n",
    "        plt.title('Boxplot of degree')\n",
    "        plt.boxplot(degree_sequence)\n",
    "        plt.show()\n",
    "    except:\n",
    "        pass\n",
    "    degreeCount = collections.Counter(degree_sequence)\n",
    "    deg, cnt = zip(*degreeCount.items())\n",
    "    try:\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.bar(deg, cnt, width=0.80, color=\"b\")\n",
    "        plt.title(\"Degree Histogram\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(\"Degree\")\n",
    "        plt.show()\n",
    "    except:\n",
    "        pass\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_dims():\n",
    "    global DATA_SOURCE\n",
    "    \n",
    "    dom_dims_path = os.path.join(DATA_SOURCE, 'domain_dims.pkl')\n",
    "    with open(dom_dims_path,'rb') as fh:\n",
    "        domain_dims =  pickle.load(fh)\n",
    "    return domain_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:15<00:00,  1.96s/it]\n"
     ]
    }
   ],
   "source": [
    "target_df = utils.convert_to_serializedID_format( target_df = df_train, DIR=DIR, data_source_loc=None,REFRESH=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = target_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_complete_graph(df):\n",
    "    from itertools import combinations\n",
    "    domain_dims = get_domain_dims()\n",
    "    complete_G =nx.Graph()\n",
    "    edges = [\n",
    "        ['Carrier','ConsigneePanjivaID'],\n",
    "        ['Carrier','HSCode'],\n",
    "        ['Carrier','PortOfLading'],\n",
    "        ['Carrier','PortOfUnlading'],\n",
    "        ['Carrier','ShipperPanjivaID'],\n",
    "        ['ConsigneePanjivaID','HSCode'],\n",
    "        ['ConsigneePanjivaID','PortOfUnlading'],\n",
    "        ['ConsigneePanjivaID','ShipmentDestination'],\n",
    "        ['ConsigneePanjivaID','ShipperPanjivaID'],\n",
    "        ['HSCode','ShipmentDestination'],\n",
    "        ['HSCode','ShipmentOrigin'],\n",
    "        ['HSCode','ShipperPanjivaID'],\n",
    "        ['PortOfLading','ShipmentOrigin'],\n",
    "        ['PortOfLading','ShipperPanjivaID'],\n",
    "        ['PortOfUnlading','ShipmentDestination'],\n",
    "        ['ConsigneePanjivaID','ShipperPanjivaID']\n",
    "    ]\n",
    "    \n",
    "    for i,j in edges:\n",
    "        __tmp__ = df[[i,j]]\n",
    "        __tmp__ = __tmp__.drop_duplicates()\n",
    "        _i = __tmp__[i].values.tolist()\n",
    "        _j = __tmp__[j].values.tolist()\n",
    "        n1_n2 = [ ( a,b)for a,b in zip(_i,_j)]\n",
    "        complete_G.add_edges_from(n1_n2)\n",
    "    return complete_G\n",
    "        \n",
    "complete_G = build_complete_graph(target_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:16<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "df_test = utils.convert_to_serializedID_format( target_df = df_test, DIR=DIR, data_source_loc=None,REFRESH=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dims = get_domain_dims()\n",
    "entity_type_list = {\n",
    "    d : list(set(target_df[d])) for d in domain_dims.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_component_subgraph(graph_obj):\n",
    "    component_id = 0\n",
    "    components = {}\n",
    "    component_size_dict = {}\n",
    "\n",
    "    for c in nx.connected_components(graph_obj):\n",
    "        components[component_id] = c\n",
    "        component_size_dict[component_id] = len(c) \n",
    "        component_id += 1\n",
    "        \n",
    "    component_size_dict = sorted(component_size_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    # Get the largest connected component\n",
    "    max_component = components[component_size_dict[0][0]]\n",
    "    subgraph = graph_obj.subgraph(max_component)\n",
    "    return subgraph , component_size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Used to get approximate candidates \n",
    "# --------------------------------------\n",
    "def get_n2v_model(G):\n",
    "    global DIR\n",
    "    fname = DIR +'_node2vec'\n",
    "    from nodevectors import Node2Vec\n",
    "    if not os.path.exists(fname + '.zip'):\n",
    "        n2v = Node2Vec()\n",
    "        n2v.fit(G)\n",
    "        \n",
    "        n2v.save(DIR +'_node2vec')\n",
    "       \n",
    "    else:\n",
    "        n2v = Node2Vec.load( fname + '.zip')\n",
    "    return n2v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Returns idx, size of component, list of nodes\n",
    "# ------------------------------------------------\n",
    "\n",
    "def get_graph_components(graph_obj):\n",
    "    component_id = 0\n",
    "    components = {}\n",
    "    component_size_dict = {}\n",
    "\n",
    "    for c in nx.connected_components(graph_obj):\n",
    "        components[component_id] = c\n",
    "        component_size_dict[component_id] = len(c) \n",
    "        component_id += 1\n",
    "        \n",
    "    component_size_dict = sorted(component_size_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    components_dict = []\n",
    "    for i, _size in component_size_dict:\n",
    "        components_dict.append((i, _size,   components[i]))\n",
    "      \n",
    "    return components_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# Get ordered list of elements that are approx most dissimilar\n",
    "# ------------------------------------------------------------\n",
    "def query_ordered_dissimilar(ent_id, entity_type, entity_type_list , query_model, exclude_list= None):\n",
    "     \n",
    "    k = int(len(entity_type_list[entity_type]) * .20)\n",
    "    _dict = { e1: query_model.similarity(str(ent_id),str(e1)) for e1 in entity_type_list[entity_type] }\n",
    "    sorted_dict = sorted(_dict.items() ,key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    if exclude_list is not None:\n",
    "        res = [ _[0] for _ in sorted_dict[-k:] if _[0] not in exclude_list]\n",
    "    else:\n",
    "        res = [ _[0] for _ in sorted_dict[-k:] if _[0] ]\n",
    "    return res\n",
    "# Example : query_ordered_dissimilar(75, 'Carrier', entity_type_list , query_model)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def query_ordered_similar(ent_id, entity_type, entity_type_list , query_model, exclude_list= None, include_list = None):\n",
    "    k = int(len(entity_type_list[entity_type]) * .25)\n",
    "    _dict = { e1: query_model.similarity(str(ent_id),str(e1)) for e1 in entity_type_list[entity_type] }\n",
    "    sorted_dict = sorted(_dict.items() ,key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    if exclude_list is not None:\n",
    "        res = [ _[0] for _ in sorted_dict[:k] if _[0] not in exclude_list]\n",
    "    else:\n",
    "        res = [ _[0] for _ in sorted_dict[:k]  ]\n",
    "   \n",
    "    if include_list is not None:\n",
    "        res = [ _ for _ in res if _ in include_list]\n",
    "    else:\n",
    "        res = [ _ for _ in res ]\n",
    "        \n",
    "    return res\n",
    "# Example : query_ordered_similar(75, 'Carrier', entity_type_list , query_model)\n",
    "# -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Return non-cooccurring entities\n",
    "# Example : get_nonCoOcc(45, 'Carrier', 'HSCode'))\n",
    "# Return entity of type HSCode which does not co-occur with Carrier:45\n",
    "# --------------------------------------------------\n",
    "\n",
    "def get_nonCoOcc(ent_id, e_type, target_e_type):\n",
    "    global main_df\n",
    "    global entity_type_list\n",
    "    tmp = set(main_df[[e_type, target_e_type]].loc[main_df[e_type]==ent_id][target_e_type].values.tolist())\n",
    "    return list(set(entity_type_list[target_e_type]).difference(tmp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making walks... Done, T=5.55\n",
      "Mapping Walk Names... Done, T=10.76\n",
      "Training W2V... "
     ]
    }
   ],
   "source": [
    "n2v = get_n2v_model(complete_G.copy())\n",
    "# Save model to gensim.KeyedVector format\n",
    "n2v.save_vectors(\"{}_n2v_model.bin\".format(DIR))\n",
    "query_model = KeyedVectors.load_word2vec_format(\"{}_n2v_model.bin\".format(DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "company_cols = ['ConsigneePanjivaID', 'ShipperPanjivaID']\n",
    "\n",
    "df = target_df.copy()\n",
    "print('>> ', len(df))\n",
    "attributes = [ _ for _ in list(df.columns) if _ not in id_col]\n",
    "df = df.drop_duplicates(subset = attributes)\n",
    "df_subset = df[company_cols].groupby(\n",
    "    company_cols).size().reset_index(\n",
    "    name='count'\n",
    ").sort_values(by='count', ascending=False)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Create a bipartite graph\n",
    "# --------------------\n",
    "B = nx.Graph()\n",
    "B.add_nodes_from(set(df_subset['ConsigneePanjivaID'].values), bipartite=0)\n",
    "B.add_nodes_from(set(df_subset['ShipperPanjivaID'].values), bipartite=1)\n",
    "edges = []\n",
    "\n",
    "for i, j, k in zip(df_subset['ConsigneePanjivaID'].values,\n",
    "                   df_subset['ShipperPanjivaID'].values,\n",
    "                   df_subset['count'].values):\n",
    "    edges.append((i, j, {'weight': k}))\n",
    "\n",
    "B.add_edges_from(edges)\n",
    "print(' Is the bipartite graph of Comapnies connected ? ', nx.is_connected(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "B0, _ = get_largest_component_subgraph(B)\n",
    "print('Nodes in the largest component', B0.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_sorted = betweenness_centrality_parallel(B0)\n",
    "bt_sorted = sorted(bt_sorted.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "centrality_d = nx.degree_centrality(B0)\n",
    "centrality_d = sorted(centrality_d.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Top comapnies in terms of Between Ness centrality and Degree Centrality\n",
    "# Set K = 50\n",
    "# -----------------------------\n",
    "K = 100\n",
    "top_K_companies = set([_[0] for _ in  bt_sorted[:K] + centrality_d[:K]])\n",
    "top_K_companies = list(set(top_K_companies))\n",
    "tmp1 = df.loc[(df['ConsigneePanjivaID'].isin(top_K_companies)) | (df['ShipperPanjivaID'].isin(top_K_companies))].copy()\n",
    "\n",
    "# The set of HS codes that are mostly transacted\n",
    "popular_hscodes = set(tmp1['HSCode'])\n",
    "hscode_degree_dict = {node_hsc : complete_G.degree(node_hsc) for node_hsc in popular_hscodes}\n",
    "K = int(len(hscode_degree_dict)*0.5)\n",
    "top_K_hscodes = [_[0] for _ in sorted(hscode_degree_dict.items() , key = operator.itemgetter(1), reverse=True)][:K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# To get community/clusters of nodes ,\n",
    "# We remove some highly visible nodes\n",
    "# =====================================\n",
    "rmv = []\n",
    "K = 1000\n",
    "for sorted_dict in [ bt_sorted, centrality_d]:\n",
    "    rmv.extend( [ _[0] for _ in sorted_dict[:K] ])\n",
    "rmv = list(set(rmv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = nx.Graph(B0)\n",
    "print(B1.number_of_edges(), B1.number_of_nodes())\n",
    "\n",
    "B1.remove_nodes_from(rmv)\n",
    "print(B1.number_of_edges(), B1.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_comp, _components = get_largest_component_subgraph(B1)\n",
    "g_components = get_graph_components(B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set_PoL =  set(df['PortOfLading'])\n",
    "# PoL_distances = {}\n",
    "# for h in set_PoL:\n",
    "#     _dict = {_h:len(nx.shortest_path(complete_G, h,_h))-2 for _h in set_PoL if _h != h }\n",
    "#     PoL_distances[h] = sorted(_dict.items(), key = operator.itemgetter(1), reverse=True)\n",
    "domain_dims = get_domain_dims()\n",
    "domain_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_length = int(len(df_test)*config_ANOM_PERC/100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_candidate = None\n",
    "\n",
    "\n",
    "# Leave out the largest community\n",
    "for i in range(1,10):\n",
    "    g = B1.subgraph(g_components[i][-1])\n",
    "    # what are the entity ids ?\n",
    "    \n",
    "    # Omit nodes which have degree 1\n",
    "    _nodes_ = [ _ for _ in g.nodes() if g.degree(_) > 1]\n",
    "    tmp_df = df.loc[(df['ConsigneePanjivaID'].isin(_nodes_)) & (df['ShipperPanjivaID'].isin(_nodes_))].copy()\n",
    "  \n",
    "    _ratio = len(tmp_df) * config_SIMTYPE / required_length\n",
    "    print(len(tmp_df), _ratio)\n",
    "    if _ratio <= 1.3 and _ratio >= 0.75 :\n",
    "        print('Valid candidate found')\n",
    "        tmp_df = tmp_df.sample(n = int(required_length/config_SIMTYPE), replace=True) \n",
    "        print(\n",
    "            ' Length of the candidate df >>', \n",
    "            tmp_df.shape[0], \n",
    "            '# Consignee & Shipper ', \n",
    "            len(set(tmp_df['ConsigneePanjivaID'])), \n",
    "            len(set(tmp_df['ShipperPanjivaID']))\n",
    "        )\n",
    "        positive_candidate = tmp_df.copy().reset_index(drop=True)\n",
    "        candidate_df = tmp_df.copy()\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = B.subgraph(g_components[1][-1])\n",
    "# print(nx.info(g))\n",
    "# # what are the entity ids ?\n",
    "# list_e = list(g.nodes())\n",
    "# tmp_df = df.loc[(df['ConsigneePanjivaID'].isin(list_e)) & (df['ShipperPanjivaID'].isin(list_e))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(positive_candidate['HSCode'])\n",
    "# query_ordered_dissimilar(ent_id, entity_type, entity_type_list , query_model, exclude_list= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_aux(\n",
    "    cand_df, \n",
    "    idx, \n",
    "    num_samples, \n",
    "    perturb_extra= True\n",
    "):\n",
    "    global df\n",
    "    global query_model\n",
    "    global entity_type_list\n",
    "    global id_col\n",
    "    global domain_dims\n",
    "    global config_SIMTYPE # Ensue this is an even number \n",
    "    print(' IDX', idx)\n",
    "    # Choose the anchor point\n",
    "    a = np.random.uniform()\n",
    "    domain_combinations = { 1 : { \n",
    "            'company_anchor' : 'ConsigneePanjivaID',\n",
    "            'domain_set' : {\n",
    "            1 : ('HSCode',  'ShipmentOrigin'),\n",
    "            2 : ('Carrier', 'PortOfLading'),\n",
    "            3 : ('HSCode',  'Carrier')}},\n",
    "        2 : { \n",
    "            'company_anchor' : 'ShipperPanjivaID',\n",
    "            'domain_set' : {\n",
    "            1 : ( 'HSCode', 'ShipmentDestination'),\n",
    "            2 : ( 'Carrier', 'PortOfUnlading'),\n",
    "            3 : ( 'HSCode',  'Carrier'),\n",
    "            }\n",
    "        }\n",
    "    } \n",
    "    req_count = num_samples \n",
    "    c_idx = [1,2]\n",
    "    ds_idx = [1,2,3]\n",
    "\n",
    "    np.random.shuffle(c_idx)\n",
    "    np.random.shuffle(ds_idx)\n",
    "    _options  =[]\n",
    "    for i in c_idx:\n",
    "        for j in ds_idx:\n",
    "            _options.append((i,j))\n",
    "    for i_j in _options:\n",
    "        i = i_j[0]\n",
    "        j = i_j[1]\n",
    "        \n",
    "        company_anchor = domain_combinations[i]['company_anchor']\n",
    "        d_set = domain_combinations[i]['domain_set'][j]\n",
    "      \n",
    "        print(d_set)\n",
    "        if perturb_extra:\n",
    "            perturb_columns = [ _ for _ in domain_dims.keys() if _ not in ['ConsigneePanjivaID','ShipperPanjivaID'] + list(d_set) ]\n",
    "\n",
    "        # Company entity\n",
    "        company_ent = cand_df.loc[idx, company_anchor]\n",
    "        # entity that does not co-occur with company entity\n",
    "        a1 = get_nonCoOcc(ent_id=company_ent, e_type=company_anchor, target_e_type=d_set[0])\n",
    "        domain_0 = d_set[0]\n",
    "        domain_1 = d_set[1]\n",
    "        # entity that is most dissimilar to company entity : domain_0\n",
    "        a2 = query_ordered_dissimilar(\n",
    "            ent_id = company_ent, \n",
    "            entity_type = domain_0, \n",
    "            entity_type_list = entity_type_list,\n",
    "            query_model=query_model,\n",
    "            exclude_list= None\n",
    "        )\n",
    "        \n",
    "        a3 = list(set(a1).intersection(set(a2)))\n",
    "        \n",
    "        # entity that is most dissimilar to company entity : domain_1\n",
    "        b1 = get_nonCoOcc(ent_id = company_ent, e_type = company_anchor, target_e_type=domain_1)\n",
    "        b2 =  query_ordered_dissimilar(\n",
    "                ent_id = company_ent, \n",
    "                entity_type = domain_1, \n",
    "                entity_type_list = entity_type_list,\n",
    "                query_model=query_model,\n",
    "                exclude_list= None\n",
    "            )\n",
    "        b3 = list(set(b1).intersection(set(b2)))\n",
    "        if a3 is None or b3 is None or len(a3) == 0 or len(b3)==0  : \n",
    "            continue\n",
    "        e1_0 = np.random.choice(a3, size=1, replace=False)[0]\n",
    "        e2_0 = np.random.choice(b3, size=1, replace=False)[0]\n",
    "        \n",
    "        e1_ = query_ordered_similar(\n",
    "            ent_id = e1_0, \n",
    "            entity_type = d_set[0], \n",
    "            entity_type_list = entity_type_list,\n",
    "            query_model=query_model,\n",
    "            exclude_list= [e1_0],\n",
    "            include_list = list(a3)\n",
    "        )\n",
    "\n",
    "        e1 = [e1_0] + e1_[:req_count-1]\n",
    "        e2_ = query_ordered_similar(\n",
    "            ent_id = e2_0, \n",
    "            entity_type = d_set[1], \n",
    "            entity_type_list = entity_type_list,\n",
    "            query_model=query_model,\n",
    "            exclude_list= [e2_0],\n",
    "            include_list = list(b3)\n",
    "        )\n",
    "        e2 = [e2_0] + e2_[:req_count-1]\n",
    "        print( ' >>>' ,len(e1), len(e2))\n",
    "        try:\n",
    "            list_ent_1 = []\n",
    "            list_ent_2 = []\n",
    "\n",
    "            for _i in range(req_count):\n",
    "                if _i%2 == 0:\n",
    "                    list_ent_1.extend([e1[_i],e1[_i]])\n",
    "                    list_ent_2.extend([e2[_i]] )\n",
    "                if _i%2 == 1:\n",
    "                    list_ent_1.extend([e1[_i]])\n",
    "                    list_ent_2.extend([e2[_i],e2[_i]] )\n",
    "                if  len(list_ent_1) >= req_count  or len(list_ent_2)>=req_count:\n",
    "                    break\n",
    "            \n",
    "            res = []\n",
    "            cause = {}\n",
    "            print(list_ent_1, list_ent_2)\n",
    "            for i in range(req_count):\n",
    "                row_copy = cand_df.loc[idx].copy()\n",
    "                row_copy[id_col] = int(str(row_copy[id_col]) + '00{}'.format(i+1))\n",
    "                row_copy[domain_0]= list_ent_1[i]\n",
    "                row_copy[domain_1]= list_ent_2[i]\n",
    "                _p = np.random.choice(perturb_columns,size=1)[0]\n",
    "            \n",
    "                row_copy[_p] = np.random.choice(entity_type_list[_p],size=1)[0]\n",
    "                res.append(row_copy)\n",
    "                cause[str(row_copy[id_col])] = [(company_anchor,domain_0),(company_anchor,domain_1)]\n",
    "            res = pd.concat(res,axis=1).transpose().reset_index(drop=True)\n",
    "            return (res, cause)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    return (None, None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Positive samples\n",
    "# =========================\n",
    "\n",
    "candidate_df = candidate_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = config_SIMTYPE\n",
    "results = Parallel(\n",
    "    n_jobs = mp.cpu_count()\n",
    ")(delayed(generate_aux)( candidate_df, idx, num_samples, ) for idx in range(candidate_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = [] \n",
    "causes = {}\n",
    "for r in results:\n",
    "    if r[0] is not None:\n",
    "        df_results.append(r[0])\n",
    "        causes.update(r[1])\n",
    "        \n",
    "positive_anomalies = pd.concat(df_results).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Save the causes\n",
    "# ---------------------------\n",
    "pos_anom_causes_file = os.path.join(Anomaly_Output_path,'pos_anomalies_explantions.json')\n",
    "print(pos_anom_causes_file)\n",
    "with open(pos_anom_causes_file, \"w\") as fh:  \n",
    "    json.dump(causes, fh) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Negative anomalies\n",
    "# ==========================\n",
    "# Consider the largest component\n",
    "g = B1.subgraph(g_components[0][-1])\n",
    "_nodes_ = [ _ for _ in g.nodes() if g.degree(_) > 1]\n",
    "tmp_df = df.loc[(df['ConsigneePanjivaID'].isin(_nodes_)) & (df['ShipperPanjivaID'].isin(_nodes_))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_candidates = tmp_df.sample(positive_anomalies.shape[0],replace=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_perturbed_aux(cand_df, idx, num_pert = 3):\n",
    "    global entity_type_list\n",
    "    global id_col\n",
    "    global company_attributes\n",
    "    row_copy = cand_df.loc[idx].copy()\n",
    "    perturb_domains = [ _ for _ in list(entity_type_list.keys()) if _ not in company_attributes ]\n",
    "    perturb_domains = np.random.choice(perturb_domains, num_pert)\n",
    "    \n",
    "    for i in range(num_pert):\n",
    "        _dom = perturb_domains[i]\n",
    "        row_copy[_dom] = np.random.choice(entity_type_list[_dom],size=1)[0]\n",
    "    row_copy[id_col] = int(str(row_copy[id_col]) + '00{}'.format(1))\n",
    "    res = pd.concat([row_copy],axis=1).transpose().reset_index(drop=True)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = Parallel(\n",
    "    n_jobs = mp.cpu_count()\n",
    "    )(delayed(generate_perturbed_aux)( \n",
    "        negative_candidates, idx, \n",
    "    ) for idx in range(negative_candidates.shape[0])\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_anomalies = pd.concat(res).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Save the data frames\n",
    "# ==========================\n",
    "neg_anom_f_path = os.path.join(Anomaly_Output_path,'neg_anomalies.csv')\n",
    "pos_anom_f_path = os.path.join(Anomaly_Output_path,'pos_anomalies.csv')\n",
    "negative_anomalies.to_csv(neg_anom_f_path,index=None)\n",
    "positive_anomalies.to_csv(pos_anom_f_path,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
