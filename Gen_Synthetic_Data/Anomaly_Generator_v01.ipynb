{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from  networkx import bipartite\n",
    "sys.path.append('./../..')\n",
    "sys.path.append('./..')\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from pandarallel import pandarallel\n",
    "from gensim.models import KeyedVectors\n",
    "import multiprocessing as mp\n",
    "pandarallel.initialize()\n",
    "import re\n",
    "from IPython.core.display import display, HTML\n",
    "import yaml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "sys.path.append('./..')\n",
    "sys.path.append('./../..')\n",
    "from common_utils import utils\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "id_col = 'PanjivaRecordID'\n",
    "import networkx as nx\n",
    "import operator\n",
    "import collections\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from networkx.algorithms import community\n",
    "import json\n",
    "# -------------------------------------------------\n",
    "config_ANOM_PERC = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_SIMTYPE = 4  # This means how many similar \"types\" of a transaction should be generated should be there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CONFIG = None\n",
    "DIR_LOC = None\n",
    "CONFIG = None\n",
    "CONFIG_FILE = 'config.yaml'\n",
    "save_dir = None\n",
    "id_col = 'PanjivaRecordID'\n",
    "use_cols = None\n",
    "freq_bound = None\n",
    "attribute_columns = None\n",
    "DIR = None\n",
    "CUT_OFF = None\n",
    "company_attributes = ['ConsigneePanjivaID','ShipperPanjivaID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# To find Betwenness Centrality\n",
    "# -------------------\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Divide a list of nodes `l` in `n` chunks\"\"\"\n",
    "    l_c = iter(l)\n",
    "    while 1:\n",
    "        x = tuple(itertools.islice(l_c, n))\n",
    "        if not x:\n",
    "            return\n",
    "        yield x\n",
    "\n",
    "\n",
    "def betweenness_centrality_parallel(G, processes=None):\n",
    "    \"\"\"Parallel betweenness centrality  function\"\"\"\n",
    "    p = Pool(processes=processes)\n",
    "    node_divisor = len(p._pool) * 4\n",
    "    node_chunks = list(chunks(G.nodes(), int(G.order() / node_divisor)))\n",
    "    num_chunks = len(node_chunks)\n",
    "    bt_sc = p.starmap(\n",
    "        nx.betweenness_centrality_subset,\n",
    "        zip(\n",
    "            [G] * num_chunks,\n",
    "            node_chunks,\n",
    "            [list(G)] * num_chunks,\n",
    "            [True] * num_chunks,\n",
    "            [None] * num_chunks,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Reduce the partial solutions\n",
    "    bt_c = bt_sc[0]\n",
    "    for bt in bt_sc[1:]:\n",
    "        for n in bt:\n",
    "            bt_c[n] += bt[n]\n",
    "    return bt_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_config(_DIR=None):\n",
    "    global DIR\n",
    "    global CONFIG\n",
    "    global CONFIG_FILE\n",
    "    global use_cols\n",
    "    global num_neg_samples\n",
    "    global DATA_SOURCE\n",
    "    global DIR_LOC\n",
    "    global save_dir\n",
    "    global id_col\n",
    "    global attribute_columns\n",
    "\n",
    "    DATA_SOURCE = './../generated_data_v1/'\n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "\n",
    "    if _DIR is not None:\n",
    "        DIR = _DIR\n",
    "        CONFIG['DIR'] = _DIR\n",
    "    else:\n",
    "        DIR = CONFIG['DIR']\n",
    "\n",
    "    DIR_LOC = re.sub('[0-9]', '', DIR)\n",
    "    DATA_SOURCE = os.path.join(DATA_SOURCE, DIR)\n",
    "    save_dir = '__2'\n",
    "    save_dir = os.path.join(\n",
    "        DATA_SOURCE,\n",
    "        save_dir\n",
    "    )\n",
    "    \n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    use_cols = CONFIG[DIR]['use_cols']\n",
    "    _cols = list(use_cols)\n",
    "    _cols.remove(id_col)\n",
    "    attribute_columns = list(sorted(_cols))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'us_import2'\n",
    "set_up_config(DIR)\n",
    "\n",
    "Anomaly_Output_path = './../generated_data_v1/generated_anomalies/{}'.format(DIR)\n",
    "path = Path(Anomaly_Output_path)\n",
    "path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\n",
    "    os.path.join(DATA_SOURCE, 'train_data.csv'),\n",
    "    low_memory=False,\n",
    "    index_col=None\n",
    ")\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "    os.path.join(DATA_SOURCE, 'test_data.csv'),\n",
    "    low_memory=False,\n",
    "    index_col=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_degree_distribution(graph_obj):\n",
    "    degree_sequence = sorted([d for n, d in graph_obj.degree()], reverse=True)\n",
    "    try:\n",
    "        plt.title('Boxplot of degree')\n",
    "        plt.boxplot(degree_sequence)\n",
    "        plt.show()\n",
    "    except:\n",
    "        pass\n",
    "    degreeCount = collections.Counter(degree_sequence)\n",
    "    deg, cnt = zip(*degreeCount.items())\n",
    "    try:\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.bar(deg, cnt, width=0.80, color=\"b\")\n",
    "        plt.title(\"Degree Histogram\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(\"Degree\")\n",
    "        plt.show()\n",
    "    except:\n",
    "        pass\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_dims():\n",
    "    global DATA_SOURCE\n",
    "    \n",
    "    dom_dims_path = os.path.join(DATA_SOURCE, 'domain_dims.pkl')\n",
    "    with open(dom_dims_path,'rb') as fh:\n",
    "        domain_dims =  pickle.load(fh)\n",
    "    return domain_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:17<00:00,  2.22s/it]\n"
     ]
    }
   ],
   "source": [
    "target_df = utils.convert_to_serializedID_format( target_df = df_train, DIR=DIR, data_source_loc=None,REFRESH=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = target_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_complete_graph(df):\n",
    "    from itertools import combinations\n",
    "    domain_dims = get_domain_dims()\n",
    "    complete_G =nx.Graph()\n",
    "    edges = [\n",
    "        ['Carrier','ConsigneePanjivaID'],\n",
    "        ['Carrier','HSCode'],\n",
    "        ['Carrier','PortOfLading'],\n",
    "        ['Carrier','PortOfUnlading'],\n",
    "        ['Carrier','ShipperPanjivaID'],\n",
    "        ['ConsigneePanjivaID','HSCode'],\n",
    "        ['ConsigneePanjivaID','PortOfUnlading'],\n",
    "        ['ConsigneePanjivaID','ShipmentDestination'],\n",
    "        ['ConsigneePanjivaID','ShipperPanjivaID'],\n",
    "        ['HSCode','ShipmentDestination'],\n",
    "        ['HSCode','ShipmentOrigin'],\n",
    "        ['HSCode','ShipperPanjivaID'],\n",
    "        ['PortOfLading','ShipmentOrigin'],\n",
    "        ['PortOfLading','ShipperPanjivaID'],\n",
    "        ['PortOfUnlading','ShipmentDestination'],\n",
    "        ['ConsigneePanjivaID','ShipperPanjivaID']\n",
    "    ]\n",
    "    \n",
    "    for i,j in edges:\n",
    "        __tmp__ = df[[i,j]]\n",
    "        __tmp__ = __tmp__.drop_duplicates()\n",
    "        _i = __tmp__[i].values.tolist()\n",
    "        _j = __tmp__[j].values.tolist()\n",
    "        n1_n2 = [ ( a,b)for a,b in zip(_i,_j)]\n",
    "        complete_G.add_edges_from(n1_n2)\n",
    "    return complete_G\n",
    "        \n",
    "complete_G = build_complete_graph(target_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:18<00:00,  2.27s/it]\n"
     ]
    }
   ],
   "source": [
    "df_test = utils.convert_to_serializedID_format( target_df = df_test, DIR=DIR, data_source_loc=None,REFRESH=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dims = get_domain_dims()\n",
    "entity_type_list = {\n",
    "    d : list(set(target_df[d])) for d in domain_dims.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_component_subgraph(graph_obj):\n",
    "    component_id = 0\n",
    "    components = {}\n",
    "    component_size_dict = {}\n",
    "\n",
    "    for c in nx.connected_components(graph_obj):\n",
    "        components[component_id] = c\n",
    "        component_size_dict[component_id] = len(c) \n",
    "        component_id += 1\n",
    "        \n",
    "    component_size_dict = sorted(component_size_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    # Get the largest connected component\n",
    "    max_component = components[component_size_dict[0][0]]\n",
    "    subgraph = graph_obj.subgraph(max_component)\n",
    "    return subgraph , component_size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Used to get approximate candidates \n",
    "# --------------------------------------\n",
    "def get_n2v_model(G):\n",
    "    global DIR\n",
    "    fname = DIR +'_node2vec'\n",
    "    from nodevectors import Node2Vec\n",
    "    if not os.path.exists(fname + '.zip'):\n",
    "        n2v = Node2Vec()\n",
    "        n2v.fit(G)\n",
    "        \n",
    "        n2v.save(DIR +'_node2vec')\n",
    "       \n",
    "    else:\n",
    "        n2v = Node2Vec.load( fname + '.zip')\n",
    "    return n2v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Returns idx, size of component, list of nodes\n",
    "# ------------------------------------------------\n",
    "\n",
    "def get_graph_components(graph_obj):\n",
    "    component_id = 0\n",
    "    components = {}\n",
    "    component_size_dict = {}\n",
    "\n",
    "    for c in nx.connected_components(graph_obj):\n",
    "        components[component_id] = c\n",
    "        component_size_dict[component_id] = len(c) \n",
    "        component_id += 1\n",
    "        \n",
    "    component_size_dict = sorted(component_size_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    components_dict = []\n",
    "    for i, _size in component_size_dict:\n",
    "        components_dict.append((i, _size,   components[i]))\n",
    "      \n",
    "    return components_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# Get ordered list of elements that are approx most dissimilar\n",
    "# ------------------------------------------------------------\n",
    "def query_ordered_dissimilar(ent_id, entity_type, entity_type_list , query_model, exclude_list= None):\n",
    "     \n",
    "    k = int(len(entity_type_list[entity_type]) * .20)\n",
    "    _dict = { e1: query_model.similarity(str(ent_id),str(e1)) for e1 in entity_type_list[entity_type] }\n",
    "    sorted_dict = sorted(_dict.items() ,key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    if exclude_list is not None:\n",
    "        res = [ _[0] for _ in sorted_dict[-k:] if _[0] not in exclude_list]\n",
    "    else:\n",
    "        res = [ _[0] for _ in sorted_dict[-k:] if _[0] ]\n",
    "    return res\n",
    "# Example : query_ordered_dissimilar(75, 'Carrier', entity_type_list , query_model)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def query_ordered_similar(ent_id, entity_type, entity_type_list , query_model, exclude_list= None, include_list = None):\n",
    "    k = int(len(entity_type_list[entity_type]) * .25)\n",
    "    _dict = { e1: query_model.similarity(str(ent_id),str(e1)) for e1 in entity_type_list[entity_type] }\n",
    "    sorted_dict = sorted(_dict.items() ,key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    if exclude_list is not None:\n",
    "        res = [ _[0] for _ in sorted_dict[:k] if _[0] not in exclude_list]\n",
    "    else:\n",
    "        res = [ _[0] for _ in sorted_dict[:k]  ]\n",
    "   \n",
    "    if include_list is not None:\n",
    "        res = [ _ for _ in res if _ in include_list]\n",
    "    else:\n",
    "        res = [ _ for _ in res ]\n",
    "        \n",
    "    return res\n",
    "# Example : query_ordered_similar(75, 'Carrier', entity_type_list , query_model)\n",
    "# -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Return non-cooccurring entities\n",
    "# Example : get_nonCoOcc(45, 'Carrier', 'HSCode'))\n",
    "# Return entity of type HSCode which does not co-occur with Carrier:45\n",
    "# --------------------------------------------------\n",
    "\n",
    "def get_nonCoOcc(ent_id, e_type, target_e_type):\n",
    "    global main_df\n",
    "    global entity_type_list\n",
    "    tmp = set(main_df[[e_type, target_e_type]].loc[main_df[e_type]==ent_id][target_e_type].values.tolist())\n",
    "    return list(set(entity_type_list[target_e_type]).difference(tmp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v = get_n2v_model(complete_G.copy())\n",
    "# Save model to gensim.KeyedVector format\n",
    "n2v.save_vectors(\"{}_n2v_model.bin\".format(DIR))\n",
    "query_model = KeyedVectors.load_word2vec_format(\"{}_n2v_model.bin\".format(DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  68257\n",
      " Is the bipartite graph of Comapnies connected ?  False\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "company_cols = ['ConsigneePanjivaID', 'ShipperPanjivaID']\n",
    "\n",
    "df = target_df.copy()\n",
    "print('>> ', len(df))\n",
    "attributes = [ _ for _ in list(df.columns) if _ not in id_col]\n",
    "df = df.drop_duplicates(subset = attributes)\n",
    "df_subset = df[company_cols].groupby(\n",
    "    company_cols).size().reset_index(\n",
    "    name='count'\n",
    ").sort_values(by='count', ascending=False)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Create a bipartite graph\n",
    "# --------------------\n",
    "B = nx.Graph()\n",
    "B.add_nodes_from(set(df_subset['ConsigneePanjivaID'].values), bipartite=0)\n",
    "B.add_nodes_from(set(df_subset['ShipperPanjivaID'].values), bipartite=1)\n",
    "edges = []\n",
    "\n",
    "for i, j, k in zip(df_subset['ConsigneePanjivaID'].values,\n",
    "                   df_subset['ShipperPanjivaID'].values,\n",
    "                   df_subset['count'].values):\n",
    "    edges.append((i, j, {'weight': k}))\n",
    "\n",
    "B.add_edges_from(edges)\n",
    "print(' Is the bipartite graph of Comapnies connected ? ', nx.is_connected(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes in the largest component 10130\n"
     ]
    }
   ],
   "source": [
    "B0, _ = get_largest_component_subgraph(B)\n",
    "print('Nodes in the largest component', B0.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_sorted = betweenness_centrality_parallel(B0)\n",
    "bt_sorted = sorted(bt_sorted.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "centrality_d = nx.degree_centrality(B0)\n",
    "centrality_d = sorted(centrality_d.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Top comapnies in terms of Between Ness centrality and Degree Centrality\n",
    "# Set K = 50\n",
    "# -----------------------------\n",
    "K = 100\n",
    "top_K_companies = set([_[0] for _ in  bt_sorted[:K] + centrality_d[:K]])\n",
    "top_K_companies = list(set(top_K_companies))\n",
    "tmp1 = df.loc[(df['ConsigneePanjivaID'].isin(top_K_companies)) | (df['ShipperPanjivaID'].isin(top_K_companies))].copy()\n",
    "\n",
    "# The set of HS codes that are mostly transacted\n",
    "popular_hscodes = set(tmp1['HSCode'])\n",
    "hscode_degree_dict = {node_hsc : complete_G.degree(node_hsc) for node_hsc in popular_hscodes}\n",
    "K = int(len(hscode_degree_dict)*0.5)\n",
    "top_K_hscodes = [_[0] for _ in sorted(hscode_degree_dict.items() , key = operator.itemgetter(1), reverse=True)][:K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# To get community/clusters of nodes ,\n",
    "# We remove some highly visible nodes\n",
    "# =====================================\n",
    "rmv = []\n",
    "K = 1000\n",
    "for sorted_dict in [ bt_sorted, centrality_d]:\n",
    "    rmv.extend( [ _[0] for _ in sorted_dict[:K] ])\n",
    "rmv = list(set(rmv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22382 10130\n",
      "5942 8738\n"
     ]
    }
   ],
   "source": [
    "B1 = nx.Graph(B0)\n",
    "print(B1.number_of_edges(), B1.number_of_nodes())\n",
    "\n",
    "B1.remove_nodes_from(rmv)\n",
    "print(B1.number_of_edges(), B1.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_comp, _components = get_largest_component_subgraph(B1)\n",
    "g_components = get_graph_components(B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Carrier': 589,\n",
       " 'ConsigneePanjivaID': 5817,\n",
       " 'HSCode': 125,\n",
       " 'PortOfLading': 243,\n",
       " 'PortOfUnlading': 67,\n",
       " 'ShipmentDestination': 126,\n",
       " 'ShipmentOrigin': 115,\n",
       " 'ShipperPanjivaID': 7271}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set_PoL =  set(df['PortOfLading'])\n",
    "# PoL_distances = {}\n",
    "# for h in set_PoL:\n",
    "#     _dict = {_h:len(nx.shortest_path(complete_G, h,_h))-2 for _h in set_PoL if _h != h }\n",
    "#     PoL_distances[h] = sorted(_dict.items(), key = operator.itemgetter(1), reverse=True)\n",
    "domain_dims = get_domain_dims()\n",
    "domain_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_length = int(len(df_test)*config_ANOM_PERC/100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 5.103448275862069\n",
      "132 1.5172413793103448\n",
      "111 1.2758620689655173\n",
      "Valid candidate found\n",
      " Length of the candidate df >> 87 # Consignee & Shipper  26 26\n"
     ]
    }
   ],
   "source": [
    "positive_candidate = None\n",
    "\n",
    "\n",
    "# Leave out the largest community\n",
    "for i in range(1,10):\n",
    "    g = B1.subgraph(g_components[i][-1])\n",
    "    # what are the entity ids ?\n",
    "    \n",
    "    # Omit nodes which have degree 1\n",
    "    _nodes_ = [ _ for _ in g.nodes() if g.degree(_) > 1]\n",
    "    tmp_df = df.loc[(df['ConsigneePanjivaID'].isin(_nodes_)) & (df['ShipperPanjivaID'].isin(_nodes_))].copy()\n",
    "  \n",
    "    _ratio = len(tmp_df) * config_SIMTYPE / required_length\n",
    "    print(len(tmp_df), _ratio)\n",
    "    if _ratio <= 1.3 and _ratio >= 0.75 :\n",
    "        print('Valid candidate found')\n",
    "        tmp_df = tmp_df.sample(n = int(required_length/config_SIMTYPE), replace=True) \n",
    "        print(\n",
    "            ' Length of the candidate df >>', \n",
    "            tmp_df.shape[0], \n",
    "            '# Consignee & Shipper ', \n",
    "            len(set(tmp_df['ConsigneePanjivaID'])), \n",
    "            len(set(tmp_df['ShipperPanjivaID']))\n",
    "        )\n",
    "        positive_candidate = tmp_df.copy().reset_index(drop=True)\n",
    "        candidate_df = tmp_df.copy()\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = B.subgraph(g_components[1][-1])\n",
    "# print(nx.info(g))\n",
    "# # what are the entity ids ?\n",
    "# list_e = list(g.nodes())\n",
    "# tmp_df = df.loc[(df['ConsigneePanjivaID'].isin(list_e)) & (df['ShipperPanjivaID'].isin(list_e))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(positive_candidate['HSCode'])\n",
    "# query_ordered_dissimilar(ent_id, entity_type, entity_type_list , query_model, exclude_list= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_aux(\n",
    "    cand_df, \n",
    "    idx, \n",
    "    num_samples, \n",
    "    perturb_extra= True\n",
    "):\n",
    "    global df\n",
    "    global query_model\n",
    "    global entity_type_list\n",
    "    global id_col\n",
    "    global domain_dims\n",
    "    global config_SIMTYPE # Ensue this is an even number \n",
    "    print(' IDX', idx)\n",
    "    # Choose the anchor point\n",
    "    a = np.random.uniform()\n",
    "    domain_combinations = { 1 : { \n",
    "            'company_anchor' : 'ConsigneePanjivaID',\n",
    "            'domain_set' : {\n",
    "            1 : ('HSCode',  'ShipmentOrigin'),\n",
    "            2 : ('Carrier', 'PortOfLading'),\n",
    "            3 : ('HSCode',  'Carrier')}},\n",
    "        2 : { \n",
    "            'company_anchor' : 'ShipperPanjivaID',\n",
    "            'domain_set' : {\n",
    "            1 : ( 'HSCode', 'ShipmentDestination'),\n",
    "            2 : ( 'Carrier', 'PortOfUnlading'),\n",
    "            3 : ( 'HSCode',  'Carrier'),\n",
    "            }\n",
    "        }\n",
    "    } \n",
    "    req_count = num_samples \n",
    "    c_idx = [1,2]\n",
    "    ds_idx = [1,2,3]\n",
    "\n",
    "    np.random.shuffle(c_idx)\n",
    "    np.random.shuffle(ds_idx)\n",
    "    _options  =[]\n",
    "    for i in c_idx:\n",
    "        for j in ds_idx:\n",
    "            _options.append((i,j))\n",
    "    for i_j in _options:\n",
    "        i = i_j[0]\n",
    "        j = i_j[1]\n",
    "        \n",
    "        company_anchor = domain_combinations[i]['company_anchor']\n",
    "        d_set = domain_combinations[i]['domain_set'][j]\n",
    "      \n",
    "        print(d_set)\n",
    "        if perturb_extra:\n",
    "            perturb_columns = [ _ for _ in domain_dims.keys() if _ not in ['ConsigneePanjivaID','ShipperPanjivaID'] + list(d_set) ]\n",
    "\n",
    "        # Company entity\n",
    "        company_ent = cand_df.loc[idx, company_anchor]\n",
    "        # entity that does not co-occur with company entity\n",
    "        a1 = get_nonCoOcc(ent_id=company_ent, e_type=company_anchor, target_e_type=d_set[0])\n",
    "        domain_0 = d_set[0]\n",
    "        domain_1 = d_set[1]\n",
    "        # entity that is most dissimilar to company entity : domain_0\n",
    "        a2 = query_ordered_dissimilar(\n",
    "            ent_id = company_ent, \n",
    "            entity_type = domain_0, \n",
    "            entity_type_list = entity_type_list,\n",
    "            query_model=query_model,\n",
    "            exclude_list= None\n",
    "        )\n",
    "        \n",
    "        a3 = list(set(a1).intersection(set(a2)))\n",
    "        \n",
    "        # entity that is most dissimilar to company entity : domain_1\n",
    "        b1 = get_nonCoOcc(ent_id = company_ent, e_type = company_anchor, target_e_type=domain_1)\n",
    "        b2 =  query_ordered_dissimilar(\n",
    "                ent_id = company_ent, \n",
    "                entity_type = domain_1, \n",
    "                entity_type_list = entity_type_list,\n",
    "                query_model=query_model,\n",
    "                exclude_list= None\n",
    "            )\n",
    "        b3 = list(set(b1).intersection(set(b2)))\n",
    "        if a3 is None or b3 is None or len(a3) == 0 or len(b3)==0  : \n",
    "            continue\n",
    "        e1_0 = np.random.choice(a3, size=1, replace=False)[0]\n",
    "        e2_0 = np.random.choice(b3, size=1, replace=False)[0]\n",
    "        \n",
    "        e1_ = query_ordered_similar(\n",
    "            ent_id = e1_0, \n",
    "            entity_type = d_set[0], \n",
    "            entity_type_list = entity_type_list,\n",
    "            query_model=query_model,\n",
    "            exclude_list= [e1_0],\n",
    "            include_list = list(a3)\n",
    "        )\n",
    "\n",
    "        e1 = [e1_0] + e1_[:req_count-1]\n",
    "        e2_ = query_ordered_similar(\n",
    "            ent_id = e2_0, \n",
    "            entity_type = d_set[1], \n",
    "            entity_type_list = entity_type_list,\n",
    "            query_model=query_model,\n",
    "            exclude_list= [e2_0],\n",
    "            include_list = list(b3)\n",
    "        )\n",
    "        e2 = [e2_0] + e2_[:req_count-1]\n",
    "        print( ' >>>' ,len(e1), len(e2))\n",
    "        try:\n",
    "            list_ent_1 = []\n",
    "            list_ent_2 = []\n",
    "\n",
    "            for _i in range(req_count):\n",
    "                if _i%2 == 0:\n",
    "                    list_ent_1.extend([e1[_i],e1[_i]])\n",
    "                    list_ent_2.extend([e2[_i]] )\n",
    "                if _i%2 == 1:\n",
    "                    list_ent_1.extend([e1[_i]])\n",
    "                    list_ent_2.extend([e2[_i],e2[_i]] )\n",
    "                if  len(list_ent_1) >= req_count  or len(list_ent_2)>=req_count:\n",
    "                    break\n",
    "            \n",
    "            res = []\n",
    "            cause = {}\n",
    "            print(list_ent_1, list_ent_2)\n",
    "            for i in range(req_count):\n",
    "                row_copy = cand_df.loc[idx].copy()\n",
    "                row_copy[id_col] = int(str(row_copy[id_col]) + '00{}'.format(i+1))\n",
    "                row_copy[domain_0]= list_ent_1[i]\n",
    "                row_copy[domain_1]= list_ent_2[i]\n",
    "                _p = np.random.choice(perturb_columns,size=1)[0]\n",
    "            \n",
    "                row_copy[_p] = np.random.choice(entity_type_list[_p],size=1)[0]\n",
    "                res.append(row_copy)\n",
    "                cause[str(row_copy[id_col])] = [(company_anchor,domain_0),(company_anchor,domain_1)]\n",
    "            res = pd.concat(res,axis=1).transpose().reset_index(drop=True)\n",
    "            return (res, cause)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    return (None, None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Positive samples\n",
    "# =========================\n",
    "\n",
    "candidate_df = candidate_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = config_SIMTYPE\n",
    "results = Parallel(\n",
    "    n_jobs = mp.cpu_count()\n",
    ")(delayed(generate_aux)( candidate_df, idx, num_samples, ) for idx in range(candidate_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = [] \n",
    "causes = {}\n",
    "for r in results:\n",
    "    if r[0] is not None:\n",
    "        df_results.append(r[0])\n",
    "        causes.update(r[1])\n",
    "        \n",
    "positive_anomalies = pd.concat(df_results).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../generated_data_v1/generated_anomalies/us_import2/pos_anomalies_explantions.json\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Save the causes\n",
    "# ---------------------------\n",
    "pos_anom_causes_file = os.path.join(Anomaly_Output_path,'pos_anomalies_explantions.json')\n",
    "print(pos_anom_causes_file)\n",
    "with open(pos_anom_causes_file, \"w\") as fh:  \n",
    "    json.dump(causes, fh) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Negative anomalies\n",
    "# ==========================\n",
    "# Consider the largest component\n",
    "g = B1.subgraph(g_components[0][-1])\n",
    "_nodes_ = [ _ for _ in g.nodes() if g.degree(_) > 1]\n",
    "tmp_df = df.loc[(df['ConsigneePanjivaID'].isin(_nodes_)) & (df['ShipperPanjivaID'].isin(_nodes_))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_candidates = tmp_df.sample(positive_anomalies.shape[0],replace=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_perturbed_aux(cand_df, idx, num_pert = 3):\n",
    "    global entity_type_list\n",
    "    global id_col\n",
    "    global company_attributes\n",
    "    row_copy = cand_df.loc[idx].copy()\n",
    "    perturb_domains = [ _ for _ in list(entity_type_list.keys()) if _ not in company_attributes ]\n",
    "    perturb_domains = np.random.choice(perturb_domains, num_pert)\n",
    "    \n",
    "    for i in range(num_pert):\n",
    "        _dom = perturb_domains[i]\n",
    "        row_copy[_dom] = np.random.choice(entity_type_list[_dom],size=1)[0]\n",
    "    row_copy[id_col] = int(str(row_copy[id_col]) + '00{}'.format(1))\n",
    "    res = pd.concat([row_copy],axis=1).transpose().reset_index(drop=True)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = Parallel(\n",
    "    n_jobs = mp.cpu_count()\n",
    "    )(delayed(generate_perturbed_aux)( \n",
    "        negative_candidates, idx, \n",
    "    ) for idx in range(negative_candidates.shape[0])\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_anomalies = pd.concat(res).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PanjivaRecordID</th>\n",
       "      <th>Carrier</th>\n",
       "      <th>ConsigneePanjivaID</th>\n",
       "      <th>HSCode</th>\n",
       "      <th>PortOfLading</th>\n",
       "      <th>PortOfUnlading</th>\n",
       "      <th>ShipmentDestination</th>\n",
       "      <th>ShipmentOrigin</th>\n",
       "      <th>ShipperPanjivaID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112331093001</td>\n",
       "      <td>39</td>\n",
       "      <td>6116</td>\n",
       "      <td>6443</td>\n",
       "      <td>6743</td>\n",
       "      <td>6818</td>\n",
       "      <td>6947</td>\n",
       "      <td>6970</td>\n",
       "      <td>9590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112328996001</td>\n",
       "      <td>499</td>\n",
       "      <td>1987</td>\n",
       "      <td>6521</td>\n",
       "      <td>6699</td>\n",
       "      <td>6818</td>\n",
       "      <td>6848</td>\n",
       "      <td>6987</td>\n",
       "      <td>10745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>113233858001</td>\n",
       "      <td>378</td>\n",
       "      <td>5414</td>\n",
       "      <td>6441</td>\n",
       "      <td>6626</td>\n",
       "      <td>6781</td>\n",
       "      <td>6865</td>\n",
       "      <td>7021</td>\n",
       "      <td>9232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110077911001</td>\n",
       "      <td>184</td>\n",
       "      <td>1158</td>\n",
       "      <td>6514</td>\n",
       "      <td>6582</td>\n",
       "      <td>6780</td>\n",
       "      <td>6890</td>\n",
       "      <td>6991</td>\n",
       "      <td>12980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109992843001</td>\n",
       "      <td>282</td>\n",
       "      <td>5456</td>\n",
       "      <td>6520</td>\n",
       "      <td>6600</td>\n",
       "      <td>6840</td>\n",
       "      <td>6943</td>\n",
       "      <td>7007</td>\n",
       "      <td>11648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>110703776001</td>\n",
       "      <td>514</td>\n",
       "      <td>2951</td>\n",
       "      <td>6520</td>\n",
       "      <td>6641</td>\n",
       "      <td>6837</td>\n",
       "      <td>6959</td>\n",
       "      <td>6987</td>\n",
       "      <td>9446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>110174383001</td>\n",
       "      <td>56</td>\n",
       "      <td>4296</td>\n",
       "      <td>6512</td>\n",
       "      <td>6671</td>\n",
       "      <td>6837</td>\n",
       "      <td>6959</td>\n",
       "      <td>6987</td>\n",
       "      <td>8609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>110977940001</td>\n",
       "      <td>40</td>\n",
       "      <td>5426</td>\n",
       "      <td>6509</td>\n",
       "      <td>6582</td>\n",
       "      <td>6800</td>\n",
       "      <td>6923</td>\n",
       "      <td>6987</td>\n",
       "      <td>11573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>113172348001</td>\n",
       "      <td>279</td>\n",
       "      <td>4799</td>\n",
       "      <td>6509</td>\n",
       "      <td>6771</td>\n",
       "      <td>6780</td>\n",
       "      <td>6959</td>\n",
       "      <td>7019</td>\n",
       "      <td>9073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>110100913001</td>\n",
       "      <td>260</td>\n",
       "      <td>3337</td>\n",
       "      <td>6427</td>\n",
       "      <td>6743</td>\n",
       "      <td>6804</td>\n",
       "      <td>6935</td>\n",
       "      <td>7072</td>\n",
       "      <td>14340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>348 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PanjivaRecordID  Carrier  ConsigneePanjivaID  HSCode  PortOfLading  \\\n",
       "0       112331093001       39                6116    6443          6743   \n",
       "1       112328996001      499                1987    6521          6699   \n",
       "2       113233858001      378                5414    6441          6626   \n",
       "3       110077911001      184                1158    6514          6582   \n",
       "4       109992843001      282                5456    6520          6600   \n",
       "..               ...      ...                 ...     ...           ...   \n",
       "343     110703776001      514                2951    6520          6641   \n",
       "344     110174383001       56                4296    6512          6671   \n",
       "345     110977940001       40                5426    6509          6582   \n",
       "346     113172348001      279                4799    6509          6771   \n",
       "347     110100913001      260                3337    6427          6743   \n",
       "\n",
       "     PortOfUnlading  ShipmentDestination  ShipmentOrigin  ShipperPanjivaID  \n",
       "0              6818                 6947            6970              9590  \n",
       "1              6818                 6848            6987             10745  \n",
       "2              6781                 6865            7021              9232  \n",
       "3              6780                 6890            6991             12980  \n",
       "4              6840                 6943            7007             11648  \n",
       "..              ...                  ...             ...               ...  \n",
       "343            6837                 6959            6987              9446  \n",
       "344            6837                 6959            6987              8609  \n",
       "345            6800                 6923            6987             11573  \n",
       "346            6780                 6959            7019              9073  \n",
       "347            6804                 6935            7072             14340  \n",
       "\n",
       "[348 rows x 9 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Save the data frames\n",
    "# ==========================\n",
    "neg_anom_f_path = os.path.join(Anomaly_Output_path,'neg_anomalies.csv')\n",
    "pos_anom_f_path = os.path.join(Anomaly_Output_path,'pos_anomalies.csv')\n",
    "negative_anomalies.to_csv(neg_anom_f_path,index=None)\n",
    "positive_anomalies.to_csv(pos_anom_f_path,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
