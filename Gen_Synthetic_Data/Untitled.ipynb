{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./../..')\n",
    "sys.path.append('./..')\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "import re\n",
    "import yaml\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "id_col = 'PanjivaRecordID'\n",
    "import networkx as nx\n",
    "import operator\n",
    "import collections\n",
    "import argparse\n",
    "\n",
    "CONFIG = None\n",
    "DIR_LOC = None \n",
    "CONFIG = None\n",
    "CONFIG_FILE = 'config.yaml'\n",
    "save_dir = None\n",
    "id_col = 'PanjivaRecordID'\n",
    "use_cols = None\n",
    "freq_bound = None\n",
    "attribute_columns = None\n",
    "domain_dims=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_up_config(_DIR=None):\n",
    "    global DIR\n",
    "    global CONFIG\n",
    "    global CONFIG_FILE\n",
    "    global use_cols\n",
    "    global num_neg_samples\n",
    "    global DATA_SOURCE\n",
    "    global DIR_LOC\n",
    "    global save_dir\n",
    "    global id_col\n",
    "    global attribute_columns\n",
    "    global domain_dims\n",
    "    \n",
    "    DATA_SOURCE = './../generated_data_v1/'\n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "\n",
    "    if _DIR is not None:\n",
    "        DIR = _DIR\n",
    "        CONFIG['DIR'] = _DIR\n",
    "    else:\n",
    "        DIR = CONFIG['DIR']\n",
    "\n",
    "    DIR_LOC = re.sub('[0-9]', '', DIR)\n",
    "    DATA_SOURCE = os.path.join(DATA_SOURCE, DIR)\n",
    "    save_dir = 'stage_2'\n",
    "    save_dir = os.path.join(\n",
    "        DATA_SOURCE,\n",
    "        save_dir\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    use_cols = CONFIG[DIR]['use_cols']\n",
    "    _cols = list(use_cols)\n",
    "    _cols.remove(id_col)\n",
    "    attribute_columns = list(sorted(_cols))\n",
    "    with open(os.path.join(DATA_SOURCE,'domain_dims.pkl'),'rb') as fh:\n",
    "        domain_dims = pickle.load(fh)\n",
    "    print('Domains and sizes', domain_dims)\n",
    "\n",
    "    return\n",
    "\n",
    "def get_positive_nodes():\n",
    "    global save_dir\n",
    "    with open(os.path.join(save_dir,'seed_nodes.pkl'),'rb') as fh:\n",
    "        nodes_dict = pickle.load(fh)\n",
    "       \n",
    "    return nodes_dict\n",
    "\n",
    "# Remove duplicates from test set1\n",
    "def check_suprious_coOcc(\n",
    "    target_df, \n",
    "    ref_df, \n",
    "    domain_dims,\n",
    "    actor_columns = ['ConsigneePanjvaID','ShipperPanjivaID'],\n",
    "    id_col='PanjivaRecordID'):\n",
    "    \n",
    "    # =========================================\n",
    "    # create a hash\n",
    "    # =========================================\n",
    "    print(len(target_df))\n",
    "    domains = [ _ for _ in domain_dims.keys() if _ not in actor_columns]\n",
    "    domain_pairs = [sorted(a) for a in combinations(domains,2)]  \n",
    "    domain_pair_keys = ['_'.join(a) for a in domain_pairs]\n",
    "    valid_values_dict = {}\n",
    "   \n",
    "    for domain_pair in domain_pairs:\n",
    "        df_tmp = ref_df.groupby(domain_pair).size().reset_index(name='count')\n",
    "        d1 = domain_pair[0]\n",
    "        d2 = domain_pair[1]\n",
    "        key = '_'.join(domain_pair)\n",
    "        df_tmp['pair'] = df_tmp.apply(lambda x: str(x[d1]) + '_' + str(x[d2]), axis = 1)\n",
    "        valid_values_dict[key] = list(df_tmp['pair'].values)\n",
    "    \n",
    "    def aux_check(row, domain_pairs):\n",
    "        flag = True\n",
    "        for domain_pair in domain_pairs:\n",
    "            d1 = domain_pair[0]\n",
    "            d2 = domain_pair[1]\n",
    "            key = '_'.join(domain_pair)\n",
    "            value = str(row[d1]) + '_' + str(row[d2])\n",
    "            if value not  in valid_values_dict[key]: \n",
    "                flag = False\n",
    "            \n",
    "        return flag\n",
    "    \n",
    "    target_df['valid'] =  target_df.parallel_apply(aux_check, axis=1,args=(domain_pairs,))\n",
    "    target_df = target_df.loc[target_df['valid']==True]\n",
    "    del target_df['valid']\n",
    "    print( ' Post check length of test set::', len(target_df) )\n",
    "    return target_df\n",
    "\n",
    "\n",
    "def perturb_row(\n",
    "    row,\n",
    "    fixed_columns,\n",
    "    domain_dims,\n",
    "    perturb_count = 3,\n",
    "    id_col = 'PanjivaRecordID'\n",
    "):\n",
    "    new_row = row.copy()\n",
    "    row_dict = row.to_dict()\n",
    "    domains_perturb = [_  for _ in  domain_dims.keys() if _ not in fixed_columns]\n",
    "    \n",
    "    domains_perturb = np.random.choice(domains_perturb,size=perturb_count,replace=False)\n",
    "    \n",
    "    for i in range(perturb_count):\n",
    "        d = domains_perturb[i]\n",
    "        e = row_dict[d]\n",
    "        # select a random entity\n",
    "        while True:\n",
    "            rnd_e = np.random.randint(0, domain_dims[d])\n",
    "            if rnd_e == e : \n",
    "                continue\n",
    "            new_row[d]=rnd_e\n",
    "            break\n",
    "    #Perturb the ID\n",
    "    new_row[id_col] = int(str(new_row[id_col]) + str(1001))\n",
    "    return new_row\n",
    "            \n",
    "    \n",
    "def generate_anomalies(\n",
    "    target_df, \n",
    "    actor_columns, \n",
    "    domain_dims\n",
    "):\n",
    "    anomalous_records = target_df.parallel_apply(\n",
    "        perturb_row,\n",
    "        axis=1,\n",
    "        args=(actor_columns, domain_dims , )\n",
    "    )\n",
    "    return anomalous_records\n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "    \n",
    "    global DAT_SOURCE\n",
    "    global domain_dims\n",
    "    global save_dir\n",
    "    \n",
    "    \n",
    "    actor_pos_nodes_dict = get_positive_nodes()\n",
    "    actor_columns = ['ConsigneePanjivaID','ShipperPanjivaID']\n",
    "    \n",
    "\n",
    "    train_df = pd.read_csv(os.path.join(DATA_SOURCE,'train_data.csv' ),low_memory=False)\n",
    "    test_df = pd.read_csv(os.path.join(DATA_SOURCE,'test_data.csv' ),low_memory=False)\n",
    "    test_df = test_df.drop_duplicates(list(domain_dims.keys()))\n",
    "\n",
    "\n",
    "    cleaned_records = check_suprious_coOcc(\n",
    "        test_df, \n",
    "        train_df, \n",
    "        domain_dims\n",
    "    )\n",
    "\n",
    "\n",
    "    # Select some records that should be used  to generate anomalies\n",
    "\n",
    "    positive_samples = cleaned_records.loc[(cleaned_records['ConsigneePanjivaID'].isin(actor_pos_nodes_dict['ConsigneePanjivaID']))|(cleaned_records['ShipperPanjivaID'].isin(actor_pos_nodes_dict['ShipperPanjivaID']))]\n",
    "    num_positive_samples = len(positive_samples)\n",
    "\n",
    "    candidates = cleaned_records.loc[~(cleaned_records['ConsigneePanjivaID'].isin(actor_pos_nodes_dict['ConsigneePanjivaID'])) & ~(cleaned_records['ShipperPanjivaID'].isin(actor_pos_nodes_dict['ShipperPanjivaID']))]\n",
    "    negative_samples = candidates.sample(num_positive_samples)\n",
    "\n",
    "    print('Print # positive, negative samples', num_positive_samples,len(negative_samples))\n",
    "    pos_neg_IDs = list(negative_samples[id_col].values) + list(positive_samples[id_col].values)\n",
    "\n",
    "    negative_samples = generate_anomalies(\n",
    "        negative_samples, \n",
    "        actor_columns, \n",
    "        domain_dims\n",
    "    )\n",
    "\n",
    "    positive_samples = generate_anomalies(\n",
    "        positive_samples, \n",
    "        actor_columns, \n",
    "        domain_dims\n",
    "    )\n",
    "    positive_samples[id_col] = positive_samples[id_col].apply(lambda x: int(str(x) + str(1002)))\n",
    "    normal_samples = cleaned_records.loc[~(cleaned_records[id_col].isin(pos_neg_IDs))]\n",
    "\n",
    "    # ========================================\n",
    "    # Save the data to csv file \n",
    "    # ========================================\n",
    "\n",
    "    # Normal samples \n",
    "    save_path = os.path.join(save_dir, 'test_normal_data_csv')\n",
    "    normal_samples.to_csv(save_path,index=None)\n",
    "\n",
    "    # Positive samples \n",
    "    save_path = os.path.join(save_dir, 'test_pos_data_csv')\n",
    "    positive_samples.to_csv(save_path,index=None)\n",
    "\n",
    "    # Negative samples \n",
    "    save_path = os.path.join(save_dir, 'test_neg_data_csv')\n",
    "    negative_samples.to_csv(save_path,index=None)\n",
    "\n",
    "    # Save all the cleaned records\n",
    "    save_path = os.path.join(save_dir, 'cleaned_test_data_csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--DIR', choices=['us_import1', 'us_import2', 'us_import3' ],\n",
    "    default= 'us_import1'\n",
    ")\n",
    "\n",
    "args = parser.parse_args()\n",
    "DIR = args.DIR\n",
    "\n",
    "set_up_config(DIR)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37367\n",
      " Post check length of test set:: 22935\n",
      "Print # positive, negative samples 466 466\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'positive_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d8d2f3876e81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpositive_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'positive_samples' is not defined"
     ]
    }
   ],
   "source": [
    "positive_samples.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(466, 466)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_samples),len(negative_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Carrier', 'ConsigneePanjivaID'],\n",
       " ['Carrier', 'HSCode'],\n",
       " ['Carrier', 'PortOfLading'],\n",
       " ['Carrier', 'PortOfUnlading'],\n",
       " ['Carrier', 'ShipmentDestination'],\n",
       " ['Carrier', 'ShipmentOrigin'],\n",
       " ['Carrier', 'ShipperPanjivaID'],\n",
       " ['ConsigneePanjivaID', 'HSCode'],\n",
       " ['ConsigneePanjivaID', 'PortOfLading'],\n",
       " ['ConsigneePanjivaID', 'PortOfUnlading'],\n",
       " ['ConsigneePanjivaID', 'ShipmentDestination'],\n",
       " ['ConsigneePanjivaID', 'ShipmentOrigin'],\n",
       " ['ConsigneePanjivaID', 'ShipperPanjivaID'],\n",
       " ['HSCode', 'PortOfLading'],\n",
       " ['HSCode', 'PortOfUnlading'],\n",
       " ['HSCode', 'ShipmentDestination'],\n",
       " ['HSCode', 'ShipmentOrigin'],\n",
       " ['HSCode', 'ShipperPanjivaID'],\n",
       " ['PortOfLading', 'PortOfUnlading'],\n",
       " ['PortOfLading', 'ShipmentDestination'],\n",
       " ['PortOfLading', 'ShipmentOrigin'],\n",
       " ['PortOfLading', 'ShipperPanjivaID'],\n",
       " ['PortOfUnlading', 'ShipmentDestination'],\n",
       " ['PortOfUnlading', 'ShipmentOrigin'],\n",
       " ['PortOfUnlading', 'ShipperPanjivaID'],\n",
       " ['ShipmentDestination', 'ShipmentOrigin'],\n",
       " ['ShipmentDestination', 'ShipperPanjivaID'],\n",
       " ['ShipmentOrigin', 'ShipperPanjivaID']]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
